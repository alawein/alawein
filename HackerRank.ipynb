{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOGPNZjvJ4h8l4BM2FOWvk/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alawein/alawein/blob/main/HackerRank.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7bf1c04"
      },
      "source": [
        "# HackerRank Intensive Prep: 1-Day Crash Course for Senior Data Scientist\n",
        "\n",
        "This document outlines a comprehensive 1-day crash course designed to rapidly refresh senior data scientists on essential skills for HackerRank assessments and technical interviews. The course is structured into four main pillars, building from fundamental Python concepts to advanced ML framework applications and real-world problem-solving patterns.\n",
        "\n",
        "## Course Outline\n",
        "\n",
        "### 1. Quick Fundamentals Review (1-2 hours)\n",
        "\n",
        "#### 1.1 Python Basics: Syntax, Data Structures, and Core Operations\n",
        "*   **Core Python Syntax:** Variables, Data Types, Operators, Control Flow (`if/elif/else`, `for`, `while`), Loop Control (`break`, `continue`).\n",
        "    *   _Refer to notebook sections on 'Core Python Syntax' for detailed explanations and executable examples with `O(1)` or `O(N)` complexities._\n",
        "*   **Built-in Data Structures:**\n",
        "    *   **Lists:** Mutable, ordered, allows duplicates. Operations include creation (`O(N)`), access (`O(1)`), slicing (`O(k)`), append (amortized `O(1)`), insert (`O(N)`), pop (`O(1)`/`O(N)`), remove (`O(N)`), membership (`O(N)`), iteration (`O(N)`).\n",
        "    *   **Tuples:** Immutable, ordered, allows duplicates. Operations include creation (`O(N)`), access (`O(1)`), slicing (`O(k)`), concatenation (`O(N+M)`), repetition (`O(N*k)`), unpacking (`O(1)`).\n",
        "    *   **Sets:** Mutable, unordered, unique elements. Operations include creation (`O(N)`), add (amortized `O(1)`), remove/discard (amortized `O(1)`), membership (average `O(1)`), set operations (union, intersection, difference).\n",
        "    *   **Dictionaries:** Mutable, unordered (insertion-ordered Python 3.7+), key-value pairs. Operations include creation (`O(N)`), access/add/modify/remove (average `O(1)`), membership (`O(1)`), iteration (`O(N)`).\n",
        "    *   _Refer to notebook sections on 'Built-in Data Structures' for detailed explanations and executable examples with complexity analyses._\n",
        "*   **String Manipulation Techniques:** Common methods (`strip()`, `lower()`, `replace()`, `split()`, `join()`, `find()`, `startswith()`, `count()`, `isdigit()`) and basic Regular Expressions (`re` module functions like `re.search()`, `re.findall()`, `re.sub()`).\n",
        "    *   _Refer to notebook sections on 'String Manipulation Techniques' for detailed explanations and executable examples with `O(N)` or `O(N*M)` complexities._\n",
        "*   **List Comprehensions and Lambda Functions:** Concise, Pythonic constructs for list creation/transformation (`O(N)`) and anonymous functions (`O(1)` for simple expressions), often used with `map()`, `filter()`, `sorted()`.\n",
        "    *   _Refer to notebook sections on 'List Comprehensions and Lambda Functions' for explanations and examples._\n",
        "\n",
        "#### 1.2 HackerRank Practice: Reinforcing Python Fundamentals\n",
        "*   **Suggested Easy Problems:**\n",
        "    1.  **\"Python If-Else\"**: Conditional logic.\n",
        "    2.  **\"List Comprehensions\"**: Efficient list generation.\n",
        "    3.  **\"Finding the Percentage\"**: Dictionaries, lists, basic arithmetic.\n",
        "*   **Guidance:** Focus on reading carefully, planning, code clarity, and initial complexity thoughts. Leverage the [GitHub repository](https://github.com/hevalhazalkurt/Hackerrank_Python_Solutions) for solution patterns after attempting.\n",
        "    *   _Refer to notebook section 'Quick Fundamentals Review - HackerRank Practice' for direct links and detailed advice._\n",
        "\n",
        "### 2. Algorithm Essentials (2-3 hours)\n",
        "\n",
        "#### 2.1 Core Algorithms & Complexity\n",
        "*   **Sorting Algorithms:**\n",
        "    *   **Merge Sort:** Divide and conquer. Time: `O(N log N)` (best, average, worst). Space: `O(N)`. _(Stable sort)_\n",
        "    *   **Quick Sort:** Divide and conquer with partitioning. Time: `O(N log N)` (best, average), `O(N^2)` (worst). Space: `O(log N)` (average), `O(N)` (worst).\n",
        "    *   _Refer to notebook sections on 'Merge Sort' and 'Quick Sort' for implementations and detailed analyses._\n",
        "*   **Searching Algorithms:**\n",
        "    *   **Binary Search:** Efficiently finds an item in a **sorted** list. Time: `O(log N)`. Space: `O(1)` (iterative), `O(log N)` (recursive).\n",
        "    *   _Refer to notebook sections on 'Binary Search' for implementations and detailed analyses._\n",
        "*   **Two-Pointer Technique:** Uses two pointers to traverse data structures. Time: `O(N)`. Space: `O(1)`. Useful for"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7c1fe9b3"
      },
      "source": [
        "# Quick Fundamentals Review: Cheat Sheet\n",
        "\n",
        "This section provides a rapid, condensed overview of Python fundamentals, focusing on essential syntax, data structures, string operations, and Pythonic constructs, along with their complexities and best practices for senior data scientists.\n",
        "\n",
        "## 1. Python Basics: Syntax & Core Operations\n",
        "\n",
        "*   **Variables & Data Types:** Dynamically typed. `int`, `float`, `bool`, `str`. Access: `O(1)`. Assignment: `O(1)`.\n",
        "*   **Operators:** Arithmetic (`+`, `-`, `*`, `/`, `%`, `**`), Comparison (`==`, `!=`, `<`, `>`), Logical (`and`, `or`, `not`), Membership (`in`, `not in`). All `O(1)` for primitive types.\n",
        "*   **Control Flow:**\n",
        "    *   `if/elif/else`: Conditional execution. `O(1)` per condition check.\n",
        "    *   `for` loops: Iterate over sequences. `O(N)` for N elements.\n",
        "    *   `while` loops: Repeat until condition false. `O(N)` for N iterations.\n",
        "    *   `break`, `continue`: Loop control. `O(1)` to jump.\n",
        "*   **Best Practice:** Write readable code. Use meaningful variable names. Avoid deep nesting for clarity.\n",
        "\n",
        "## 2. Built-in Data Structures\n",
        "\n",
        "### 2.1 Lists (`[]`)\n",
        "*   **Characteristics:** Ordered, Mutable, Allows Duplicates.\n",
        "*   **Key Operations & Complexity:**\n",
        "    *   Creation: `[1, 2, 3]` (`O(N)`)\n",
        "    *   Access/Modification: `my_list[idx]` (`O(1)`)\n",
        "    *   Slicing: `my_list[s:e]` (`O(k)` where k is slice length)\n",
        "    *   `append()`: Amortized `O(1)` (occasionally `O(N)` on resize)\n",
        "    *   `insert(idx, val)`: `O(N)`\n",
        "    *   `pop()`: `O(1)` (end), `pop(idx)`: `O(N)`\n",
        "    *   `remove(val)`: `O(N)`\n",
        "    *   Membership (`in`): `O(N)` (worst case)\n",
        "    *   Iteration: `O(N)`\n",
        "*   **Use Cases:** General-purpose collections, stacks, queues.\n",
        "*   **Common Pitfall:** Modifying a list while iterating over it (can lead to unexpected behavior).\n",
        "\n",
        "### 2.2 Tuples (`()`)\n",
        "*   **Characteristics:** Ordered, Immutable, Allows Duplicates.\n",
        "*   **Key Operations & Complexity:** (Similar to List for creation, access, slicing, membership, iteration).\n",
        "    *   Creation: `(1, 2, 3)` or `1, 2, 3` (`O(N)`)\n",
        "    *   Concatenation: `O(N+M)`\n",
        "    *   Repetition: `O(N*k)`\n",
        "    *   Unpacking: `O(1)`\n",
        "*   **Use Cases:** Fixed collections (e.g., coordinates), dictionary keys (due to immutability), function return values.\n",
        "\n",
        "### 2.3 Sets (`{}`)\n",
        "*   **Characteristics:** Unordered, Mutable, Unique Elements (hashable items only).\n",
        "*   **Key Operations & Complexity:**\n",
        "    *   Creation: `{1, 2, 3}` or `set([1, 2])` (`O(N)`)\n",
        "    *   `add(item)`: Amortized `O(1)`\n",
        "    *   `remove(item)`/`discard(item)`: Amortized `O(1)`\n",
        "    *   Membership (`in`): Average `O(1)` (worst `O(N)` due to hash collisions)\n",
        "    *   Set Operations (`union`, `intersection`, `difference`): `O(|set1| + |set2|)` or `O(min(|set1|, |set2|))`\n",
        "*   **Use Cases:** Removing duplicates, fast membership testing, mathematical set operations.\n",
        "*   **Common Problem:** Cannot contain mutable types (e.g., lists) as elements.\n",
        "\n",
        "### 2.4 Dictionaries (`{key: value}`)\n",
        "*   **Characteristics:** Unordered (insertion-ordered Python 3.7+), Mutable, Key-Value Pairs (keys must be unique and hashable).\n",
        "*   **Key Operations & Complexity:**\n",
        "    *   Creation: `{'a': 1, 'b': 2}` (`O(N)`)\n",
        "    *   Access/Add/Modify/Remove: `my_dict[key]` or `my_dict.get(key)` (`O(1)` average, `O(N)` worst)\n",
        "    *   Membership (`key in my_dict`): Average `O(1)`\n",
        "    *   Iteration (`.keys()`, `.values()`, `.items()`): `O(N)`\n",
        "*   **Use Cases:** Fast lookups by key, representing structured data, frequency counters.\n",
        "*   **Common Problem:** `KeyError` if accessing a non-existent key (use `.get()` or `defaultdict`).\n",
        "\n",
        "## 3. String Manipulation & Regular Expressions\n",
        "\n",
        "### 3.1 Common String Methods\n",
        "*   **Concatenation:** `str1 + str2` (`O(M+N)`).\n",
        "*   **Indexing/Slicing:** `my_str[idx]` (`O(1)`), `my_str[s:e]` (`O(k)`).\n",
        "*   **`len()`:** `O(1)`.\n",
        "*   **Formatting:** `f-strings`, `.format()`, `s % args`.\n",
        "*   **Cleaning/Transformation:** `strip()`, `lower()`, `upper()`, `replace(old, new)` (`O(N)` or `O(N*M)`).\n",
        "*   **Splitting/Joining:** `split(delimiter)` (`O(N)`), `delimiter.join(list)` (`O(N)` total length).\n",
        "*   **Searching:** `find(sub)`, `index(sub)`, `startswith(prefix)`, `endswith(suffix)` (`O(N*M)` or `O(M)`).\n",
        "*   **Verification:** `isdigit()`, `isalpha()`, `isalnum()` (`O(N)`).\n",
        "*   **Best Practice:** Use f-strings for clear string formatting. For multiple concatenations, `join()` is usually more efficient.\n",
        "\n",
        "### 3.2 Basic Regular Expressions (`re` module)\n",
        "*   **Concept:** Powerful pattern matching using a mini-language.\n",
        "*   **Key Functions & Complexity:**\n",
        "    *   `re.search(pattern, string)`: First match anywhere. `O(N*M)` worst.\n",
        "    *   `re.match(pattern, string)`: Match at start of string. `O(M)` worst.\n",
        "    *   `re.findall(pattern, string)`: All non-overlapping matches. `O(N*M)` worst.\n",
        "    *   `re.sub(pattern, repl, string)`: Replace matches. `O(N*M)` worst.\n",
        "*   **Common Patterns:** `.` (any char), `*` (0+), `+` (1+), `?` (0 or 1), `^` (start), `$` (end), `\\d` (digit), `\\w` (word char), `\\s` (whitespace), `[abc]`, `[^abc]`, `(group)`.\n",
        "*   **Use Cases:** Data extraction, validation, text cleaning.\n",
        "*   **Puzzle:** Validate email addresses or complex data formats.\n",
        "\n",
        "## 4. List Comprehensions & Lambda Functions\n",
        "\n",
        "### 4.1 List Comprehensions\n",
        "*   **Syntax:** `[expression for item in iterable if condition]`.\n",
        "*   **Benefits:** Concise, readable, efficient (`O(N)` typically faster than loops).\n",
        "*   **Applications:** Creating lists, filtering, transforming, flattening nested lists.\n",
        "*   **Example:** `[x**2 for x in range(5) if x % 2 == 0]` â†’ `[0, 4, 16]`.\n",
        "*   **Best Practice:** Prefer over explicit `for` loops for simple list construction and transformation.\n",
        "\n",
        "### 4.2 Lambda Functions\n",
        "*   **Syntax:** `lambda arguments: expression`.\n",
        "*   **Characteristics:** Small, anonymous, single-expression functions.\n",
        "*   **Benefits:** Concise, inline usage.\n",
        "*   **Applications:** Often used with `map()`, `filter()`, `sorted()`, `key` arguments.\n",
        "*   **Example:** `sorted(data, key=lambda x: x['score'])`.\n",
        "*   **Best Practice:** Use for simple, one-off functions. Avoid complex logic; use `def` for clarity.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a82dbb95"
      },
      "source": [
        "# Task\n",
        "Generate a comprehensive 1-day HackerRank crash course for senior data scientists covering Python fundamentals, essential algorithms (sorting, searching, graph traversal, dynamic programming), progressive problem-solving across various data structures, and senior-level ML/framework applications (NumPy, Pandas, PyTorch/JAX/TensorFlow). The course should include concise explanations, executable Colab code snippets with complexity analysis, suggested HackerRank problems (Easy, Medium, Advanced), and guidance on leveraging \"https://github.com/hevalhazalkurt/Hackerrank_Python_Solutions\" for solution patterns."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5a1dbd9"
      },
      "source": [
        "## Introduction to the 1-Day Crash Course\n",
        "\n",
        "### Subtask:\n",
        "Introduce the purpose and structure of the intensive 1-day HackerRank crash course designed for senior data scientists. Outline the key areas of focus: Python fundamentals, algorithm essentials, progressive problem-solving, and senior-level ML/framework applications.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0e12ef5f"
      },
      "source": [
        "This intensive 1-day HackerRank crash course is specifically designed for senior data scientists.\n",
        "\n",
        "**Purpose:** The primary goal of this course is to equip senior data scientists with the necessary skills, strategies, and confidence to excel in HackerRank assessments and other technical interviews.\n",
        "\n",
        "**Structure and Key Areas of Focus:** The course will cover the following essential areas:\n",
        "\n",
        "*   **Python Fundamentals:** A rapid review and deep dive into advanced Python concepts relevant for efficient and scalable data science solutions.\n",
        "*   **Algorithm Essentials:** Understanding and implementing core algorithms and data structures critical for problem-solving.\n",
        "*   **Progressive Problem-Solving:** A structured approach to tackling increasingly complex problems across various data structures, emphasizing optimization and efficiency.\n",
        "*   **Senior-Level ML/Framework Applications:** Practical application of machine learning concepts using industry-standard frameworks like NumPy, Pandas, and deep learning libraries such as PyTorch, JAX, or TensorFlow."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1db31c4f"
      },
      "source": [
        "## Quick Fundamentals Review - Python Basics\n",
        "\n",
        "### Subtask:\n",
        "Review core Python syntax, built-in data structures (lists, dictionaries, sets, tuples), string manipulation techniques (including basic regex patterns), list comprehensions, and lambda functions. Provide concise explanations, working code examples (as Colab snippets), and discuss common pitfalls and performance considerations. All examples will include complexity analysis.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52fcac04"
      },
      "source": [
        "## Core Python Syntax\n",
        "\n",
        "Python's syntax is designed to be readable and straightforward, emphasizing clarity and reducing development time. Understanding these fundamental elements is crucial for writing any Python program.\n",
        "\n",
        "### Variables\n",
        "Variables are used to store data values. Python is dynamically typed, meaning you don't have to declare the type of a variable when you declare it; the interpreter infers the type at runtime. Variable names must start with a letter or an underscore, and can contain alphanumeric characters and underscores.\n",
        "\n",
        "### Data Types\n",
        "Python has several built-in data types, including:\n",
        "- **Numeric Types**: `int` (integers), `float` (floating-point numbers), `complex` (complex numbers).\n",
        "- **Boolean Type**: `bool` (True/False).\n",
        "- **Sequence Types**: `str` (strings), `list` (lists), `tuple` (tuples).\n",
        "- **Set Types**: `set` (sets), `frozenset` (immutable sets).\n",
        "- **Mapping Type**: `dict` (dictionaries).\n",
        "\n",
        "### Operators\n",
        "Operators are special symbols that perform operations on values and variables. Common types include:\n",
        "- **Arithmetic Operators**: `+`, `-`, `*`, `/`, `%` (modulus), `**` (exponentiation), `//` (floor division).\n",
        "- **Comparison Operators**: `==` (equal to), `!=` (not equal to), `>` (greater than), `<` (less than), `>=` (greater than or equal to), `<=` (less than or equal to).\n",
        "- **Assignment Operators**: `=`, `+=`, `-=`, `*=`, `/=`, `%=`, `**=`, `//=`.\n",
        "- **Logical Operators**: `and`, `or`, `not`.\n",
        "- **Identity Operators**: `is`, `is not`.\n",
        "- **Membership Operators**: `in`, `not in`.\n",
        "\n",
        "### Control Flow\n",
        "Control flow statements determine the order in which code is executed. Key constructs include:\n",
        "- **Conditional Statements (`if`/`elif`/`else`)**: Allow code execution to be based on certain conditions.\n",
        "- **Loops (`for`/`while`)**: Used for repetitive execution of a block of code.\n",
        "  - `for` loops iterate over a sequence (e.g., list, tuple, string, range).\n",
        "  - `while` loops execute a block of code as long as a condition is true.\n",
        "- **Loop Control Statements**: `break` (terminates the loop), `continue` (skips the rest of the current iteration and moves to the next), `pass` (a null operation; nothing happens when it executes)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "71d64b8a",
        "outputId": "c01c13ae-29b3-4a42-fd51-4f97fe63972f"
      },
      "source": [
        "print('--- Variables ---')\n",
        "# Variable assignment: O(1) time complexity\n",
        "x = 10\n",
        "y = \"Hello Python\"\n",
        "print(f\"x: {x}, type: {type(x)}\")\n",
        "print(f\"y: {y}, type: {type(y)}\")\n",
        "\n",
        "print('\\n--- Data Types ---')\n",
        "# Integers\n",
        "a = 5         # O(1)\n",
        "# Floats\n",
        "b = 3.14      # O(1)\n",
        "# Booleans\n",
        "c = True      # O(1)\n",
        "# Strings\n",
        "d = \"Data\"    # O(1)\n",
        "print(f\"a (int): {a}, b (float): {b}, c (bool): {c}, d (str): {d}\")\n",
        "\n",
        "print('\\n--- Operators ---')\n",
        "# Arithmetic operations: O(1) time complexity\n",
        "result_add = 10 + 5    # O(1)\n",
        "result_sub = 10 - 5    # O(1)\n",
        "result_mul = 10 * 5    # O(1)\n",
        "result_div = 10 / 5    # O(1)\n",
        "result_mod = 10 % 3    # O(1)\n",
        "print(f\"Addition: {result_add}, Subtraction: {result_sub}, Multiplication: {result_mul}, Division: {result_div}, Modulus: {result_mod}\")\n",
        "\n",
        "# Comparison operations: O(1) time complexity\n",
        "is_equal = (result_add == 15) # O(1)\n",
        "is_greater = (result_mul > 40) # O(1)\n",
        "print(f\"Is result_add equal to 15? {is_equal}\")\n",
        "print(f\"Is result_mul greater than 40? {is_greater}\")\n",
        "\n",
        "# Logical operations: O(1) time complexity\n",
        "logic_and = (True and False) # O(1)\n",
        "logic_or = (True or False)   # O(1)\n",
        "logic_not = (not True)       # O(1)\n",
        "print(f\"True and False: {logic_and}, True or False: {logic_or}, not True: {logic_not}\")\n",
        "\n",
        "print('\\n--- Control Flow (if/elif/else) ---')\n",
        "# Conditional statement: O(1) in the worst case for a fixed number of conditions\n",
        "score = 85\n",
        "if score >= 90:\n",
        "    print(\"Grade: A\")\n",
        "elif score >= 80:\n",
        "    print(\"Grade: B\")\n",
        "elif score >= 70:\n",
        "    print(\"Grade: C\")\n",
        "else:\n",
        "    print(\"Grade: F\")\n",
        "\n",
        "print('\\n--- Control Flow (for loop) ---')\n",
        "# For loop: O(N) where N is the number of elements in the range\n",
        "# Iterating over a range of numbers\n",
        "print(\"Counting from 0 to 2:\")\n",
        "for i in range(3):\n",
        "    print(i)\n",
        "\n",
        "# Iterating over a list\n",
        "print(\"Iterating over a list:\")\n",
        "my_list = [\"apple\", \"banana\", \"cherry\"]\n",
        "for item in my_list:\n",
        "    print(item)\n",
        "\n",
        "print('\\n--- Control Flow (while loop) ---')\n",
        "# While loop: O(N) where N is the number of iterations\n",
        "count = 0\n",
        "print(\"Counting up to 3:\")\n",
        "while count < 3:\n",
        "    print(count)\n",
        "    count += 1\n",
        "\n",
        "print('\\n--- Loop Control Statements ---')\n",
        "# break statement\n",
        "print(\"Demonstrating 'break':\")\n",
        "for i in range(5):\n",
        "    if i == 3:\n",
        "        break  # O(1) to break, overall O(k) where k is when break occurs\n",
        "    print(i)\n",
        "\n",
        "# continue statement\n",
        "print(\"Demonstrating 'continue':\")\n",
        "for i in range(5):\n",
        "    if i == 2:\n",
        "        continue # O(1) to continue, overall O(N) for N iterations\n",
        "    print(i)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Variables ---\n",
            "x: 10, type: <class 'int'>\n",
            "y: Hello Python, type: <class 'str'>\n",
            "\n",
            "--- Data Types ---\n",
            "a (int): 5, b (float): 3.14, c (bool): True, d (str): Data\n",
            "\n",
            "--- Operators ---\n",
            "Addition: 15, Subtraction: 5, Multiplication: 50, Division: 2.0, Modulus: 1\n",
            "Is result_add equal to 15? True\n",
            "Is result_mul greater than 40? True\n",
            "True and False: False, True or False: True, not True: False\n",
            "\n",
            "--- Control Flow (if/elif/else) ---\n",
            "Grade: B\n",
            "\n",
            "--- Control Flow (for loop) ---\n",
            "Counting from 0 to 2:\n",
            "0\n",
            "1\n",
            "2\n",
            "Iterating over a list:\n",
            "apple\n",
            "banana\n",
            "cherry\n",
            "\n",
            "--- Control Flow (while loop) ---\n",
            "Counting up to 3:\n",
            "0\n",
            "1\n",
            "2\n",
            "\n",
            "--- Loop Control Statements ---\n",
            "Demonstrating 'break':\n",
            "0\n",
            "1\n",
            "2\n",
            "Demonstrating 'continue':\n",
            "0\n",
            "1\n",
            "3\n",
            "4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28adb0ae"
      },
      "source": [
        "## Built-in Data Structures\n",
        "\n",
        "Python's built-in data structures are fundamental for organizing and manipulating data efficiently. Understanding their properties, common operations, and appropriate use cases is crucial for effective programming.\n",
        "\n",
        "### 1. Lists (`list`)\n",
        "\n",
        "**Characteristics:**\n",
        "- Ordered collection of items.\n",
        "- Mutable: Elements can be added, removed, or changed after creation.\n",
        "- Allows duplicate members.\n",
        "- Can contain items of different data types.\n",
        "\n",
        "**Common Operations & Use Cases:**\n",
        "- **Creation:** `my_list = [1, 'hello', 3.14]`\n",
        "- **Accessing elements:** `my_list[0]` (O(1))\n",
        "- **Slicing:** `my_list[1:3]` (O(k) where k is slice length)\n",
        "- **Adding elements:**\n",
        "  - `my_list.append(item)`: Adds to the end. (Amortized O(1))\n",
        "  - `my_list.insert(index, item)`: Inserts at a specific index. (O(N))\n",
        "- **Removing elements:**\n",
        "  - `my_list.pop()`: Removes and returns last item. (O(1))\n",
        "  - `my_list.pop(index)`: Removes and returns item at index. (O(N))\n",
        "  - `my_list.remove(value)`: Removes first occurrence of value. (O(N))\n",
        "  - `del my_list[index]` (O(N))\n",
        "- **Checking membership:** `item in my_list` (O(N))\n",
        "- **Iteration:** `for item in my_list:` (O(N))\n",
        "- **Use Cases:** Storing collections where order matters, frequent modifications (add/remove), implementing stacks/queues.\n",
        "\n",
        "### 2. Tuples (`tuple`)\n",
        "\n",
        "**Characteristics:**\n",
        "- Ordered collection of items.\n",
        "- Immutable: Elements cannot be changed after creation.\n",
        "- Allows duplicate members.\n",
        "- Can contain items of different data types.\n",
        "\n",
        "**Common Operations & Use Cases:**\n",
        "- **Creation:** `my_tuple = (1, 'hello', 3.14)` or `my_tuple = 1, 'hello', 3.14`\n",
        "- **Accessing elements:** `my_tuple[0]` (O(1))\n",
        "- **Slicing:** `my_tuple[1:3]` (O(k))\n",
        "- **Checking membership:** `item in my_tuple` (O(N))\n",
        "- **Iteration:** `for item in my_tuple:` (O(N))\n",
        "- **Use Cases:** Storing heterogeneous data that should not change (e.g., coordinates, database records), returning multiple values from a function, dictionary keys (because they are hashable).\n",
        "\n",
        "### 3. Sets (`set`)\n",
        "\n",
        "**Characteristics:**\n",
        "- Unordered collection of unique items.\n",
        "- Mutable: Elements can be added or removed.\n",
        "- Does not allow duplicate members.\n",
        "- Items must be hashable (immutable).\n",
        "\n",
        "**Common Operations & Use Cases:**\n",
        "- **Creation:** `my_set = {1, 2, 3}` or `my_set = set([1, 2, 3])`\n",
        "- **Adding elements:** `my_set.add(item)` (Amortized O(1))\n",
        "- **Removing elements:**\n",
        "  - `my_set.remove(item)`: Raises error if item not found. (Amortized O(1))\n",
        "  - `my_set.discard(item)`: No error if item not found. (Amortized O(1))\n",
        "- **Checking membership:** `item in my_set` (Average O(1), Worst O(N) due to hash collisions)\n",
        "- **Set operations:**\n",
        "  - `union()`: `set1.union(set2)` (O(|set1| + |set2|))\n",
        "  - `intersection()`: `set1.intersection(set2)` (O(min(|set1|, |set2|)))\n",
        "  - `difference()`: `set1.difference(set2)` (O(|set1|))\n",
        "- **Use Cases:** Removing duplicates from a list, efficient membership testing, mathematical set operations.\n",
        "\n",
        "### 4. Dictionaries (`dict`)\n",
        "\n",
        "**Characteristics:**\n",
        "- Unordered collection of key-value pairs (as of Python 3.7, insertion order is preserved).\n",
        "- Mutable: Key-value pairs can be added, removed, or changed.\n",
        "- Keys must be unique and hashable (immutable).\n",
        "- Values can be of any data type.\n",
        "\n",
        "**Common Operations & Use Cases:**\n",
        "- **Creation:** `my_dict = {'name': 'Alice', 'age': 30}`\n",
        "- **Accessing values:** `my_dict['name']` (Average O(1), Worst O(N))\n",
        "- **Adding/Modifying elements:** `my_dict['city'] = 'New York'` (Average O(1))\n",
        "- **Removing elements:**\n",
        "  - `del my_dict['age']` (Average O(1))\n",
        "  - `my_dict.pop('name')` (Average O(1))\n",
        "- **Checking key membership:** `'name' in my_dict` (Average O(1))\n",
        "- **Iterating:** `for key, value in my_dict.items():` (O(N) for iteration)\n",
        "- **Use Cases:** Representing structured data, fast lookups by key, frequency counters, mapping relationships."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b1a43209",
        "outputId": "1ac798ac-26db-4243-c773-dab0b0350077"
      },
      "source": [
        "print('--- Lists (Mutable, Ordered, Allows Duplicates) ---')\n",
        "\n",
        "# 1. Creation: O(N) where N is the number of elements\n",
        "my_list = [1, 2, 'three', 4.0, 2]\n",
        "print(f\"Original list: {my_list}\")\n",
        "\n",
        "# 2. Accessing elements: O(1)\n",
        "first_element = my_list[0]\n",
        "last_element = my_list[-1]\n",
        "print(f\"First element: {first_element}, Last element: {last_element}\")\n",
        "\n",
        "# 3. Slicing: O(k) where k is the length of the slice\n",
        "sub_list = my_list[1:4]\n",
        "print(f\"Sliced list (index 1 to 3): {sub_list}\")\n",
        "\n",
        "# 4. Adding elements:\n",
        "#    a. append(): Amortized O(1) (usually O(1), occasionally O(N) when resizing)\n",
        "my_list.append(5)\n",
        "print(f\"After append(5): {my_list}\")\n",
        "\n",
        "#    b. insert(): O(N) because elements need to be shifted\n",
        "my_list.insert(1, 'new_second')\n",
        "print(f\"After insert(1, 'new_second'): {my_list}\")\n",
        "\n",
        "# 5. Removing elements:\n",
        "#    a. pop() (last element): O(1)\n",
        "popped_last = my_list.pop()\n",
        "print(f\"Popped last element: {popped_last}, List now: {my_list}\")\n",
        "\n",
        "#    b. pop(index): O(N) because elements need to be shifted\n",
        "popped_indexed = my_list.pop(1)\n",
        "print(f\"Popped element at index 1: {popped_indexed}, List now: {my_list}\")\n",
        "\n",
        "#    c. remove(value): O(N) because it searches for the value and then shifts elements\n",
        "if 2 in my_list:\n",
        "    my_list.remove(2)\n",
        "print(f\"After remove(2): {my_list}\")\n",
        "\n",
        "#    d. del my_list[index]: O(N)\n",
        "del my_list[0]\n",
        "print(f\"After del my_list[0]: {my_list}\")\n",
        "\n",
        "# 6. Checking membership (in operator): O(N) in worst case\n",
        "is_three_present = 'three' in my_list\n",
        "print(f\"'three' in list? {is_three_present}\")\n",
        "\n",
        "# 7. Iteration: O(N)\n",
        "print(\"Iterating through list:\")\n",
        "for item in my_list:\n",
        "    print(f\"Item: {item}\")\n",
        "\n",
        "# 8. Modifying an element: O(1)\n",
        "my_list[0] = 'one_changed'\n",
        "print(f\"After modifying first element: {my_list}\")\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Lists (Mutable, Ordered, Allows Duplicates) ---\n",
            "Original list: [1, 2, 'three', 4.0, 2]\n",
            "First element: 1, Last element: 2\n",
            "Sliced list (index 1 to 3): [2, 'three', 4.0]\n",
            "After append(5): [1, 2, 'three', 4.0, 2, 5]\n",
            "After insert(1, 'new_second'): [1, 'new_second', 2, 'three', 4.0, 2, 5]\n",
            "Popped last element: 5, List now: [1, 'new_second', 2, 'three', 4.0, 2]\n",
            "Popped element at index 1: new_second, List now: [1, 2, 'three', 4.0, 2]\n",
            "After remove(2): [1, 'three', 4.0, 2]\n",
            "After del my_list[0]: ['three', 4.0, 2]\n",
            "'three' in list? True\n",
            "Iterating through list:\n",
            "Item: three\n",
            "Item: 4.0\n",
            "Item: 2\n",
            "After modifying first element: ['one_changed', 4.0, 2]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2d7ef547",
        "outputId": "3113efae-ca85-43e4-85b0-d73e729ff61e"
      },
      "source": [
        "print('\\n--- Tuples (Immutable, Ordered, Allows Duplicates) ---')\n",
        "\n",
        "# 1. Creation: O(N) where N is the number of elements\n",
        "my_tuple = (1, 'hello', 3.14, 1)\n",
        "print(f\"Original tuple: {my_tuple}\")\n",
        "\n",
        "# 2. Accessing elements: O(1)\n",
        "first_element_tuple = my_tuple[0]\n",
        "last_element_tuple = my_tuple[-1]\n",
        "print(f\"First element: {first_element_tuple}, Last element: {last_element_tuple}\")\n",
        "\n",
        "# 3. Slicing: O(k) where k is the length of the slice\n",
        "sub_tuple = my_tuple[1:3]\n",
        "print(f\"Sliced tuple (index 1 to 2): {sub_tuple}\")\n",
        "\n",
        "# 4. Attempting to modify (will raise TypeError): Immutability\n",
        "# try:\n",
        "#     my_tuple[0] = 5  # This line would cause an error\n",
        "# except TypeError as e:\n",
        "#     print(f\"Attempted modification error: {e}\")\n",
        "\n",
        "# 5. Checking membership (in operator): O(N) in worst case\n",
        "is_hello_present = 'hello' in my_tuple\n",
        "is_world_present = 'world' in my_tuple\n",
        "print(f\"'hello' in tuple? {is_hello_present}\")\n",
        "print(f\"'world' in tuple? {is_world_present}\")\n",
        "\n",
        "# 6. Iteration: O(N)\n",
        "print(\"Iterating through tuple:\")\n",
        "for item in my_tuple:\n",
        "    print(f\"Item: {item}\")\n",
        "\n",
        "# 7. Concatenation: O(N+M) where N and M are lengths of the tuples\n",
        "new_tuple = my_tuple + ('world', 5)\n",
        "print(f\"After concatenation: {new_tuple}\")\n",
        "\n",
        "# 8. Repetition: O(N*k) where k is the repetition factor\n",
        "repeated_tuple = my_tuple * 2\n",
        "print(f\"After repetition: {repeated_tuple}\")\n",
        "\n",
        "# 9. Tuple unpacking: O(1)\n",
        "a, b, c, d = my_tuple\n",
        "print(f\"Unpacked elements: a={a}, b={b}, c={c}, d={d}\")\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Tuples (Immutable, Ordered, Allows Duplicates) ---\n",
            "Original tuple: (1, 'hello', 3.14, 1)\n",
            "First element: 1, Last element: 1\n",
            "Sliced tuple (index 1 to 2): ('hello', 3.14)\n",
            "'hello' in tuple? True\n",
            "'world' in tuple? False\n",
            "Iterating through tuple:\n",
            "Item: 1\n",
            "Item: hello\n",
            "Item: 3.14\n",
            "Item: 1\n",
            "After concatenation: (1, 'hello', 3.14, 1, 'world', 5)\n",
            "After repetition: (1, 'hello', 3.14, 1, 1, 'hello', 3.14, 1)\n",
            "Unpacked elements: a=1, b=hello, c=3.14, d=1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "96110a5a",
        "outputId": "b6b2a06b-d380-4b73-f6f5-ddf40412900c"
      },
      "source": [
        "print('\\n--- Sets (Mutable, Unordered, Unique Elements) ---')\n",
        "\n",
        "# 1. Creation: O(N) where N is the number of elements in the iterable\n",
        "my_set = {1, 2, 3, 2, 4}\n",
        "print(f\"Original set (duplicates removed): {my_set}\")\n",
        "\n",
        "# Create from a list\n",
        "my_list_for_set = [3, 4, 5, 5]\n",
        "another_set = set(my_list_for_set)\n",
        "print(f\"Set created from list: {another_set}\")\n",
        "\n",
        "# 2. Adding elements: Amortized O(1)\n",
        "my_set.add(5)\n",
        "my_set.add(1) # Adding an existing element has no effect\n",
        "print(f\"After adding 5 and 1: {my_set}\")\n",
        "\n",
        "# 3. Removing elements:\n",
        "#    a. remove(item): Amortized O(1), raises KeyError if item not found\n",
        "try:\n",
        "    my_set.remove(1)\n",
        "    print(f\"After removing 1: {my_set}\")\n",
        "    # my_set.remove(10) # Uncommenting this would raise KeyError\n",
        "except KeyError:\n",
        "    print(\"10 not found in set (KeyError handled)\")\n",
        "\n",
        "#    b. discard(item): Amortized O(1), no error if item not found\n",
        "my_set.discard(2)\n",
        "my_set.discard(10) # No error\n",
        "print(f\"After discarding 2 and 10: {my_set}\")\n",
        "\n",
        "#    c. pop(): Removes and returns an arbitrary element. Amortized O(1)\n",
        "if my_set:\n",
        "    popped_element = my_set.pop()\n",
        "    print(f\"Popped arbitrary element: {popped_element}, Set now: {my_set}\")\n",
        "\n",
        "# 4. Checking membership (in operator): Average O(1), Worst O(N) due to hash collisions\n",
        "is_three_in_set = 3 in my_set\n",
        "is_one_in_set = 1 in my_set\n",
        "print(f\"3 in set? {is_three_in_set}, 1 in set? {is_one_in_set}\")\n",
        "\n",
        "# 5. Iteration: O(N)\n",
        "print(\"Iterating through set:\")\n",
        "for item in my_set:\n",
        "    print(f\"Item: {item}\")\n",
        "\n",
        "# 6. Set operations:\n",
        "set1 = {1, 2, 3, 4}\n",
        "set2 = {3, 4, 5, 6}\n",
        "\n",
        "#    a. Union: O(|set1| + |set2|)\n",
        "union_set = set1.union(set2)\n",
        "print(f\"Union of {{set1}} and {{set2}}: {union_set}\")\n",
        "\n",
        "#    b. Intersection: O(min(|set1|, |set2|))\n",
        "intersection_set = set1.intersection(set2)\n",
        "print(f\"Intersection of {{set1}} and {{set2}}: {intersection_set}\")\n",
        "\n",
        "#    c. Difference: O(|set1|)\n",
        "difference_set = set1.difference(set2)\n",
        "print(f\"Difference of {{set1}} and {{set2}}: {difference_set}\")\n",
        "\n",
        "#    d. Symmetric Difference: O(|set1| + |set2|)\n",
        "symmetric_difference_set = set1.symmetric_difference(set2)\n",
        "print(f\"Symmetric Difference of {{set1}} and {{set2}}: {symmetric_difference_set}\")\n",
        "\n",
        "# 7. Check subset/superset:\n",
        "subset_check = {3, 4}.issubset(set1) # O(|subset|) to check\n",
        "superset_check = set1.issuperset({3, 4}) # O(|superset|) to check\n",
        "print(f\"{{3, 4}} is subset of set1? {subset_check}\")\n",
        "print(f\"set1 is superset of {{3, 4}}? {superset_check}\")\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Sets (Mutable, Unordered, Unique Elements) ---\n",
            "Original set (duplicates removed): {1, 2, 3, 4}\n",
            "Set created from list: {3, 4, 5}\n",
            "After adding 5 and 1: {1, 2, 3, 4, 5}\n",
            "After removing 1: {2, 3, 4, 5}\n",
            "After discarding 2 and 10: {3, 4, 5}\n",
            "Popped arbitrary element: 3, Set now: {4, 5}\n",
            "3 in set? False, 1 in set? False\n",
            "Iterating through set:\n",
            "Item: 4\n",
            "Item: 5\n",
            "Union of {set1} and {set2}: {1, 2, 3, 4, 5, 6}\n",
            "Intersection of {set1} and {set2}: {3, 4}\n",
            "Difference of {set1} and {set2}: {1, 2}\n",
            "Symmetric Difference of {set1} and {set2}: {1, 2, 5, 6}\n",
            "{3, 4} is subset of set1? True\n",
            "set1 is superset of {3, 4}? True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "551cfdc4",
        "outputId": "2ed946bb-b36f-439d-d36e-c78eaf9c64b8"
      },
      "source": [
        "print('\\n--- Dictionaries (Mutable, Unordered*, Key-Value Pairs) ---')\n",
        "\n",
        "# 1. Creation: O(N) where N is the number of key-value pairs\n",
        "# *As of Python 3.7, insertion order is preserved\n",
        "my_dict = {'name': 'Alice', 'age': 30, 'city': 'New York'}\n",
        "print(f\"Original dictionary: {my_dict}\")\n",
        "\n",
        "# 2. Accessing values: Average O(1), Worst O(N) due to hash collisions\n",
        "name = my_dict['name']\n",
        "print(f\"Name: {name}\")\n",
        "\n",
        "# Using .get() method: Average O(1), Worst O(N). Returns None if key not found.\n",
        "country = my_dict.get('country')\n",
        "print(f\"Country (using .get()): {country}\")\n",
        "country_default = my_dict.get('country', 'USA')\n",
        "print(f\"Country with default (using .get()): {country_default}\")\n",
        "\n",
        "# 3. Adding/Modifying elements: Average O(1), Worst O(N)\n",
        "my_dict['occupation'] = 'Data Scientist'\n",
        "my_dict['age'] = 31 # Modifying an existing value\n",
        "print(f\"After adding 'occupation' and modifying 'age': {my_dict}\")\n",
        "\n",
        "# 4. Removing elements:\n",
        "#    a. del my_dict[key]: Average O(1), Worst O(N). Raises KeyError if key not found.\n",
        "try:\n",
        "    del my_dict['city']\n",
        "    print(f\"After del 'city': {my_dict}\")\n",
        "    # del my_dict['nonexistent_key'] # Uncommenting this would raise KeyError\n",
        "except KeyError:\n",
        "    print(\"Nonexistent key not found in dict (KeyError handled)\")\n",
        "\n",
        "#    b. pop(key): Average O(1), Worst O(N). Removes and returns value. Raises KeyError if key not found.\n",
        "popped_occupation = my_dict.pop('occupation')\n",
        "print(f\"Popped occupation: {popped_occupation}, Dict now: {my_dict}\")\n",
        "\n",
        "#    c. popitem(): Average O(1), Worst O(N). Removes and returns an arbitrary (last inserted prior to Python 3.7) key-value pair.\n",
        "if my_dict:\n",
        "    popped_item = my_dict.popitem()\n",
        "    print(f\"Popped item: {popped_item}, Dict now: {my_dict}\")\n",
        "\n",
        "# 5. Checking key membership (in operator): Average O(1), Worst O(N)\n",
        "is_name_present = 'name' in my_dict\n",
        "is_salary_present = 'salary' in my_dict\n",
        "print(f\"'name' in dict? {is_name_present}, 'salary' in dict? {is_salary_present}\")\n",
        "\n",
        "# 6. Iteration: O(N) for N key-value pairs\n",
        "print(\"Iterating through dictionary keys:\")\n",
        "for key in my_dict:\n",
        "    print(f\"Key: {key}, Value: {my_dict[key]}\")\n",
        "\n",
        "print(\"Iterating through dictionary values:\")\n",
        "for value in my_dict.values():\n",
        "    print(f\"Value: {value}\")\n",
        "\n",
        "print(\"Iterating through dictionary items (key-value pairs):\")\n",
        "for key, value in my_dict.items():\n",
        "    print(f\"Key: {key}, Value: {value}\")\n",
        "\n",
        "# 7. Clearing dictionary: O(N)\n",
        "my_dict.clear()\n",
        "print(f\"After clearing dictionary: {my_dict}\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Dictionaries (Mutable, Unordered*, Key-Value Pairs) ---\n",
            "Original dictionary: {'name': 'Alice', 'age': 30, 'city': 'New York'}\n",
            "Name: Alice\n",
            "Country (using .get()): None\n",
            "Country with default (using .get()): USA\n",
            "After adding 'occupation' and modifying 'age': {'name': 'Alice', 'age': 31, 'city': 'New York', 'occupation': 'Data Scientist'}\n",
            "After del 'city': {'name': 'Alice', 'age': 31, 'occupation': 'Data Scientist'}\n",
            "Popped occupation: Data Scientist, Dict now: {'name': 'Alice', 'age': 31}\n",
            "Popped item: ('age', 31), Dict now: {'name': 'Alice'}\n",
            "'name' in dict? True, 'salary' in dict? False\n",
            "Iterating through dictionary keys:\n",
            "Key: name, Value: Alice\n",
            "Iterating through dictionary values:\n",
            "Value: Alice\n",
            "Iterating through dictionary items (key-value pairs):\n",
            "Key: name, Value: Alice\n",
            "After clearing dictionary: {}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d8d91abf"
      },
      "source": [
        "## String Manipulation Techniques\n",
        "\n",
        "Strings are sequences of characters and are one of the most commonly used data types in Python. Python provides a rich set of built-in methods and the `re` module for powerful string manipulation.\n",
        "\n",
        "### Common String Methods\n",
        "\n",
        "- **Concatenation:** Joining two or more strings. `new_string = str1 + str2` (O(M+N) for strings of length M and N)\n",
        "- **Indexing and Slicing:** Accessing individual characters or substrings. `my_string[0]`, `my_string[1:5]` (O(1) for index, O(k) for slice of length k)\n",
        "- **`len()`:** Returns the length of a string. (O(1))\n",
        "- **`strip()` / `lstrip()` / `rstrip()`:** Removes leading/trailing whitespace or specified characters. (O(N))\n",
        "- **`lower()` / `upper()`:** Converts string to lowercase/uppercase. (O(N))\n",
        "- **`replace(old, new)`:** Replaces all occurrences of a substring. (O(N*M) where N is string length, M is `old` length)\n",
        "- **`split(separator)`:** Splits a string into a list of substrings based on a delimiter. (O(N))\n",
        "- **`join(iterable)`:** Joins elements of an iterable (e.g., list of strings) into a single string using the string itself as a separator. (O(N) where N is total length of strings to be joined)\n",
        "- **`find(substring)` / `index(substring)`:** Returns the lowest index of the substring. `find` returns -1 if not found, `index` raises an error. (O(N*M))\n",
        "- **`startswith(prefix)` / `endswith(suffix)`:** Checks if the string starts/ends with a specified prefix/suffix. (O(M) where M is length of prefix/suffix)\n",
        "- **`count(substring)`:** Returns the number of occurrences of a substring. (O(N*M))\n",
        "- **`isdigit()` / `isalpha()` / `isalnum()`:** Checks if all characters in the string are digits, alphabetic, or alphanumeric respectively. (O(N))\n",
        "\n",
        "### Basic Regular Expressions (`re` module)\n",
        "\n",
        "Regular expressions (regex) are powerful tools for pattern matching within strings. The `re` module in Python provides operations for working with regular expressions.\n",
        "\n",
        "**Key functions:**\n",
        "\n",
        "- **`re.search(pattern, string)`:** Scans through a string looking for the first location where the regular expression pattern produces a match. Returns a match object if successful, `None` otherwise. (O(N*M) worst case, N=string length, M=pattern length)\n",
        "- **`re.match(pattern, string)`:** Attempts to match the pattern to the beginning of the string. Returns a match object if successful, `None` otherwise. (O(M) worst case)\n",
        "- **`re.findall(pattern, string)`:** Returns all non-overlapping matches of pattern in string, as a list of strings. (O(N*M) worst case)\n",
        "- **`re.sub(pattern, repl, string)`:** Replaces occurrences of a pattern in a string with a replacement string. (O(N*M) worst case)\n",
        "\n",
        "**Basic Regex Patterns:**\n",
        "\n",
        "- `.` : Any character (except newline)\n",
        "- `*` : Zero or more occurrences of the preceding character/group\n",
        "- `+` : One or more occurrences of the preceding character/group\n",
        "- `?` : Zero or one occurrence of the preceding character/group\n",
        "- `^` : Matches the beginning of the string\n",
        "- `$` : Matches the end of the string\n",
        "- `\\d` : Matches any digit (0-9)\n",
        "- `\\w` : Matches any word character (alphanumeric + underscore)\n",
        "- `\\s` : Matches any whitespace character\n",
        "- `[abc]` : Matches any one of the characters a, b, or c\n",
        "- `[^abc]` : Matches any character NOT in a, b, or c\n",
        "- `[a-z]` : Matches any lowercase letter\n",
        "- `(pattern)` : Groups patterns together"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b7b4978c",
        "outputId": "bc8192ad-c1a5-4284-9403-5debe2734a68"
      },
      "source": [
        "import re\n",
        "\n",
        "print('--- String Manipulation Techniques ---')\n",
        "\n",
        "# 1. Concatenation: O(M+N)\n",
        "str1 = \"Hello\"\n",
        "str2 = \" World\"\n",
        "concatenated_str = str1 + str2\n",
        "print(f\"Concatenation ('{str1}' + '{str2}'): '{concatenated_str}'\")\n",
        "\n",
        "# 2. Indexing and Slicing: O(1) for index, O(k) for slice\n",
        "my_string = \"Python Programming\"\n",
        "first_char = my_string[0]\n",
        "substring = my_string[7:18] # 'Programming'\n",
        "print(f\"First char: '{first_char}', Substring: '{substring}'\")\n",
        "\n",
        "# 3. len(): O(1)\n",
        "length = len(my_string)\n",
        "print(f\"Length of '{my_string}': {length}\")\n",
        "\n",
        "# 4. strip(), lstrip(), rstrip(): O(N)\n",
        "whitespace_str = \"  Hello Python   \"\n",
        "stripped_str = whitespace_str.strip()\n",
        "print(f\"Original: '{whitespace_str}', Stripped: '{stripped_str}'\")\n",
        "\n",
        "# 5. lower(), upper(): O(N)\n",
        "upper_str = my_string.upper()\n",
        "lower_str = my_string.lower()\n",
        "print(f\"Uppercase: '{upper_str}', Lowercase: '{lower_str}'\")\n",
        "\n",
        "# 6. replace(old, new): O(N*M)\n",
        "replaced_str = my_string.replace(\"Programming\", \"Development\")\n",
        "print(f\"Replaced: '{replaced_str}'\")\n",
        "\n",
        "# 7. split(separator): O(N)\n",
        "words = my_string.split(\" \")\n",
        "print(f\"Split into words: {words}\")\n",
        "\n",
        "# 8. join(iterable): O(N) where N is total length of strings\n",
        "joined_str = \"-\".join(words)\n",
        "print(f\"Joined words: '{joined_str}'\")\n",
        "\n",
        "# 9. find(substring) / index(substring): O(N*M)\n",
        "find_result = my_string.find(\"Pro\")\n",
        "index_result = my_string.index(\"gram\")\n",
        "print(f\"'Pro' found at index: {find_result}, 'gram' found at index: {index_result}\")\n",
        "\n",
        "# 10. startswith(prefix) / endswith(suffix): O(M)\n",
        "starts_with_py = my_string.startswith(\"Python\")\n",
        "ends_with_ing = my_string.endswith(\"ing\")\n",
        "print(f\"Starts with 'Python'? {starts_with_py}, Ends with 'ing'? {ends_with_ing}\")\n",
        "\n",
        "# 11. count(substring): O(N*M)\n",
        "a_count = my_string.count('P')\n",
        "print(f\"'P' appears {a_count} times in '{my_string}'\")\n",
        "\n",
        "# 12. isdigit(), isalpha(), isalnum(): O(N)\n",
        "digit_check = \"123\".isdigit()\n",
        "alph_check = \"Python\".isalpha()\n",
        "alnum_check = \"Pyth0n1\".isalnum()\n",
        "print(f\"'123' is digit? {digit_check}, 'Python' is alpha? {alph_check}, 'Pyth0n1' is alnum? {alnum_check}\")\n",
        "\n",
        "print('\\n--- Basic Regular Expressions (re module) ---')\n",
        "\n",
        "text = \"The quick brown fox jumps over the lazy dog. The number is 12345.\"\n",
        "\n",
        "# re.search(pattern, string): O(N*M) worst case\n",
        "match = re.search(r\"fox\", text)\n",
        "if match:\n",
        "    print(f\"'fox' found at index {match.start()} using re.search\")\n",
        "\n",
        "# re.match(pattern, string): O(M) worst case\n",
        "match_start = re.match(r\"The\", text)\n",
        "if match_start:\n",
        "    print(f\"'The' matches at start using re.match\")\n",
        "else:\n",
        "    print(\"'The' does not match at start using re.match (example of failed match)\")\n",
        "\n",
        "match_python_start = re.match(r\"Python\", text)\n",
        "if match_python_start:\n",
        "    print(f\"'Python' matches at start using re.match\")\n",
        "else:\n",
        "    print(\"'Python' does not match at start using re.match\")\n",
        "\n",
        "# re.findall(pattern, string): O(N*M) worst case\n",
        "all_the = re.findall(r\"The\", text)\n",
        "print(f\"All occurrences of 'The': {all_the}\")\n",
        "\n",
        "digits = re.findall(r\"\\d+\", text) # + for one or more digits\n",
        "print(f\"All digits: {digits}\")\n",
        "\n",
        "# re.sub(pattern, repl, string): O(N*M) worst case\n",
        "replaced_text = re.sub(r\"fox\", \"cat\", text)\n",
        "print(f\"Replaced 'fox' with 'cat': {replaced_text}\")\n",
        "\n",
        "hash_tag_text = \"This is a #test with #multiple #hashtags.\"\n",
        "cleaned_hashtags = re.sub(r\"#\\w+\", \"[TAG]\", hash_tag_text) # \\w+ for one or more word characters\n",
        "print(f\"Cleaned hashtags: {cleaned_hashtags}\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- String Manipulation Techniques ---\n",
            "Concatenation ('Hello' + ' World'): 'Hello World'\n",
            "First char: 'P', Substring: 'Programming'\n",
            "Length of 'Python Programming': 18\n",
            "Original: '  Hello Python   ', Stripped: 'Hello Python'\n",
            "Uppercase: 'PYTHON PROGRAMMING', Lowercase: 'python programming'\n",
            "Replaced: 'Python Development'\n",
            "Split into words: ['Python', 'Programming']\n",
            "Joined words: 'Python-Programming'\n",
            "'Pro' found at index: 7, 'gram' found at index: 10\n",
            "Starts with 'Python'? True, Ends with 'ing'? True\n",
            "'P' appears 2 times in 'Python Programming'\n",
            "'123' is digit? True, 'Python' is alpha? True, 'Pyth0n1' is alnum? True\n",
            "\n",
            "--- Basic Regular Expressions (re module) ---\n",
            "'fox' found at index 16 using re.search\n",
            "'The' matches at start using re.match\n",
            "'Python' does not match at start using re.match\n",
            "All occurrences of 'The': ['The', 'The']\n",
            "All digits: ['12345']\n",
            "Replaced 'fox' with 'cat': The quick brown cat jumps over the lazy dog. The number is 12345.\n",
            "Cleaned hashtags: This is a [TAG] with [TAG] [TAG].\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "793bcfcc"
      },
      "source": [
        "## List Comprehensions and Lambda Functions\n",
        "\n",
        "### List Comprehensions\n",
        "\n",
        "List comprehensions provide a concise way to create lists. It consists of brackets containing an expression followed by a `for` clause, then zero or more `for` or `if` clauses. The result will be a new list resulting from evaluating the expression in the context of the `for` and `if` clauses which follow it.\n",
        "\n",
        "**Syntax:** `[expression for item in iterable if condition]`\n",
        "\n",
        "**Key Benefits:**\n",
        "- **Conciseness:** Often reduces several lines of code to a single line.\n",
        "- **Readability:** When used appropriately, they can make code easier to understand than traditional loops.\n",
        "- **Efficiency:** Generally faster than traditional `for` loops for creating lists, as they are optimized at the C level in Python.\n",
        "\n",
        "**Typical Applications:**\n",
        "- Creating new lists based on existing iterables.\n",
        "- Filtering elements from an iterable.\n",
        "- Transforming elements in an iterable.\n",
        "- Flattening lists of lists.\n",
        "\n",
        "**Complexity Analysis:** Generally O(N) where N is the number of elements in the iterable, as each element is processed once. Filtering or complex transformations within the expression might add constant factors but usually don't change the asymptotic complexity.\n",
        "\n",
        "### Lambda Functions (Anonymous Functions)\n",
        "\n",
        "Lambda functions are small, anonymous functions defined with the `lambda` keyword. They can take any number of arguments but can only have one expression. They are often used for short, throwaway functions that are not intended to be reused.\n",
        "\n",
        "**Syntax:** `lambda arguments: expression`\n",
        "\n",
        "**Key Benefits:**\n",
        "- **Conciseness:** Ideal for functions that require a single expression.\n",
        "- **Inline Use:** Can be defined and used immediately, often as arguments to higher-order functions (e.g., `map()`, `filter()`, `sorted()`).\n",
        "\n",
        "**Typical Applications:**\n",
        "- Simple arithmetic operations.\n",
        "- Sorting custom objects based on a specific key.\n",
        "- Filtering data based on a simple condition.\n",
        "- As callback functions in GUI programming.\n",
        "\n",
        "**Complexity Analysis:** The execution complexity of a lambda function itself is determined by the complexity of its single expression, usually O(1) for simple operations. When passed to functions like `map` or `filter`, the overall complexity will depend on the higher-order function's behavior (e.g., O(N) for `map` over N elements)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a353fb89",
        "outputId": "5f8b1075-3968-486e-f19f-e10bfdcacd50"
      },
      "source": [
        "print('--- List Comprehensions ---')\n",
        "\n",
        "# 1. Basic list comprehension: O(N)\n",
        "numbers = [1, 2, 3, 4, 5]\n",
        "squared_numbers = [x * x for x in numbers]\n",
        "print(f\"Original numbers: {numbers}\")\n",
        "print(f\"Squared numbers (list comprehension): {squared_numbers}\")\n",
        "\n",
        "# 2. List comprehension with a condition (filtering): O(N)\n",
        "even_numbers = [x for x in numbers if x % 2 == 0]\n",
        "print(f\"Even numbers: {even_numbers}\")\n",
        "\n",
        "# 3. List comprehension with nested loops (flattening): O(M*N)\n",
        "matrix = [[1, 2], [3, 4], [5, 6]]\n",
        "flattened_list = [num for row in matrix for num in row]\n",
        "print(f\"Matrix: {matrix}\")\n",
        "print(f\"Flattened list: {flattened_list}\")\n",
        "\n",
        "# 4. List comprehension with a transformation and condition: O(N)\n",
        "words = [\"apple\", \"banana\", \"cherry\", \"date\"]\n",
        "long_words_upper = [word.upper() for word in words if len(word) > 5]\n",
        "print(f\"Words: {words}\")\n",
        "print(f\"Long words (upper): {long_words_upper}\")\n",
        "\n",
        "print('\\n--- Lambda Functions ---')\n",
        "\n",
        "# 1. Simple lambda for addition: O(1)\n",
        "add_two = lambda x: x + 2\n",
        "print(f\"Lambda (add_two) applied to 5: {add_two(5)}\")\n",
        "\n",
        "# 2. Lambda with multiple arguments: O(1)\n",
        "multiply = lambda x, y: x * y\n",
        "print(f\"Lambda (multiply) applied to 3, 4: {multiply(3, 4)}\")\n",
        "\n",
        "# 3. Using lambda with map(): O(N)\n",
        "# Applies a function to all items in an input list\n",
        "numbers_map = [1, 2, 3, 4, 5]\n",
        "squared_map = list(map(lambda x: x * x, numbers_map))\n",
        "print(f\"Original numbers (map): {numbers_map}\")\n",
        "print(f\"Squared numbers (map with lambda): {squared_map}\")\n",
        "\n",
        "# 4. Using lambda with filter(): O(N)\n",
        "# Filters items from an iterable based on a function\n",
        "numbers_filter = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
        "even_numbers_filter = list(filter(lambda x: x % 2 == 0, numbers_filter))\n",
        "print(f\"Original numbers (filter): {numbers_filter}\")\n",
        "print(f\"Even numbers (filter with lambda): {even_numbers_filter}\")\n",
        "\n",
        "# 5. Using lambda with sorted() for custom sorting: O(N log N)\n",
        "students = [('Alice', 20), ('Bob', 25), ('Charlie', 18)]\n",
        "sorted_students_by_age = sorted(students, key=lambda student: student[1])\n",
        "print(f\"Original students: {students}\")\n",
        "print(f\"Sorted students by age: {sorted_students_by_age}\")\n",
        "\n",
        "print('\\n--- When to use (Performance Considerations) ---')\n",
        "print(\"List Comprehensions are generally preferred for creating new lists, filtering, or transforming elements due to their readability and often better performance than explicit for loops.\")\n",
        "print(\"Lambda Functions are best for small, anonymous functions used as arguments to higher-order functions (like map, filter, sorted) or for quick, single-expression operations. Avoid using them for complex logic or when a standard named function would improve clarity.\")\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- List Comprehensions ---\n",
            "Original numbers: [1, 2, 3, 4, 5]\n",
            "Squared numbers (list comprehension): [1, 4, 9, 16, 25]\n",
            "Even numbers: [2, 4]\n",
            "Matrix: [[1, 2], [3, 4], [5, 6]]\n",
            "Flattened list: [1, 2, 3, 4, 5, 6]\n",
            "Words: ['apple', 'banana', 'cherry', 'date']\n",
            "Long words (upper): ['BANANA', 'CHERRY']\n",
            "\n",
            "--- Lambda Functions ---\n",
            "Lambda (add_two) applied to 5: 7\n",
            "Lambda (multiply) applied to 3, 4: 12\n",
            "Original numbers (map): [1, 2, 3, 4, 5]\n",
            "Squared numbers (map with lambda): [1, 4, 9, 16, 25]\n",
            "Original numbers (filter): [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
            "Even numbers (filter with lambda): [2, 4, 6, 8, 10]\n",
            "Original students: [('Alice', 20), ('Bob', 25), ('Charlie', 18)]\n",
            "Sorted students by age: [('Charlie', 18), ('Alice', 20), ('Bob', 25)]\n",
            "\n",
            "--- When to use (Performance Considerations) ---\n",
            "List Comprehensions are generally preferred for creating new lists, filtering, or transforming elements due to their readability and often better performance than explicit for loops.\n",
            "Lambda Functions are best for small, anonymous functions used as arguments to higher-order functions (like map, filter, sorted) or for quick, single-expression operations. Avoid using them for complex logic or when a standard named function would improve clarity.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "12b41fb9"
      },
      "source": [
        "## Quick Fundamentals Review - HackerRank Practice\n",
        "\n",
        "### Subtask:\n",
        "Suggest 2-3 'Easy' HackerRank problems that reinforce Python fundamentals, provide problem names or direct links, and advise on efficient problem-solving, code clarity, conciseness, and optimal solutions. Also, encourage leveraging the provided GitHub repository for solution patterns.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "418f88da"
      },
      "source": [
        "### HackerRank Practice: Reinforcing Python Fundamentals\n",
        "\n",
        "Now that we've reviewed the core Python fundamentals, it's time to apply these concepts to practical problems. HackerRank is an excellent platform for honing your coding skills. We'll start with 'Easy' level problems to solidify your understanding.\n",
        "\n",
        "#### Suggested HackerRank Problems (Easy):\n",
        "\n",
        "1.  **\"Python If-Else\"**\n",
        "    *   **Concept Reinforcement:** Conditional statements (`if`, `elif`, `else`).\n",
        "    *   **Link:** [https://www.hackerrank.com/challenges/py-if-else/problem](https://www.hackerrank.com/challenges/py-if-else/problem)\n",
        "\n",
        "2.  **\"List Comprehensions\"**\n",
        "    *   **Concept Reinforcement:** List comprehensions, basic iteration.\n",
        "    *   **Link:** [https://www.hackerrank.com/challenges/list-comprehensions/problem](https://www.hackerrank.com/challenges/list-comprehensions/problem)\n",
        "\n",
        "3.  **\"Finding the Percentage\"**\n",
        "    *   **Concept Reinforcement:** Dictionaries, lists, basic arithmetic, data parsing.\n",
        "    *   **Link:** [https://www.hackerrank.com/challenges/finding-the-percentage/problem](https://www.hackerrank.com/challenges/finding-the-percentage/problem)\n",
        "\n",
        "#### Problem-Solving Guidance:\n",
        "\n",
        "When tackling these problems, keep the following in mind:\n",
        "\n",
        "*   **Read Carefully:** Understand the problem statement, input format, and output format thoroughly.\n",
        "*   **Plan Your Approach:** Before coding, think about the most straightforward way to solve the problem using the Python fundamentals you've just reviewed.\n",
        "*   **Code Clarity and Conciseness:** Write code that is easy to read and understand. Utilize Pythonic features like list comprehensions where appropriate to make your code concise.\n",
        "*   **Optimal Solutions:** For 'Easy' problems, initial solutions might not always require complex optimization. However, always consider the time and space complexity of your approach. Can you solve it more efficiently? Even for easy problems, striving for O(1) or O(N) where possible is a good habit.\n",
        "*   **Test Your Code:** Use the provided sample test cases and think of your own edge cases.\n",
        "\n",
        "#### Leveraging the GitHub Repository:\n",
        "\n",
        "After you've attempted these problems yourself and ideally passed the test cases, you can refer to the following GitHub repository for alternative solutions, different approaches, and best practices:\n",
        "\n",
        "*   **GitHub Repository:** [https://github.com/hevalhazalkurt/Hackerrank_Python_Solutions](https://github.com/hevalhazalkurt/Hackerrank_Python_Solutions)\n",
        "\n",
        "This resource can be invaluable for learning solution patterns and understanding how experienced developers approach similar problems."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "05059a9a"
      },
      "source": [
        "## Algorithm Essentials - Core Algorithms & Complexity\n",
        "\n",
        "### Subtask:\n",
        "Cover essential sorting algorithms: Merge Sort and Quick Sort. Provide clear explanations, illustrative Python code examples (as Colab snippets), and detailed time and space complexity analysis for both.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ec84782"
      },
      "source": [
        "### Merge Sort\n",
        "\n",
        "**Concept:** Merge Sort is a highly efficient, general-purpose, comparison-based sorting algorithm. It's a classic example of a **divide-and-conquer** algorithm.\n",
        "\n",
        "**How it Works:**\n",
        "1.  **Divide:** The algorithm recursively divides the unsorted list into `n` sublists, each containing one element (a list of one element is considered sorted).\n",
        "2.  **Conquer (Sort):** Repeatedly merge sublists to produce new sorted sublists until there is only one sorted list remaining. The core operation is the `merge` step, which takes two sorted sublists and merges them into a single sorted list.\n",
        "\n",
        "**Time Complexity:**\n",
        "*   **Best Case:** O(N log N)\n",
        "*   **Average Case:** O(N log N)\n",
        "*   **Worst Case:** O(N log N)\n",
        "    *   The `log N` factor comes from the number of times the list can be divided in half. There are `log N` levels of recursion.\n",
        "    *   The `N` factor comes from the merging step. At each level of recursion, merging all sublists requires approximately `N` comparisons and data movements.\n",
        "\n",
        "**Space Complexity:**\n",
        "*   **Worst Case:** O(N)\n",
        "    *   This is because an auxiliary array of size `N` is typically used during the merging process to temporarily store elements.\n",
        "\n",
        "### Quick Sort\n",
        "\n",
        "**Concept:** Quick Sort is also a highly efficient, comparison-based sorting algorithm that uses the **divide-and-conquer** paradigm. It is often faster in practice than Merge Sort, though its worst-case performance is worse.\n",
        "\n",
        "**How it Works:**\n",
        "1.  **Pivot Selection:** The algorithm first selects a 'pivot' element from the array. The choice of pivot is critical for performance; common strategies include choosing the first, last, middle, or a random element.\n",
        "2.  **Partitioning:** It partitions the array around the pivot. All elements smaller than the pivot are moved to its left, and all elements greater than the pivot are moved to its right. Elements equal to the pivot can go on either side. After partitioning, the pivot is in its final sorted position.\n",
        "3.  **Conquer (Recursion):** The algorithm then recursively sorts the subarrays on the left and right of the pivot.\n",
        "\n",
        "**Time Complexity:**\n",
        "*   **Best Case:** O(N log N) (when pivot always divides the array into two roughly equal halves)\n",
        "*   **Average Case:** O(N log N)\n",
        "*   **Worst Case:** O(N^2) (occurs when the pivot selection consistently results in highly unbalanced partitions, e.g., always picking the smallest or largest element, leading to one subarray with `N-1` elements and another with `0` elements). However, with good pivot selection strategies (like median-of-three or random pivot), the worst-case is rare.\n",
        "\n",
        "**Space Complexity:**\n",
        "*   **Average Case:** O(log N)\n",
        "    *   This is due to the recursion stack space required. In a balanced partition scenario, the depth of the recursion tree is `log N`.\n",
        "*   **Worst Case:** O(N)\n",
        "    *   Occurs during unbalanced partitions, where the recursion depth can go up to `N`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "06d1039b",
        "outputId": "04c75456-5de0-4519-f8dc-16ad3a72cc58"
      },
      "source": [
        "import random\n",
        "\n",
        "def merge_sort(arr):\n",
        "    \"\"\"Merge Sort implementation.\n",
        "    Time Complexity: O(N log N)\n",
        "    Space Complexity: O(N)\n",
        "    \"\"\"\n",
        "    # Base case: if the list has 0 or 1 element, it's already sorted. O(1)\n",
        "    if len(arr) <= 1:\n",
        "        return arr\n",
        "\n",
        "    # Divide: Find the middle point and divide the list into two halves. O(1)\n",
        "    mid = len(arr) // 2\n",
        "    left_half = arr[:mid]\n",
        "    right_half = arr[mid:]\n",
        "\n",
        "    # Conquer: Recursively sort both halves. (2 * T(N/2))\n",
        "    left_sorted = merge_sort(left_half)\n",
        "    right_sorted = merge_sort(right_half)\n",
        "\n",
        "    # Combine: Merge the sorted halves. O(N)\n",
        "    return _merge(left_sorted, right_sorted)\n",
        "\n",
        "def _merge(left, right):\n",
        "    \"\"\"Helper function to merge two sorted lists.\n",
        "    Time Complexity: O(N) where N is total elements in left + right\n",
        "    Space Complexity: O(N) for the result list\n",
        "    \"\"\"\n",
        "    merged = []\n",
        "    i = j = 0\n",
        "\n",
        "    # Compare elements from both lists and append the smaller one. O(N) comparisons\n",
        "    while i < len(left) and j < len(right):\n",
        "        if left[i] < right[j]:\n",
        "            merged.append(left[i])\n",
        "            i += 1\n",
        "        else:\n",
        "            merged.append(right[j])\n",
        "            j += 1\n",
        "\n",
        "    # Append any remaining elements. O(N) in total\n",
        "    while i < len(left):\n",
        "        merged.append(left[i])\n",
        "        i += 1\n",
        "    while j < len(right):\n",
        "        merged.append(right[j])\n",
        "        j += 1\n",
        "\n",
        "    return merged\n",
        "\n",
        "def quick_sort(arr):\n",
        "    \"\"\"Quick Sort implementation (in-place).\n",
        "    Average Time Complexity: O(N log N)\n",
        "    Worst Time Complexity: O(N^2)\n",
        "    Average Space Complexity: O(log N) (recursion stack)\n",
        "    Worst Space Complexity: O(N) (recursion stack)\n",
        "    \"\"\"\n",
        "    # Use a helper function for in-place sorting\n",
        "    _quick_sort_helper(arr, 0, len(arr) - 1)\n",
        "    return arr\n",
        "\n",
        "def _quick_sort_helper(arr, low, high):\n",
        "    \"\"\"Recursive helper for Quick Sort.\"\"\"\n",
        "    # Base case: if the sub-array has 0 or 1 element, it's already sorted. O(1)\n",
        "    if low < high:\n",
        "        # Partition the array and get the pivot index. O(N)\n",
        "        pivot_idx = _partition(arr, low, high)\n",
        "\n",
        "        # Recursively sort the sub-arrays before and after the pivot.\n",
        "        # Average: 2 * T(N/2), Worst: T(N-1) + T(0)\n",
        "        _quick_sort_helper(arr, low, pivot_idx - 1)\n",
        "        _quick_sort_helper(arr, pivot_idx + 1, high)\n",
        "\n",
        "def _partition(arr, low, high):\n",
        "    \"\"\"Helper function to partition the array around a pivot (Hoare's partition scheme).\n",
        "    Time Complexity: O(N) where N is high - low + 1\n",
        "    Space Complexity: O(1)\n",
        "    \"\"\"\n",
        "    # Choose a pivot (here, we pick the last element, but random or median-of-three is better).\n",
        "    # O(1)\n",
        "    pivot = arr[high]\n",
        "    i = low - 1  # Index of smaller element\n",
        "\n",
        "    for j in range(low, high): # Iterate through elements O(N)\n",
        "        # If current element is smaller than or equal to pivot\n",
        "        if arr[j] <= pivot:\n",
        "            i += 1\n",
        "            # Swap elements O(1)\n",
        "            arr[i], arr[j] = arr[j], arr[i]\n",
        "\n",
        "    # Swap pivot to its correct position O(1)\n",
        "    arr[i + 1], arr[high] = arr[high], arr[i + 1]\n",
        "    return i + 1\n",
        "\n",
        "# --- Demonstration ---\n",
        "print(\"--- Sorting Algorithm Demonstration ---\")\n",
        "\n",
        "# Example unsorted list\n",
        "unsorted_list = [38, 27, 43, 3, 9, 82, 10, 1, 56]\n",
        "print(f\"Original List: {unsorted_list}\")\n",
        "\n",
        "# Demonstrate Merge Sort\n",
        "# Create a copy to preserve the original list for Quick Sort\n",
        "list_for_merge_sort = list(unsorted_list)\n",
        "merge_sorted_list = merge_sort(list_for_merge_sort)\n",
        "print(f\"Merge Sorted List: {merge_sorted_list}\")\n",
        "\n",
        "# Demonstrate Quick Sort\n",
        "# Create another copy\n",
        "list_for_quick_sort = list(unsorted_list)\n",
        "quick_sorted_list = quick_sort(list_for_quick_sort)\n",
        "print(f\"Quick Sorted List: {quick_sorted_list}\")\n",
        "\n",
        "# Edge case: Empty list\n",
        "empty_list = []\n",
        "print(f\"\\nEmpty List (original): {empty_list}\")\n",
        "print(f\"Empty List (Merge Sort): {merge_sort(list(empty_list))}\")\n",
        "print(f\"Empty List (Quick Sort): {quick_sort(list(empty_list))}\")\n",
        "\n",
        "# Edge case: Already sorted list\n",
        "already_sorted = [1, 2, 3, 4, 5]\n",
        "print(f\"\\nAlready Sorted List (original): {already_sorted}\")\n",
        "print(f\"Already Sorted List (Merge Sort): {merge_sort(list(already_sorted))}\")\n",
        "print(f\"Already Sorted List (Quick Sort): {quick_sort(list(already_sorted))}\")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Sorting Algorithm Demonstration ---\n",
            "Original List: [38, 27, 43, 3, 9, 82, 10, 1, 56]\n",
            "Merge Sorted List: [1, 3, 9, 10, 27, 38, 43, 56, 82]\n",
            "Quick Sorted List: [1, 3, 9, 10, 27, 38, 43, 56, 82]\n",
            "\n",
            "Empty List (original): []\n",
            "Empty List (Merge Sort): []\n",
            "Empty List (Quick Sort): []\n",
            "\n",
            "Already Sorted List (original): [1, 2, 3, 4, 5]\n",
            "Already Sorted List (Merge Sort): [1, 2, 3, 4, 5]\n",
            "Already Sorted List (Quick Sort): [1, 2, 3, 4, 5]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "db0e6556"
      },
      "source": [
        "## Algorithm Essentials - Core Algorithms & Complexity\n",
        "\n",
        "### Subtask:\n",
        "Cover essential searching algorithms (Binary Search), the two-pointer technique, and the sliding window technique. Provide clear explanations, illustrative Python code examples (as Colab snippets), and detailed time and space complexity analysis for each.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1255723e"
      },
      "source": [
        "```markdown\n",
        "### Binary Search\n",
        "\n",
        "**Concept:** Binary Search is an efficient algorithm for finding an item from a **sorted** list of items. It works by repeatedly dividing in half the portion of the list that could contain the item, until you've narrowed down the possible locations to just one.\n",
        "\n",
        "**How it Works:**\n",
        "1.  **Start:** Begin with an interval covering the whole list.\n",
        "2.  **Middle Element:** If the value of the search key is less than the item in the middle of the interval, narrow the interval to the lower half.\n",
        "3.  **Middle Element:** Otherwise, narrow it to the upper half.\n",
        "4.  **Repeat:** Repeatedly check until the value is found or the interval is empty.\n",
        "\n",
        "**Typical Applications:**\n",
        "-   Searching in sorted arrays or lists.\n",
        "-   Finding the first or last occurrence of an element.\n",
        "-   Finding an element closest to a given value.\n",
        "-   Applications where elements are ordered, like finding a specific page in a sorted book.\n",
        "\n",
        "**Time Complexity:**\n",
        "*   **Worst Case:** O(log N)\n",
        "*   **Average Case:** O(log N)\n",
        "*   **Best Case:** O(1) (when the middle element is the target)\n",
        "    *   Each step of the binary search algorithm halves the search space. This logarithmic behavior leads to very efficient searches, especially for large datasets.\n",
        "\n",
        "**Space Complexity:**\n",
        "*   **Iterative:** O(1) (constant space, as only a few variables are used).\n",
        "*   **Recursive:** O(log N) (due to the recursion stack depth).\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b59a42ea",
        "outputId": "b4615c99-8f26-418a-aafb-7e360f578147"
      },
      "source": [
        "def binary_search_iterative(arr, target):\n",
        "    \"\"\"Iterative Binary Search implementation.\n",
        "    Time Complexity: O(log N)\n",
        "    Space Complexity: O(1)\n",
        "    \"\"\"\n",
        "    low = 0  # O(1)\n",
        "    high = len(arr) - 1  # O(1)\n",
        "\n",
        "    while low <= high:  # Loop runs log N times\n",
        "        mid = (low + high) // 2  # O(1)\n",
        "        mid_val = arr[mid]  # O(1)\n",
        "\n",
        "        if mid_val == target:  # O(1)\n",
        "            return mid\n",
        "        elif mid_val < target:  # O(1)\n",
        "            low = mid + 1\n",
        "        else:  # mid_val > target # O(1)\n",
        "            high = mid - 1\n",
        "    return -1 # O(1) - target not found\n",
        "\n",
        "def binary_search_recursive(arr, target, low, high):\n",
        "    \"\"\"Recursive Binary Search implementation.\n",
        "    Time Complexity: O(log N)\n",
        "    Space Complexity: O(log N) due to recursion stack\n",
        "    \"\"\"\n",
        "    if low > high:  # Base case: target not found. O(1)\n",
        "        return -1\n",
        "\n",
        "    mid = (low + high) // 2  # O(1)\n",
        "    mid_val = arr[mid]  # O(1)\n",
        "\n",
        "    if mid_val == target:  # O(1)\n",
        "        return mid\n",
        "    elif mid_val < target:  # O(1)\n",
        "        return binary_search_recursive(arr, target, mid + 1, high)\n",
        "    else:  # mid_val > target # O(1)\n",
        "        return binary_search_recursive(arr, target, low, mid - 1)\n",
        "\n",
        "# --- Demonstration ---\n",
        "print(\"--- Binary Search Demonstration ---\")\n",
        "\n",
        "sorted_list = [2, 5, 8, 12, 16, 23, 38, 56, 72, 91]\n",
        "print(f\"Sorted list: {sorted_list}\")\n",
        "\n",
        "# Test iterative binary search\n",
        "target1 = 23\n",
        "result1 = binary_search_iterative(sorted_list, target1)\n",
        "print(f\"Iterative search for {target1}: {'Found at index ' + str(result1) if result1 != -1 else 'Not found'}\")\n",
        "\n",
        "target2 = 10\n",
        "result2 = binary_search_iterative(sorted_list, target2)\n",
        "print(f\"Iterative search for {target2}: {'Found at index ' + str(result2) if result2 != -1 else 'Not found'}\")\n",
        "\n",
        "# Test recursive binary search\n",
        "target3 = 5\n",
        "result3 = binary_search_recursive(sorted_list, target3, 0, len(sorted_list) - 1)\n",
        "print(f\"Recursive search for {target3}: {'Found at index ' + str(result3) if result3 != -1 else 'Not found'}\")\n",
        "\n",
        "target4 = 100\n",
        "result4 = binary_search_recursive(sorted_list, target4, 0, len(sorted_list) - 1)\n",
        "print(f\"Recursive search for {target4}: {'Found at index ' + str(result4) if result4 != -1 else 'Not found'}\")\n",
        "\n",
        "# Edge cases\n",
        "empty_list = []\n",
        "print(f\"\\nEmpty list: {empty_list}\")\n",
        "print(f\"Search in empty list (target=5): {binary_search_iterative(empty_list, 5)}\")\n",
        "\n",
        "single_element_list = [7]\n",
        "print(f\"Single element list: {single_element_list}\")\n",
        "print(f\"Search for 7 in single element list: {binary_search_iterative(single_element_list, 7)}\")\n",
        "print(f\"Search for 1 in single element list: {binary_search_iterative(single_element_list, 1)}\")\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Binary Search Demonstration ---\n",
            "Sorted list: [2, 5, 8, 12, 16, 23, 38, 56, 72, 91]\n",
            "Iterative search for 23: Found at index 5\n",
            "Iterative search for 10: Not found\n",
            "Recursive search for 5: Found at index 1\n",
            "Recursive search for 100: Not found\n",
            "\n",
            "Empty list: []\n",
            "Search in empty list (target=5): -1\n",
            "Single element list: [7]\n",
            "Search for 7 in single element list: 0\n",
            "Search for 1 in single element list: -1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e13d11d0"
      },
      "source": [
        "### Two-Pointer Technique\n",
        "\n",
        "**Concept:** The Two-Pointer Technique is a powerful and frequently used algorithmic pattern, particularly effective for problems involving sorted arrays or linked lists. It involves using two pointers (indices or references) that traverse the data structure from different positions (e.g., from both ends, or both from the beginning but at different paces) to solve problems efficiently.\n",
        "\n",
        "**How it Works:**\n",
        "There are generally two main variants:\n",
        "1.  **Pointers moving towards each other:** One pointer starts at the beginning of the data structure and the other at the end. They move inward based on certain conditions until they meet or cross. This is often used to find pairs, triplets, or check properties in a sorted array.\n",
        "2.  **Pointers moving in the same direction:** Both pointers start at the same end (usually the beginning) and move forward, potentially at different speeds. This is useful for problems like finding subarrays, removing duplicates, or processing sliding windows.\n",
        "\n",
        "**Typical Applications:**\n",
        "-   Finding pairs with a certain sum in a sorted array (e.g., Two Sum problem).\n",
        "-   Reversing an array or string in-place.\n",
        "-   Removing duplicates from a sorted array.\n",
        "-   Checking if a string is a palindrome.\n",
        "-   Merging two sorted arrays.\n",
        "-   Finding a specific subarray or subsequence.\n",
        "\n",
        "**Complexity Analysis:**\n",
        "-   **Time Complexity:** The Two-Pointer Technique often reduces problems that might otherwise require O(N^2) or O(N log N) solutions to O(N). This is because each pointer typically traverses the data structure at most once.\n",
        "-   **Space Complexity:** Usually O(1) as it only requires a few variables to store the pointers, making it very memory-efficient."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ff644b8a",
        "outputId": "e0aeadff-752d-4a8c-a4e6-a9b8c131597a"
      },
      "source": [
        "print('--- Two-Pointer Technique Demonstration ---')\n",
        "\n",
        "# 1. Two Pointers moving towards each other (e.g., Two Sum problem in a sorted array)\n",
        "def two_sum_sorted(arr, target):\n",
        "    \"\"\"Finds two numbers in a sorted array that add up to a target.\n",
        "    Time Complexity: O(N) - pointers traverse the array at most once.\n",
        "    Space Complexity: O(1) - constant extra space.\n",
        "    \"\"\"\n",
        "    left, right = 0, len(arr) - 1 # O(1)\n",
        "\n",
        "    while left < right: # O(N) in worst case\n",
        "        current_sum = arr[left] + arr[right] # O(1)\n",
        "        if current_sum == target: # O(1)\n",
        "            return [left, right]\n",
        "        elif current_sum < target: # O(1)\n",
        "            left += 1\n",
        "        else: # current_sum > target # O(1)\n",
        "            right -= 1\n",
        "    return [-1, -1] # O(1) - not found\n",
        "\n",
        "print(\"\\n--- Two Pointers: Moving Towards Each Other (Two Sum) ---\")\n",
        "sorted_nums = [2, 7, 11, 15, 18, 20]\n",
        "target_sum = 29\n",
        "indices = two_sum_sorted(sorted_nums, target_sum)\n",
        "print(f\"Array: {sorted_nums}, Target Sum: {target_sum}\")\n",
        "if indices[0] != -1:\n",
        "    print(f\"Indices of numbers that sum to {target_sum}: {indices} (Values: {sorted_nums[indices[0]]}, {sorted_nums[indices[1]]})\")\n",
        "else:\n",
        "    print(\"No two numbers found that sum to the target.\")\n",
        "\n",
        "target_sum_not_found = 10\n",
        "indices_not_found = two_sum_sorted(sorted_nums, target_sum_not_found)\n",
        "print(f\"Array: {sorted_nums}, Target Sum: {target_sum_not_found}\")\n",
        "if indices_not_found[0] != -1:\n",
        "    print(f\"Indices of numbers that sum to {target_sum_not_found}: {indices_not_found}\")\n",
        "else:\n",
        "    print(\"No two numbers found that sum to the target.\")\n",
        "\n",
        "\n",
        "# 2. Two Pointers moving in the same direction (e.g., Removing duplicates from a sorted array in-place)\n",
        "def remove_duplicates_in_place(arr):\n",
        "    \"\"\"Removes duplicates from a sorted array in-place and returns the new length.\n",
        "    Time Complexity: O(N) - each pointer traverses the array at most once.\n",
        "    Space Complexity: O(1) - constant extra space.\n",
        "    \"\"\"\n",
        "    if not arr: # O(1)\n",
        "        return 0\n",
        "\n",
        "    i = 0  # 'slow' pointer - tracks the position for the next unique element # O(1)\n",
        "    # j is the 'fast' pointer - iterates through the array # O(1)\n",
        "    for j in range(1, len(arr)): # O(N) in worst case\n",
        "        if arr[j] != arr[i]: # O(1)\n",
        "            i += 1 # O(1)\n",
        "            arr[i] = arr[j] # O(1)\n",
        "    return i + 1 # O(1) - new length\n",
        "\n",
        "print(\"\\n--- Two Pointers: Moving in Same Direction (Remove Duplicates) ---\")\n",
        "duplicate_nums = [0, 0, 1, 1, 1, 2, 2, 3, 3, 4]\n",
        "original_len = len(duplicate_nums)\n",
        "new_len = remove_duplicates_in_place(duplicate_nums)\n",
        "print(f\"Original array: {duplicate_nums[:original_len]}\")\n",
        "print(f\"Array after removing duplicates (new length {new_len}): {duplicate_nums[:new_len]}\")\n",
        "\n",
        "duplicate_nums_2 = [1, 1, 2]\n",
        "original_len_2 = len(duplicate_nums_2)\n",
        "new_len_2 = remove_duplicates_in_place(duplicate_nums_2)\n",
        "print(f\"Original array: {duplicate_nums_2[:original_len_2]}\")\n",
        "print(f\"Array after removing duplicates (new length {new_len_2}): {duplicate_nums_2[:new_len_2]}\")\n",
        "\n",
        "# Example: Checking for Palindrome\n",
        "def is_palindrome(s):\n",
        "    \"\"\"Checks if a string is a palindrome using two pointers.\n",
        "    Time Complexity: O(N) - pointers traverse the string at most once.\n",
        "    Space Complexity: O(1) - constant extra space.\n",
        "    \"\"\"\n",
        "    left, right = 0, len(s) - 1 # O(1)\n",
        "\n",
        "    while left < right: # O(N) in worst case\n",
        "        if s[left] != s[right]: # O(1)\n",
        "            return False\n",
        "        left += 1 # O(1)\n",
        "        right -= 1 # O(1)\n",
        "    return True # O(1)\n",
        "\n",
        "print(\"\\n--- Two Pointers: Moving Towards Each Other (Palindrome Check) ---\")\n",
        "str1 = \"madam\"\n",
        "str2 = \"hello\"\n",
        "print(f\"'{str1}' is a palindrome? {is_palindrome(str1)}\")\n",
        "print(f\"'{str2}' is a palindrome? {is_palindrome(str2)}\")\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Two-Pointer Technique Demonstration ---\n",
            "\n",
            "--- Two Pointers: Moving Towards Each Other (Two Sum) ---\n",
            "Array: [2, 7, 11, 15, 18, 20], Target Sum: 29\n",
            "Indices of numbers that sum to 29: [2, 4] (Values: 11, 18)\n",
            "Array: [2, 7, 11, 15, 18, 20], Target Sum: 10\n",
            "No two numbers found that sum to the target.\n",
            "\n",
            "--- Two Pointers: Moving in Same Direction (Remove Duplicates) ---\n",
            "Original array: [0, 1, 2, 3, 4, 2, 2, 3, 3, 4]\n",
            "Array after removing duplicates (new length 5): [0, 1, 2, 3, 4]\n",
            "Original array: [1, 2, 2]\n",
            "Array after removing duplicates (new length 2): [1, 2]\n",
            "\n",
            "--- Two Pointers: Moving Towards Each Other (Palindrome Check) ---\n",
            "'madam' is a palindrome? True\n",
            "'hello' is a palindrome? False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6de0df72"
      },
      "source": [
        "### Sliding Window Technique\n",
        "\n",
        "**Concept:** The Sliding Window Technique is an optimization pattern used to find subarrays or substrings that satisfy certain conditions, often involving contiguous data. It transforms a nested loop into a single loop by maintaining a \"window\" (a range of elements) that slides over the data structure. Instead of re-evaluating the entire window at each step, it efficiently updates the window's state by removing elements from one end and adding elements to the other.\n",
        "\n",
        "**How it Works:**\n",
        "1.  **Initialize:** Define a window (usually by `start` and `end` pointers or indices) and calculate the initial state (e.g., sum, count, frequency map) for the first window.\n",
        "2.  **Slide:** Move the `end` pointer to expand the window to the right, incorporating the new element into the window's state.\n",
        "3.  **Shrink (Optional):** If the window violates a given condition (e.g., sum exceeds a limit, window size is too large), move the `start` pointer to the right to shrink the window, adjusting the window's state by removing the element leaving the window.\n",
        "4.  **Update Result:** At each step (or when the window satisfies a condition), update the result with the current window's properties.\n",
        "5.  **Repeat:** Continue sliding and shrinking until the `end` pointer reaches the end of the data structure.\n",
        "\n",
        "**Typical Applications:**\n",
        "-   Finding the maximum/minimum sum subarray of a fixed size.\n",
        "-   Finding the longest/shortest subarray/substring with a certain property.\n",
        "-   Finding the number of subarrays/substrings that meet a condition.\n",
        "-   Problems involving frequency counts of characters/elements within a range.\n",
        "\n",
        "**Complexity Analysis:**\n",
        "-   **Time Complexity:** Typically O(N), where N is the length of the array or string. This is because both the `start` and `end` pointers traverse the data structure at most once.\n",
        "-   **Space Complexity:** Usually O(1) or O(K) where K is the size of a auxiliary data structure (e.g., a hash map for frequency counts), often much better than O(N) approaches."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "34ceaaae",
        "outputId": "95324de5-98b5-4b23-b61d-75ca132e7cc1"
      },
      "source": [
        "print('--- Sliding Window Technique Demonstration ---')\n",
        "\n",
        "# 1. Fixed-Size Sliding Window (e.g., Maximum sum subarray of size k)\n",
        "def max_sum_subarray(arr, k):\n",
        "    \"\"\"Finds the maximum sum of a subarray of fixed size k.\n",
        "    Time Complexity: O(N) - the window slides through the array once.\n",
        "    Space Complexity: O(1) - constant extra space.\n",
        "    \"\"\"\n",
        "    if k > len(arr):\n",
        "        return -1 # Or raise an error, or return 0 depending on problem statement\n",
        "\n",
        "    window_sum = sum(arr[:k]) # O(k) for initial sum\n",
        "    max_sum = window_sum\n",
        "\n",
        "    for i in range(k, len(arr)): # O(N-k) iterations\n",
        "        window_sum += arr[i] - arr[i-k] # Slide window: add new, subtract old. O(1)\n",
        "        max_sum = max(max_sum, window_sum) # Update max sum. O(1)\n",
        "    return max_sum\n",
        "\n",
        "print(\"\\n--- Fixed-Size Sliding Window (Max Sum Subarray) ---\")\n",
        "nums1 = [1, 4, 2, 10, 2, 3, 1, 0, 20]\n",
        "k1 = 4\n",
        "print(f\"Array: {nums1}, Window Size k: {k1}, Max Sum Subarray: {max_sum_subarray(nums1, k1)}\") # Expected: 24 (10 + 2 + 3 + 1)\n",
        "\n",
        "nums2 = [2, 3, 4, 1, 5]\n",
        "k2 = 3\n",
        "print(f\"Array: {nums2}, Window Size k: {k2}, Max Sum Subarray: {max_sum_subarray(nums2, k2)}\") # Expected: 9 (3 + 4 + 1 or 4 + 1 + 5)\n",
        "\n",
        "\n",
        "# 2. Dynamic-Size Sliding Window (e.g., Smallest subarray with sum >= target)\n",
        "def smallest_subarray_with_sum(arr, target_sum):\n",
        "    \"\"\"Finds the length of the smallest contiguous subarray whose sum is greater than or equal to target_sum.\n",
        "    Time Complexity: O(N) - both pointers traverse the array at most once.\n",
        "    Space Complexity: O(1) - constant extra space.\n",
        "    \"\"\"\n",
        "    min_len = float('inf')\n",
        "    window_start = 0\n",
        "    window_sum = 0\n",
        "\n",
        "    for window_end in range(len(arr)): # Outer loop moves window_end pointer: O(N)\n",
        "        window_sum += arr[window_end] # Add current element to window_sum. O(1)\n",
        "\n",
        "        # Shrink the window if current window_sum meets/exceeds target_sum\n",
        "        while window_sum >= target_sum: # Inner loop moves window_start pointer. Amortized O(1) per outer loop iteration\n",
        "            min_len = min(min_len, window_end - window_start + 1) # Update min_len. O(1)\n",
        "            window_sum -= arr[window_start] # Subtract element leaving window. O(1)\n",
        "            window_start += 1 # Move window_start. O(1)\n",
        "\n",
        "    if min_len == float('inf'):\n",
        "        return 0 # No such subarray found\n",
        "    return min_len\n",
        "\n",
        "print(\"\\n--- Dynamic-Size Sliding Window (Smallest Subarray with Sum >= Target) ---\")\n",
        "nums3 = [2, 1, 5, 2, 3, 2]\n",
        "target3 = 7\n",
        "print(f\"Array: {nums3}, Target Sum: {target3}, Smallest Subarray Length: {smallest_subarray_with_sum(nums3, target3)}\") # Expected: 2 (e.g., [5, 2])\n",
        "\n",
        "nums4 = [3, 4, 1, 1, 6]\n",
        "target4 = 8\n",
        "print(f\"Array: {nums4}, Target Sum: {target4}, Smallest Subarray Length: {smallest_subarray_with_sum(nums4, target4)}\") # Expected: 3 (e.g., [3, 4, 1] or [1, 1, 6])\n",
        "\n",
        "nums5 = [1, 1, 1, 1, 1]\n",
        "target5 = 10\n",
        "print(f\"Array: {nums5}, Target Sum: {target5}, Smallest Subarray Length: {smallest_subarray_with_sum(nums5, target5)}\") # Expected: 0\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Sliding Window Technique Demonstration ---\n",
            "\n",
            "--- Fixed-Size Sliding Window (Max Sum Subarray) ---\n",
            "Array: [1, 4, 2, 10, 2, 3, 1, 0, 20], Window Size k: 4, Max Sum Subarray: 24\n",
            "Array: [2, 3, 4, 1, 5], Window Size k: 3, Max Sum Subarray: 10\n",
            "\n",
            "--- Dynamic-Size Sliding Window (Smallest Subarray with Sum >= Target) ---\n",
            "Array: [2, 1, 5, 2, 3, 2], Target Sum: 7, Smallest Subarray Length: 2\n",
            "Array: [3, 4, 1, 1, 6], Target Sum: 8, Smallest Subarray Length: 3\n",
            "Array: [1, 1, 1, 1, 1], Target Sum: 10, Smallest Subarray Length: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8cd3d0fd"
      },
      "source": [
        "## Algorithm Essentials - Core Algorithms & Complexity\n",
        "\n",
        "### Subtask:\n",
        "Cover essential algorithms: graph traversal (BFS, DFS), and dynamic programming basics. For each algorithm, provide clear explanations, illustrative Python code examples (as Colab snippets), and detailed time and space complexity analysis.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6f4314ba"
      },
      "source": [
        "### Breadth-First Search (BFS)\n",
        "\n",
        "**Concept:** Breadth-First Search (BFS) is an algorithm for traversing or searching tree or graph data structures. It starts at the tree root (or some arbitrary node of a graph, sometimes referred to as a 'search key' or 'goal node') and explores all of the neighbor nodes at the present depth prior to moving on to the nodes at the next depth level.\n",
        "\n",
        "**How it Works:**\n",
        "1.  **Start Node:** Pick a starting node (source) and add it to a queue.\n",
        "2.  **Visit Neighbors:** While the queue is not empty:\n",
        "    a.  Dequeue a node.\n",
        "    b.  Mark it as visited.\n",
        "    c.  Process the node (e.g., print it, check if it's the target).\n",
        "    d.  Enqueue all its unvisited neighbors.\n",
        "3.  **Repeat:** Continue until the queue is empty.\n",
        "\n",
        "**Typical Applications:**\n",
        "-   Finding the shortest path in an unweighted graph.\n",
        "-   Web crawlers.\n",
        "-   Social networking path finding.\n",
        "-   Broadcasting on a network.\n",
        "-   Finding connected components in a graph.\n",
        "\n",
        "**Time Complexity:**\n",
        "*   **O(V + E)**, where V is the number of vertices (nodes) and E is the number of edges. Each vertex and each edge is visited at most once.\n",
        "\n",
        "**Space Complexity:**\n",
        "*   **O(V)**, primarily for storing the queue and the visited set. In the worst case, all vertices might be stored in the queue."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "27cfb4ab",
        "outputId": "a14f4183-4e77-4560-c6b1-3642e20613cf"
      },
      "source": [
        "from collections import deque\n",
        "\n",
        "def bfs(graph, start_node):\n",
        "    \"\"\"Breadth-First Search (BFS) implementation.\n",
        "    Time Complexity: O(V + E) - where V is the number of vertices and E is the number of edges.\n",
        "                     Each vertex is enqueued/dequeued once, and each edge is examined once.\n",
        "    Space Complexity: O(V) - for the queue and the visited set in the worst case (e.g., a star graph).\n",
        "    \"\"\"\n",
        "    visited = set()  # Stores visited nodes. O(V) space.\n",
        "    queue = deque()  # Stores nodes to visit. O(V) space.\n",
        "\n",
        "    # Start BFS from the start_node\n",
        "    visited.add(start_node) # O(1)\n",
        "    queue.append(start_node) # O(1)\n",
        "\n",
        "    bfs_traversal_order = [] # To store the order of visited nodes\n",
        "\n",
        "    while queue: # Loop runs until all reachable nodes are visited, each node enqueued/dequeued once (O(V))\n",
        "        current_node = queue.popleft() # Dequeue node. O(1)\n",
        "        bfs_traversal_order.append(current_node) # O(1)\n",
        "\n",
        "        # Explore neighbors of the current_node\n",
        "        for neighbor in graph.get(current_node, []): # Each edge is examined once in total across all node traversals (O(E))\n",
        "            if neighbor not in visited: # O(1) on average for set lookup\n",
        "                visited.add(neighbor) # O(1) on average\n",
        "                queue.append(neighbor) # O(1)\n",
        "\n",
        "    return bfs_traversal_order\n",
        "\n",
        "# --- Demonstration ---\n",
        "print(\"--- BFS Demonstration ---\")\n",
        "\n",
        "# Example Graph (Adjacency List Representation)\n",
        "#   A -- B\n",
        "#  / \\ /  \\\n",
        "# C   D -- E\n",
        "#     |    |\n",
        "#     F    G\n",
        "graph = {\n",
        "    'A': ['B', 'C', 'D'],\n",
        "    'B': ['A', 'D', 'E'],\n",
        "    'C': ['A'],\n",
        "    'D': ['A', 'B', 'E', 'F'],\n",
        "    'E': ['B', 'D', 'G'],\n",
        "    'F': ['D'],\n",
        "    'G': ['E']\n",
        "}\n",
        "\n",
        "start_node_bfs = 'A'\n",
        "print(f\"Graph: {graph}\")\n",
        "print(f\"Starting BFS from node '{start_node_bfs}':\")\n",
        "bfs_result = bfs(graph, start_node_bfs)\n",
        "print(f\"BFS Traversal Order: {bfs_result}\")\n",
        "\n",
        "# Example with a disconnected graph or a node with no outgoing edges\n",
        "graph_disconnected = {\n",
        "    '0': ['1', '2'],\n",
        "    '1': ['2'],\n",
        "    '2': ['0', '3'],\n",
        "    '3': ['3'],\n",
        "    '4': ['5'], # Disconnected component\n",
        "    '5': ['4']\n",
        "}\n",
        "\n",
        "start_node_disconnected = '0'\n",
        "print(f\"\\nGraph (disconnected): {graph_disconnected}\")\n",
        "print(f\"Starting BFS from node '{start_node_disconnected}':\")\n",
        "bfs_result_disconnected = bfs(graph_disconnected, start_node_disconnected)\n",
        "print(f\"BFS Traversal Order: {bfs_result_disconnected}\")\n",
        "\n",
        "start_node_isolated = '4'\n",
        "print(f\"\\nStarting BFS from node '{start_node_isolated}' in the disconnected graph:\")\n",
        "bfs_result_isolated = bfs(graph_disconnected, start_node_isolated)\n",
        "print(f\"BFS Traversal Order: {bfs_result_isolated}\")\n",
        "\n",
        "# Edge case: Empty graph\n",
        "empty_graph = {}\n",
        "start_node_empty = 'A'\n",
        "print(f\"\\nEmpty Graph: {empty_graph}\")\n",
        "print(f\"Starting BFS from node '{start_node_empty}':\")\n",
        "bfs_result_empty = bfs(empty_graph, start_node_empty)\n",
        "print(f\"BFS Traversal Order: {bfs_result_empty}\")\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- BFS Demonstration ---\n",
            "Graph: {'A': ['B', 'C', 'D'], 'B': ['A', 'D', 'E'], 'C': ['A'], 'D': ['A', 'B', 'E', 'F'], 'E': ['B', 'D', 'G'], 'F': ['D'], 'G': ['E']}\n",
            "Starting BFS from node 'A':\n",
            "BFS Traversal Order: ['A', 'B', 'C', 'D', 'E', 'F', 'G']\n",
            "\n",
            "Graph (disconnected): {'0': ['1', '2'], '1': ['2'], '2': ['0', '3'], '3': ['3'], '4': ['5'], '5': ['4']}\n",
            "Starting BFS from node '0':\n",
            "BFS Traversal Order: ['0', '1', '2', '3']\n",
            "\n",
            "Starting BFS from node '4' in the disconnected graph:\n",
            "BFS Traversal Order: ['4', '5']\n",
            "\n",
            "Empty Graph: {}\n",
            "Starting BFS from node 'A':\n",
            "BFS Traversal Order: ['A']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58dfc1f5"
      },
      "source": [
        "### Depth-First Search (DFS)\n",
        "\n",
        "**Concept:** Depth-First Search (DFS) is an algorithm for traversing or searching tree or graph data structures. It explores as far as possible along each branch before backtracking. It's often implemented using recursion or an explicit stack.\n",
        "\n",
        "**How it Works:**\n",
        "1.  **Start Node:** Pick a starting node (source) and add it to a stack (or call the recursive function with it).\n",
        "2.  **Visit and Explore:** While the stack is not empty (or as long as the recursive call stack isn't empty):\n",
        "    a.  Pop a node (or process the current node in recursion).\n",
        "    b.  If it has not been visited, mark it as visited and process it (e.g., print it, check if it's the target).\n",
        "    c.  Push (or recursively visit) all its unvisited neighbors onto the stack (or make recursive calls for them). The order of pushing neighbors can vary, affecting the traversal order.\n",
        "3.  **Backtrack:** When a path cannot be extended further (a node has no unvisited neighbors), the algorithm backtracks to the previous node and explores other branches.\n",
        "\n",
        "**Typical Applications:**\n",
        "-   Detecting cycles in a graph.\n",
        "-   Pathfinding (e.g., finding *a* path between two nodes, not necessarily the shortest).\n",
        "-   Topological sorting.\n",
        "-   Finding connected components (if the graph is directed) or strongly connected components.\n",
        "-   Solving puzzles with a single solution (e.g., mazes).\n",
        "\n",
        "**Time Complexity:**\n",
        "*   **O(V + E)**, where V is the number of vertices (nodes) and E is the number of edges. Each vertex and each edge is visited at most once.\n",
        "\n",
        "**Space Complexity:**\n",
        "*   **O(V)**, primarily for the recursion stack (in a recursive implementation) or an explicit stack (in an iterative implementation) and the visited set. In the worst case (e.g., a long linear graph), the depth of the recursion can be V."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c13484df",
        "outputId": "91ba0774-4fe5-4193-8379-afad474aab68"
      },
      "source": [
        "def dfs_recursive(graph, start_node, visited=None, traversal_order=None):\n",
        "    \"\"\"Recursive Depth-First Search (DFS) implementation.\n",
        "    Time Complexity: O(V + E) - where V is the number of vertices and E is the number of edges.\n",
        "                     Each vertex is visited once, and each edge is examined once.\n",
        "    Space Complexity: O(V) - for the recursion stack (in worst case, e.g., a path graph) and the visited set.\n",
        "    \"\"\"\n",
        "    if visited is None:\n",
        "        visited = set()\n",
        "    if traversal_order is None:\n",
        "        traversal_order = []\n",
        "\n",
        "    visited.add(start_node) # O(1) on average\n",
        "    traversal_order.append(start_node) # O(1)\n",
        "\n",
        "    for neighbor in graph.get(start_node, []): # Each edge is examined once in total across all node traversals (O(E))\n",
        "        if neighbor not in visited: # O(1) on average for set lookup\n",
        "            dfs_recursive(graph, neighbor, visited, traversal_order)\n",
        "\n",
        "    return traversal_order\n",
        "\n",
        "def dfs_iterative(graph, start_node):\n",
        "    \"\"\"Iterative Depth-First Search (DFS) implementation using a stack.\n",
        "    Time Complexity: O(V + E) - where V is the number of vertices and E is the number of edges.\n",
        "                     Each vertex is pushed/popped once, and each edge is examined once.\n",
        "    Space Complexity: O(V) - for the explicit stack and the visited set.\n",
        "    \"\"\"\n",
        "    visited = set() # Stores visited nodes. O(V) space.\n",
        "    stack = []      # Stores nodes to visit. O(V) space.\n",
        "    traversal_order = [] # To store the order of visited nodes\n",
        "\n",
        "    stack.append(start_node) # O(1)\n",
        "\n",
        "    while stack: # Loop runs until all reachable nodes are visited, each node pushed/popped once (O(V))\n",
        "        current_node = stack.pop() # Pop node from stack. O(1)\n",
        "\n",
        "        if current_node not in visited: # O(1) on average for set lookup\n",
        "            visited.add(current_node) # O(1) on average\n",
        "            traversal_order.append(current_node) # O(1)\n",
        "\n",
        "            # Push unvisited neighbors onto the stack\n",
        "            # Note: For consistent output with recursive DFS (which often processes neighbors in declaration order),\n",
        "            # we push them in reverse order so that the first neighbor is processed last (LIFO).\n",
        "            for neighbor in reversed(graph.get(current_node, [])): # Each edge is examined once in total (O(E))\n",
        "                if neighbor not in visited:\n",
        "                    stack.append(neighbor) # O(1)\n",
        "\n",
        "    return traversal_order\n",
        "\n",
        "# --- Demonstration ---\n",
        "print(\"--- DFS Demonstration ---\")\n",
        "\n",
        "# Example Graph (Adjacency List Representation)\n",
        "#   A -- B\n",
        "#  / \\ /  \\\n",
        "# C   D -- E\n",
        "#     |    |\n",
        "#     F    G\n",
        "graph = {\n",
        "    'A': ['B', 'C', 'D'],\n",
        "    'B': ['A', 'D', 'E'],\n",
        "    'C': ['A'],\n",
        "    'D': ['A', 'B', 'E', 'F'],\n",
        "    'E': ['B', 'D', 'G'],\n",
        "    'F': ['D'],\n",
        "    'G': ['E']\n",
        "}\n",
        "\n",
        "start_node_dfs = 'A'\n",
        "print(f\"Graph: {graph}\")\n",
        "print(f\"Starting Recursive DFS from node '{start_node_dfs}':\")\n",
        "dfs_recursive_result = dfs_recursive(graph, start_node_dfs)\n",
        "print(f\"Recursive DFS Traversal Order: {dfs_recursive_result}\")\n",
        "\n",
        "print(f\"\\nStarting Iterative DFS from node '{start_node_dfs}':\")\n",
        "dfs_iterative_result = dfs_iterative(graph, start_node_dfs)\n",
        "print(f\"Iterative DFS Traversal Order: {dfs_iterative_result}\")\n",
        "\n",
        "# Example with a disconnected graph\n",
        "graph_disconnected = {\n",
        "    '0': ['1', '2'],\n",
        "    '1': ['2'],\n",
        "    '2': ['0', '3'],\n",
        "    '3': ['3'],\n",
        "    '4': ['5'], # Disconnected component\n",
        "    '5': ['4']\n",
        "}\n",
        "\n",
        "start_node_disconnected = '0'\n",
        "print(f\"\\nGraph (disconnected): {graph_disconnected}\")\n",
        "print(f\"Starting Recursive DFS from node '{start_node_disconnected}':\")\n",
        "dfs_recursive_disconnected = dfs_recursive(graph_disconnected, start_node_disconnected)\n",
        "print(f\"Recursive DFS Traversal Order: {dfs_recursive_disconnected}\")\n",
        "\n",
        "start_node_isolated = '4'\n",
        "print(f\"\\nStarting Iterative DFS from node '{start_node_isolated}' in the disconnected graph:\")\n",
        "dfs_iterative_isolated = dfs_iterative(graph_disconnected, start_node_isolated)\n",
        "print(f\"Iterative DFS Traversal Order: {dfs_iterative_isolated}\")\n",
        "\n",
        "# Edge case: Empty graph\n",
        "empty_graph = {}\n",
        "start_node_empty = 'A'\n",
        "print(f\"\\nEmpty Graph: {empty_graph}\")\n",
        "print(f\"Starting Recursive DFS from node '{start_node_empty}':\")\n",
        "dfs_recursive_empty = dfs_recursive(empty_graph, start_node_empty)\n",
        "print(f\"Recursive DFS Traversal Order: {dfs_recursive_empty}\")"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- DFS Demonstration ---\n",
            "Graph: {'A': ['B', 'C', 'D'], 'B': ['A', 'D', 'E'], 'C': ['A'], 'D': ['A', 'B', 'E', 'F'], 'E': ['B', 'D', 'G'], 'F': ['D'], 'G': ['E']}\n",
            "Starting Recursive DFS from node 'A':\n",
            "Recursive DFS Traversal Order: ['A', 'B', 'D', 'E', 'G', 'F', 'C']\n",
            "\n",
            "Starting Iterative DFS from node 'A':\n",
            "Iterative DFS Traversal Order: ['A', 'B', 'D', 'E', 'G', 'F', 'C']\n",
            "\n",
            "Graph (disconnected): {'0': ['1', '2'], '1': ['2'], '2': ['0', '3'], '3': ['3'], '4': ['5'], '5': ['4']}\n",
            "Starting Recursive DFS from node '0':\n",
            "Recursive DFS Traversal Order: ['0', '1', '2', '3']\n",
            "\n",
            "Starting Iterative DFS from node '4' in the disconnected graph:\n",
            "Iterative DFS Traversal Order: ['4', '5']\n",
            "\n",
            "Empty Graph: {}\n",
            "Starting Recursive DFS from node 'A':\n",
            "Recursive DFS Traversal Order: ['A']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "373a72ee"
      },
      "source": [
        "### Dynamic Programming (DP)\n",
        "\n",
        "**Concept:** Dynamic Programming is an algorithmic technique for solving a complex problem by breaking it down into simpler subproblems. It is applicable to problems that have overlapping subproblems and optimal substructure.\n",
        "\n",
        "-   **Overlapping Subproblems:** This means that the same subproblems are solved again and again. DP stores the solution to these subproblems so that they can be reused later.\n",
        "-   **Optimal Substructure:** This means that an optimal solution to the problem can be constructed from optimal solutions to its subproblems.\n",
        "\n",
        "DP is primarily used for optimization problems, where we need to find the best solution (e.g., maximum, minimum, longest, shortest, fewest, etc.) among many possible solutions.\n",
        "\n",
        "**How it Works:**\n",
        "There are two main approaches to Dynamic Programming:\n",
        "1.  **Memoization (Top-Down):** This is a recursive approach where the solution to a problem is first expressed in terms of solutions to subproblems. The results of subproblems are stored (memoized) in a table (e.g., an array or a dictionary) as they are computed. If a subproblem is encountered again, its stored result is simply returned, avoiding re-computation.\n",
        "2.  **Tabulation (Bottom-Up):** This is an iterative approach. It involves solving all related subproblems first and then combining their results to solve the larger problem. It typically fills up a table of solutions from the base cases upwards.\n",
        "\n",
        "**Key Steps for DP:**\n",
        "1.  **Characterize the optimal substructure:** Define the problem in terms of its subproblems.\n",
        "2.  **Define the recursive relation:** Write down the recurrence relation that relates the solution of the current problem to the solutions of its subproblems.\n",
        "3.  **Compute the base cases:** Identify the simplest subproblems and their direct solutions.\n",
        "4.  **Decide on Memoization or Tabulation:** Implement the chosen approach (usually tabulation is preferred for iterative execution and often better space optimization, while memoization can be more intuitive to implement from a recursive definition).\n",
        "\n",
        "**Typical Applications:**\n",
        "-   Fibonacci sequence (classic example for understanding recursion vs. DP)\n",
        "-   Shortest path problems (e.g., Floyd-Warshall, Bellman-Ford for graphs with specific properties)\n",
        "-   Knapsack problem\n",
        "-   Longest Common Subsequence (LCS)\n",
        "-   Matrix Chain Multiplication\n",
        "-   Coin Change problem\n",
        "-   Edit Distance\n",
        "\n",
        "**Time and Space Complexity:**\n",
        "-   **Time Complexity:** Generally, the time complexity of a DP solution is given by `(Number of states) * (Time to compute each state)`. If `N` is the input size and `S` is the number of states, and `T_state` is the time to compute each state, then Time = `O(S * T_state)`. Often, it turns a problem from exponential or polynomial `N^k` to `N` or `N^2`.\n",
        "-   **Space Complexity:** Typically `O(Number of states)` to store the solutions of subproblems in the memoization table or tabulation array."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e7aecba2",
        "outputId": "c6fa85ac-540c-41e5-dd28-acdd250a4e67"
      },
      "source": [
        "print('--- Dynamic Programming Demonstration (Fibonacci Sequence) ---')\n",
        "\n",
        "# 1. Memoization (Top-Down DP)\n",
        "memo = {}\n",
        "def fib_memoization(n):\n",
        "    \"\"\"Fibonacci sequence using Memoization (Top-Down DP).\n",
        "    Time Complexity: O(N) - Each Fibonacci number from 0 to N is computed once.\n",
        "    Space Complexity: O(N) - For the memoization table and recursion stack.\n",
        "    \"\"\"\n",
        "    if n in memo:\n",
        "        return memo[n]\n",
        "    if n <= 1:\n",
        "        return n\n",
        "    memo[n] = fib_memoization(n - 1) + fib_memoization(n - 2)\n",
        "    return memo[n]\n",
        "\n",
        "# 2. Tabulation (Bottom-Up DP)\n",
        "def fib_tabulation(n):\n",
        "    \"\"\"Fibonacci sequence using Tabulation (Bottom-Up DP).\n",
        "    Time Complexity: O(N) - Loop runs N times.\n",
        "    Space Complexity: O(N) - For the DP table.\n",
        "    \"\"\"\n",
        "    if n <= 1:\n",
        "        return n\n",
        "    dp = [0] * (n + 1)\n",
        "    dp[1] = 1\n",
        "    for i in range(2, n + 1):\n",
        "        dp[i] = dp[i - 1] + dp[i - 2]\n",
        "    return dp[n]\n",
        "\n",
        "# 3. Tabulation (Bottom-Up DP) with Space Optimization\n",
        "def fib_tabulation_optimized(n):\n",
        "    \"\"\"Fibonacci sequence using Tabulation (Bottom-Up DP) with space optimization.\n",
        "    Time Complexity: O(N) - Loop runs N times.\n",
        "    Space Complexity: O(1) - Only a few variables are used.\n",
        "    \"\"\"\n",
        "    if n <= 1:\n",
        "        return n\n",
        "    a, b = 0, 1\n",
        "    for _ in range(2, n + 1):\n",
        "        a, b = b, a + b\n",
        "    return b\n",
        "\n",
        "# --- Demonstration ---\n",
        "print(\"Calculating Fibonacci(10):\")\n",
        "\n",
        "n_val = 10\n",
        "\n",
        "# Reset memo for each call if not global or passed\n",
        "memo = {}\n",
        "fib_result_memo = fib_memoization(n_val)\n",
        "print(f\"  Memoization (Top-Down): fib({n_val}) = {fib_result_memo}\")\n",
        "\n",
        "fib_result_tab = fib_tabulation(n_val)\n",
        "print(f\"  Tabulation (Bottom-Up): fib({n_val}) = {fib_result_tab}\")\n",
        "\n",
        "fib_result_optimized = fib_tabulation_optimized(n_val)\n",
        "print(f\"  Tabulation (Space-Optimized): fib({n_val}) = {fib_result_optimized}\")\n",
        "\n",
        "print(\"\\nCalculating Fibonacci(0) and Fibonacci(1):\")\n",
        "print(f\"  fib(0) using Memoization: {fib_memoization(0)}\") # Need to reset memo or use a fresh one\n",
        "print(f\"  fib(1) using Tabulation: {fib_tabulation(1)}\")\n",
        "print(f\"  fib(0) using Space-Optimized Tabulation: {fib_tabulation_optimized(0)}\")\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Dynamic Programming Demonstration (Fibonacci Sequence) ---\n",
            "Calculating Fibonacci(10):\n",
            "  Memoization (Top-Down): fib(10) = 55\n",
            "  Tabulation (Bottom-Up): fib(10) = 55\n",
            "  Tabulation (Space-Optimized): fib(10) = 55\n",
            "\n",
            "Calculating Fibonacci(0) and Fibonacci(1):\n",
            "  fib(0) using Memoization: 0\n",
            "  fib(1) using Tabulation: 1\n",
            "  fib(0) using Space-Optimized Tabulation: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eefee909"
      },
      "source": [
        "## Algorithm Essentials - Medium HackerRank Practice\n",
        "\n",
        "### Subtask:\n",
        "Suggest 2-3 'Medium' HackerRank problems that test the understanding and application of the core algorithms. Instruct the candidate to solve these problems, focusing on deriving optimal solutions and meticulously analyzing their complexity. Refer to patterns in the GitHub repository for guidance.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a45b18a2"
      },
      "source": [
        "### HackerRank Practice: Algorithm Essentials (Medium Problems)\n",
        "\n",
        "Now that we've covered the core algorithms and data structures, it's time to test your understanding with 'Medium' level HackerRank problems. These problems will require you to think more deeply about optimal solutions and apply the concepts learned, such as sorting, searching, two-pointer, sliding window, BFS, DFS, and dynamic programming.\n",
        "\n",
        "#### Suggested HackerRank Problems (Medium):\n",
        "\n",
        "1.  **\"Merge Two Sorted Lists\"**\n",
        "    *   **Concept Reinforcement:** Two-pointer technique, list manipulation.\n",
        "    *   **Link:** [https://www.hackerrank.com/challenges/merge-two-sorted-lists/problem](https://www.hackerrank.com/challenges/merge-two-sorted-lists/problem)\n",
        "\n",
        "2.  **\"Minimum Bribes\"**\n",
        "    *   **Concept Reinforcement:** Array manipulation, counting inversions (can be approached with a modified two-pointer or sorting-related logic).\n",
        "    *   **Link:** [https://www.hackerrank.com/challenges/new-year-chaos/problem](https://www.hackerrank.com/challenges/new-year-chaos/problem)\n",
        "\n",
        "3.  **\"Coin Change\"**\n",
        "    *   **Concept Reinforcement:** Dynamic Programming (DP) - a classic DP problem.\n",
        "    *   **Link:** [https://www.hackerrank.com/challenges/coin-change/problem](https://www.hackerrank.com/challenges/coin-change/problem)\n",
        "\n",
        "4.  **\"Roads and Libraries\"**\n",
        "    *   **Concept Reinforcement:** Graph traversal (BFS/DFS), connected components.\n",
        "    *   **Link:** [https://www.hackerrank.com/challenges/roads-and-libraries/problem](https://www.hackerrank.com/challenges/roads-and-libraries/problem)\n",
        "\n",
        "#### Problem-Solving Guidance for Medium Problems:\n",
        "\n",
        "*   **Optimal Solutions:** For 'Medium' problems, your initial brute-force approach might pass some basic tests but often won't clear the time limits. Focus on identifying the most efficient algorithm (e.g., O(N) or O(N log N) instead of O(N^2) or O(N^3)). Think about how the algorithms we just covered (sorting, binary search, two-pointers, sliding window, BFS/DFS, DP) can be applied or adapted.\n",
        "*   **Meticulous Complexity Analysis:** Before coding, or during refinement, explicitly determine the time and space complexity of your proposed solution. This helps you confirm its optimality and debug performance issues. Be able to justify why your solution is optimal or where bottlenecks might exist.\n",
        "*   **Edge Cases and Constraints:** Pay close attention to constraints on input sizes, values, and potential edge cases (empty inputs, single element inputs, extreme values). Your solution should handle these gracefully.\n",
        "\n",
        "#### Leveraging the GitHub Repository:\n",
        "\n",
        "After you've genuinely struggled with a problem, come up with your best solution, and preferably passed the tests (or understood why it failed), you can refer to the provided GitHub repository for insights:\n",
        "\n",
        "*   **GitHub Repository:** [https://github.com/hevalhazalkurt/Hackerrank_Python_Solutions](https://github.com/hevalhazalkurt/Hackerrank_Python_Solutions)\n",
        "\n",
        "Use this resource to:\n",
        "    *   Compare your solution's approach and complexity with others.\n",
        "    *   Discover alternative algorithms or more Pythonic ways to solve the problem.\n",
        "    *   Understand common patterns for certain problem types (e.g., how DP solutions are structured, common graph traversal patterns).\n",
        "\n",
        "Remember, the goal is not just to get the correct answer, but to understand *why* a particular solution is efficient and how to derive it yourself."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5eed7157"
      },
      "source": [
        "## Progressive Problem Sets - Focused Practice\n",
        "\n",
        "### Subtask:\n",
        "Detail a structured approach to tackling problems categorized by data structures: arrays, strings, linked lists, trees, and graphs. Outline a strategy for progressing from easy (15-20 mins) to medium (30-45 mins) to brief advanced problems (60+ mins). Emphasize using the provided GitHub repository for solution patterns and common approaches.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8abe5827"
      },
      "source": [
        "## Progressive Problem Sets - Focused Practice\n",
        "\n",
        "To truly master HackerRank and excel in technical interviews, a structured and progressive approach to problem-solving is essential. This section outlines how to tackle problems categorized by fundamental data structures, gradually increasing in difficulty.\n",
        "\n",
        "### Data Structure Categories for Focused Practice:\n",
        "We will concentrate on problems involving these core data structures:\n",
        "1.  **Arrays**\n",
        "2.  **Strings**\n",
        "3.  **Linked Lists**\n",
        "4.  **Trees**\n",
        "5.  **Graphs**\n",
        "\n",
        "### Progressive Problem-Solving Strategy:\n",
        "For each data structure category, adopt the following progression, focusing on understanding the underlying principles and optimizing your solutions:\n",
        "\n",
        "*   **Easy Problems (15-20 minutes):**\n",
        "    *   **Goal:** Solidify fundamental concepts, syntax, and basic operations related to the data structure. Get comfortable with input/output parsing.\n",
        "    *   **Approach:** Aim for a working solution within the time limit. Focus on correctness and clarity. Don't immediately jump to the most optimal solution if a simpler, correct one comes to mind first.\n",
        "    *   **Focus:** Direct application of methods, simple loops, basic conditional logic.\n",
        "\n",
        "*   **Medium Problems (30-45 minutes):**\n",
        "    *   **Goal:** Introduce common algorithmic patterns (e.g., Two-Pointers, Sliding Window, recursion, simple DP states for arrays/strings) and deeper understanding of data structure properties. Optimize for time and space complexity.\n",
        "    *   **Approach:** Plan your solution before coding. Consider multiple approaches. Debug efficiently. Strive for optimal time complexity (e.g., O(N) or O(N log N)).\n",
        "    *   **Focus:** Efficient traversal, subtle edge cases, combining concepts, early optimization thoughts.\n",
        "\n",
        "*   **Brief Advanced Problems (60+ minutes):**\n",
        "    *   **Goal:** Tackle more complex scenarios, intricate algorithms, or problems requiring advanced data structures, dynamic programming, or graph algorithms. Deep dive into time/space constraints.\n",
        "    *   **Approach:** Break down the problem into smaller subproblems. Think about advanced patterns and data structures. It's okay if you don't get the optimal solution immediately; the learning is in the attempt and analysis.\n",
        "    *   **Focus:** Advanced algorithms (e.g., Dijkstra, A*, complex DP states, segment trees, tries), custom data structures, complex recursion with memoization, handling large inputs.\n",
        "\n",
        "### General Guidance for Problem Solving:\n",
        "*   **Understand DS&A:** Before attempting problems, ensure you have a solid grasp of the chosen data structure's characteristics, typical operations, and the core algorithms applicable to it (e.g., traversal for trees/graphs, sorting for arrays).\n",
        "*   **Whiteboard First:** For medium and advanced problems, outline your logic, data structures, and algorithms on a piece of paper or virtual whiteboard before writing any code. This helps catch logical errors early.\n",
        "*   **Test Thoroughly:** Beyond sample cases, consider edge cases: empty inputs, single-element inputs, very large inputs, inputs with duplicates, sorted/reverse-sorted inputs.\n",
        "*   **Consistent Practice:** Regular, focused practice is key. Don't just solve problems; understand *why* a particular solution is optimal.\n",
        "*   **Self-Assessment:** After solving (or attempting) a problem, review your solution's time and space complexity. Could it be improved? What alternatives exist?\n",
        "\n",
        "### Leveraging the GitHub Repository:\n",
        "The provided GitHub repository ([https://github.com/hevalhazalkurt/Hackerrank_Python_Solutions](https://github.com/hevalhazalkurt/Hackerrank_Python_Solutions)) is an excellent resource, but use it strategically:\n",
        "\n",
        "*   **Attempt First:** Always attempt a problem yourself first. Struggle is a vital part of learning.\n",
        "*   **Review, Don't Copy:** After you've submitted your solution (whether accepted or not), or if you're truly stuck after a significant effort (e.g., 30-60 minutes for a medium problem), then refer to the repository.\n",
        "*   **Learn Patterns:** Use the repository to observe common solution patterns, efficient Python idioms, and alternative algorithmic approaches. Understand *why* a particular solution was chosen, especially its time and space complexity.\n",
        "*   **Compare and Improve:** Compare the repository's solution to your own. Identify areas for improvement in your logic, conciseness, or efficiency. Then, try to re-implement the improved logic without looking at the solution.\n",
        "\n",
        "By following this structured approach, you'll build a robust foundation in algorithmic problem-solving, making you a more effective and confident data scientist in technical assessments."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7526ad9f"
      },
      "source": [
        "## Progressive Problem Sets - Advanced HackerRank Practice\n",
        "\n",
        "### Subtask:\n",
        "Suggest 2-3 'Advanced' HackerRank problems, potentially focusing on more complex graph algorithms, tree traversals, or intricate dynamic programming scenarios. Challenge the candidate to apply learned patterns and devise efficient, senior-level solutions.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ef951b4"
      },
      "source": [
        "### Advanced HackerRank Practice\n",
        "\n",
        "This section challenges senior data scientists with 'Advanced' HackerRank problems, designed to push the boundaries of algorithmic thinking and problem-solving. These problems often involve intricate scenarios, large datasets, and require highly optimized solutions, reflecting the complexity encountered in real-world data science applications.\n",
        "\n",
        "#### Suggested HackerRank Problems (Advanced):\n",
        "\n",
        "1.  **\"Roads and Libraries\"**\n",
        "    *   **Concept Reinforcement:** Complex Graph Algorithms (BFS/DFS, Union-Find, Minimum Spanning Tree concepts, optimal cost analysis).\n",
        "    *   **Link:** [https://www.hackerrank.com/challenges/crush/problem](https://www.hackerrank.com/challenges/crush/problem)\n",
        "\n",
        "2.  **\"Abbreviation\"**\n",
        "    *   **Concept Reinforcement:** Advanced Dynamic Programming, string manipulation, state management.\n",
        "    *   **Link:** [https://www.hackerrank.com/challenges/abbr/problem](https://www.hackerrank.com/challenges/abbr/problem)\n",
        "\n",
        "3.  **\"BFS: Shortest Reach in a Graph\"** (While it might seem basic, solving it optimally with large constraints and understanding graph representation/traversal choices for performance can be advanced).\n",
        "    *   **Concept Reinforcement:** Efficient Graph Traversal (BFS optimization), shortest path in unweighted graphs.\n",
        "    *   **Link:** [https://www.hackerrank.com/challenges/bfsshortreach/problem](https://www.hackerrank.com/challenges/bfsshortreach/problem)\n",
        "\n",
        "#### Guidance for Tackling Advanced Problems:\n",
        "\n",
        "*   **Devise Highly Efficient Solutions:** Simple brute-force approaches will almost always time out. Focus on algorithms with optimal time and space complexity (e.g., O(N), O(N log N)).\n",
        "*   **Thorough Complexity Analysis:** Before coding, meticulously analyze the time and space complexity of your proposed algorithm. Understand how your solution scales with large inputs.\n",
        "*   **Break Down Complex Problems:** For intricate problems, break them into smaller, manageable subproblems. Often, these subproblems can be solved using techniques like dynamic programming or specific graph algorithms.\n",
        "*   **Handle Large Inputs and Edge Cases:** Pay close attention to constraints. Advanced problems often test your ability to handle very large inputs (e.g., N=10^5, 10^6) and obscure edge cases (empty inputs, single element inputs, extreme values).\n",
        "*   **Choose Appropriate Data Structures:** The right data structure (e.g., adjacency list for graphs, segment trees, tries) can drastically change the complexity and feasibility of your solution.\n",
        "\n",
        "#### Leveraging the GitHub Repository:\n",
        "\n",
        "As with simpler problems, the GitHub repository ([https://github.com/hevalhazalkurt/Hackerrank_Python_Solutions](https://github.com/hevalhazalkurt/Hackerrank_Python_Solutions)) remains a valuable resource. However, for 'Advanced' problems, the primary goal should be to genuinely attempt and struggle with the problem first. Only after significant personal effort (and perhaps a submission that fails due to time limits or incorrect logic) should you consult the repository to understand advanced solution patterns, optimization techniques, and alternative approaches. This deliberate practice fosters deeper learning and retention."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f212a9e0"
      },
      "source": [
        "## Senior-Level ML/Framework Applications - NumPy & Pandas\n",
        "\n",
        "### Subtask:\n",
        "Review advanced NumPy array operations, including broadcasting and vectorization, and efficient Pandas data manipulation techniques crucial for data science interviews. Provide practical, executable code examples (as Colab snippets) demonstrating performance benefits and best practices.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4e908f6a",
        "outputId": "137d0dad-dc3b-4648-b557-980a6847c123"
      },
      "source": [
        "import numpy as np\n",
        "import timeit\n",
        "\n",
        "print('--- Advanced NumPy Array Operations ---')\n",
        "\n",
        "# 1. Vectorization Demonstration\n",
        "print('\\n### Vectorization: Element-wise operations')\n",
        "\n",
        "# Scenario: Add a constant to every element in an array/list\n",
        "\n",
        "# Python list (traditional loop)\n",
        "def add_constant_python_list(data_list, constant):\n",
        "    return [x + constant for x in data_list]\n",
        "\n",
        "# NumPy array (vectorized)\n",
        "def add_constant_numpy_array(data_array, constant):\n",
        "    return data_array + constant # O(N) operation, highly optimized C-level code\n",
        "\n",
        "# Data preparation\n",
        "size = 10**6\n",
        "python_list = list(range(size))\n",
        "numpy_array = np.arange(size)\n",
        "constant_to_add = 5\n",
        "\n",
        "print(f\"Performing addition for {size} elements...\")\n",
        "\n",
        "# Performance comparison\n",
        "time_python = timeit.timeit(lambda: add_constant_python_list(python_list, constant_to_add), number=10)\n",
        "time_numpy = timeit.timeit(lambda: add_constant_numpy_array(numpy_array, constant_to_add), number=10)\n",
        "\n",
        "print(f\"  Python list (for loop): {time_python:.6f} seconds\")\n",
        "print(f\"  NumPy array (vectorized): {time_numpy:.6f} seconds\")\n",
        "print(f\"  NumPy is {time_python / time_numpy:.2f}x faster than Python list in this case.\")\n",
        "\n",
        "# Complexity Analysis for Vectorization:\n",
        "# Time Complexity: O(N) for element-wise operations, but with a significantly smaller constant factor than Python loops.\n",
        "# Space Complexity: O(N) for the resulting array.\n",
        "\n",
        "# 2. Broadcasting Demonstration\n",
        "print('\\n### Broadcasting: Operations on arrays of different shapes')\n",
        "\n",
        "# Scenario: Add a 1D array (row vector) to each row of a 2D array\n",
        "\n",
        "# Data preparation\n",
        "matrix = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]) # Shape (3, 3)\n",
        "row_vector = np.array([10, 20, 30]) # Shape (3,)\n",
        "\n",
        "print(f\"Original Matrix:\\n{matrix}\")\n",
        "print(f\"Row Vector: {row_vector}\")\n",
        "\n",
        "# Broadcasting in action: row_vector is broadcast across each row of the matrix\n",
        "# NumPy automatically expands the row_vector to match the number of rows in the matrix\n",
        "# Time Complexity: O(M*N) where M,N are dimensions of the larger array.\n",
        "# Space Complexity: O(M*N) for the result, O(1) for broadcasting logic itself (no actual data duplication).\n",
        "result_broadcast = matrix + row_vector\n",
        "print(f\"Matrix + Row Vector (Broadcasting):\\n{result_broadcast}\")\n",
        "\n",
        "# Scenario: Add a 1D array (column vector) to each column of a 2D array\n",
        "column_vector = np.array([[100], [200], [300]]) # Shape (3, 1)\n",
        "\n",
        "print(f\"\\nOriginal Matrix:\\n{matrix}\")\n",
        "print(f\"Column Vector:\\n{column_vector}\")\n",
        "\n",
        "# Broadcasting in action: column_vector is broadcast across each column of the matrix\n",
        "result_column_broadcast = matrix + column_vector\n",
        "print(f\"Matrix + Column Vector (Broadcasting):\\n{result_column_broadcast}\")\n",
        "\n",
        "# Best Practices for NumPy:\n",
        "print('\\n--- Best Practices for NumPy ---')\n",
        "print('- Prefer NumPy vectorized operations over explicit Python loops for numerical tasks.')\n",
        "print('- Understand broadcasting rules to perform operations efficiently on arrays of different shapes without explicit reshaping.')\n",
        "print('- Use appropriate data types (`dtype`) to save memory and sometimes improve performance.')\n",
        "print('- For memory-intensive tasks, consider in-place operations where appropriate (e.g., `arr += 5` instead of `arr = arr + 5`).')"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Advanced NumPy Array Operations ---\n",
            "\n",
            "### Vectorization: Element-wise operations\n",
            "Performing addition for 1000000 elements...\n",
            "  Python list (for loop): 0.563129 seconds\n",
            "  NumPy array (vectorized): 0.014336 seconds\n",
            "  NumPy is 39.28x faster than Python list in this case.\n",
            "\n",
            "### Broadcasting: Operations on arrays of different shapes\n",
            "Original Matrix:\n",
            "[[1 2 3]\n",
            " [4 5 6]\n",
            " [7 8 9]]\n",
            "Row Vector: [10 20 30]\n",
            "Matrix + Row Vector (Broadcasting):\n",
            "[[11 22 33]\n",
            " [14 25 36]\n",
            " [17 28 39]]\n",
            "\n",
            "Original Matrix:\n",
            "[[1 2 3]\n",
            " [4 5 6]\n",
            " [7 8 9]]\n",
            "Column Vector:\n",
            "[[100]\n",
            " [200]\n",
            " [300]]\n",
            "Matrix + Column Vector (Broadcasting):\n",
            "[[101 102 103]\n",
            " [204 205 206]\n",
            " [307 308 309]]\n",
            "\n",
            "--- Best Practices for NumPy ---\n",
            "- Prefer NumPy vectorized operations over explicit Python loops for numerical tasks.\n",
            "- Understand broadcasting rules to perform operations efficiently on arrays of different shapes without explicit reshaping.\n",
            "- Use appropriate data types (`dtype`) to save memory and sometimes improve performance.\n",
            "- For memory-intensive tasks, consider in-place operations where appropriate (e.g., `arr += 5` instead of `arr = arr + 5`).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5312979a",
        "outputId": "232bdf8c-c7b5-4fb5-9f5d-ab7fc998f4d1"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import timeit\n",
        "\n",
        "print('--- Efficient Pandas Data Manipulation Techniques ---')\n",
        "\n",
        "# Data preparation for Pandas examples\n",
        "data = {\n",
        "    'City': ['New York', 'London', 'Paris', 'New York', 'London', 'Paris', 'Tokyo', 'London', 'New York', 'Paris'],\n",
        "    'Category': ['A', 'B', 'A', 'C', 'B', 'C', 'A', 'A', 'B', 'C'],\n",
        "    'Value1': np.random.randint(1, 100, 10),\n",
        "    'Value2': np.random.rand(10) * 100\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "print(\"\\nOriginal DataFrame:\\n\", df)\n",
        "\n",
        "# 1. .loc and .iloc for Efficient Selection\n",
        "print('\\n### 1. .loc and .iloc for Efficient Selection')\n",
        "\n",
        "# Select rows where City is 'New York' using .loc\n",
        "# Time Complexity: O(N) in worst case for boolean indexing (needs to scan entire Series)\n",
        "# Space Complexity: O(N) for the resulting DataFrame\n",
        "ny_data_loc = df.loc[df['City'] == 'New York']\n",
        "print(\"\\nData for New York (using .loc):\\n\", ny_data_loc)\n",
        "\n",
        "# Select rows 0 to 2 and columns 'City', 'Value1' using .iloc\n",
        "# Time Complexity: O(rows * cols) for copying selected data\n",
        "# Space Complexity: O(rows * cols) for the resulting DataFrame\n",
        "subset_iloc = df.iloc[0:3, [0, 2]]\n",
        "print(\"\\nSubset (rows 0-2, cols 0,2 using .iloc):\\n\", subset_iloc)\n",
        "\n",
        "# Best Practice: Always use .loc or .iloc for explicit and efficient data selection and assignment.\n",
        "\n",
        "# 2. apply(), map(), and applymap()\n",
        "print('\\n### 2. apply(), map(), and applymap()')\n",
        "\n",
        "def categorize_value(val):\n",
        "    return 'High' if val > 50 else 'Low'\n",
        "\n",
        "# Using Series.map() for element-wise transformation on a single Series\n",
        "# When to use: element-wise ops on a single series (often with dict/series mapping, or simple function)\n",
        "# Time Complexity: O(N) where N is the length of the Series\n",
        "# Space Complexity: O(N) for the new Series\n",
        "df['Value1_Category_map'] = df['Value1'].map(categorize_value)\n",
        "print(\"\\nDataFrame with 'Value1_Category' (using map()):\\n\", df)\n",
        "\n",
        "# Using DataFrame.apply() along an axis (e.g., row-wise)\n",
        "# When to use: operations needing entire row/column, or custom complex logic not covered by vectorized ops\n",
        "# Time Complexity: O(N * K) where N is number of rows and K is complexity of applied function per row\n",
        "# Space Complexity: O(N) for the new Series/DataFrame\n",
        "def combine_values(row):\n",
        "    return f\"{row['City']}-{row['Category']}\"\n",
        "\n",
        "df['City_Category_apply'] = df.apply(combine_values, axis=1)\n",
        "print(\"\\nDataFrame with 'City_Category' (using apply() row-wise):\\n\", df)\n",
        "\n",
        "# Using DataFrame.map() for element-wise transformation across entire DataFrame\n",
        "# (Replaces applymap() for newer Pandas versions)\n",
        "# Time Complexity: O(rows * cols * K) where K is complexity of applied function per cell\n",
        "# Space Complexity: O(rows * cols) for the new DataFrame\n",
        "# Note: This is generally slower than vectorized operations for numerical data.\n",
        "# For demonstration, let's apply a simple formatting to a subset of numeric columns\n",
        "df_numeric_formatted = df[['Value1', 'Value2']].map(lambda x: f\"{x:.2f}\")\n",
        "print(\"\\nFormatted numeric columns (using map() on DataFrame):\\n\", df_numeric_formatted.head())\n",
        "\n",
        "# Performance comparison: map vs. apply vs. loop for element-wise operation\n",
        "long_series = pd.Series(np.random.randint(1, 100, 10**6))\n",
        "\n",
        "time_map = timeit.timeit(lambda: long_series.map(categorize_value), number=10)\n",
        "time_apply = timeit.timeit(lambda: long_series.apply(categorize_value), number=10)\n",
        "time_loop = timeit.timeit(lambda: [categorize_value(x) for x in long_series], number=10)\n",
        "\n",
        "print(f\"\\nPerformance for {len(long_series)} elements (map vs apply vs loop):\")\n",
        "print(f\"  map(): {time_map:.6f} seconds\")\n",
        "print(f\"  apply(): {time_apply:.6f} seconds\")\n",
        "print(f\"  Python loop: {time_loop:.6f} seconds\")\n",
        "\n",
        "# Best Practice: Prefer vectorized NumPy operations first. If not possible, use Series.map() for element-wise ops. Use DataFrame.apply() for row/column-wise ops if vectorized options are unavailable.\n",
        "\n",
        "# 3. Efficient groupby() Operations\n",
        "print('\\n### 3. Efficient groupby() Operations')\n",
        "\n",
        "# Group by 'City' and calculate mean of 'Value1'\n",
        "# Time Complexity: O(N) (hash-based grouping) + O(G) (number of groups) * O(1) for aggregation\n",
        "# Space Complexity: O(N) for intermediate data + O(G) for result\n",
        "mean_by_city = df.groupby('City')['Value1'].mean()\n",
        "print(\"\\nMean Value1 by City:\\n\", mean_by_city)\n",
        "\n",
        "# Group by 'City' and 'Category', then apply multiple aggregations using .agg()\n",
        "# Time Complexity: O(N) + O(G) * O(number_of_aggregations)\n",
        "# Space Complexity: O(N) + O(G) for result\n",
        "multi_agg = df.groupby(['City', 'Category']).agg(\n",
        "    mean_value1=('Value1', 'mean'),\n",
        "    sum_value2=('Value2', 'sum'),\n",
        "    count_records=('City', 'count')\n",
        ")\n",
        "print(\"\\nMultiple Aggregations by City and Category:\\n\", multi_agg)\n",
        "\n",
        "# Using .transform() to return a Series with same index as original DataFrame\n",
        "# (e.g., subtract city mean from each value)\n",
        "# Time Complexity: O(N) (grouping) + O(N) (transform operation)\n",
        "# Space Complexity: O(N) for the new Series\n",
        "df['Value1_Centered_by_City'] = df.groupby('City')['Value1'].transform(lambda x: x - x.mean())\n",
        "print(\"\\nValue1 Centered by City (using transform()):\\n\", df[['City', 'Value1', 'Value1_Centered_by_City']])\n",
        "\n",
        "# Best Practice: Use built-in aggregation functions directly on groupby objects, or .agg() for multiple aggregations. Use .transform() when you need to return a result with the same index as the original DataFrame.\n",
        "\n",
        "print('\\n--- General Pandas Best Practices ---')\n",
        "print('- Avoid explicit Python for-loops over DataFrame rows; prefer vectorized operations.')\n",
        "print('- Use .loc/.iloc for clear and efficient indexing.')\n",
        "print('- Choose appropriate data types (`dtype`) to save memory and improve performance.')\n",
        "print('- Be mindful of chained assignments (e.g., `df[col][idx] = val`), which can lead to `SettingWithCopyWarning` and unexpected behavior; prefer `.loc` or `.iloc` for assignments.')\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Efficient Pandas Data Manipulation Techniques ---\n",
            "\n",
            "Original DataFrame:\n",
            "        City Category  Value1     Value2\n",
            "0  New York        A      57  26.775444\n",
            "1    London        B      73  87.551274\n",
            "2     Paris        A      69  27.667813\n",
            "3  New York        C      20  35.303420\n",
            "4    London        B      70  47.956620\n",
            "5     Paris        C      20  60.599445\n",
            "6     Tokyo        A      12  84.617912\n",
            "7    London        A      24  30.764830\n",
            "8  New York        B      76  64.650646\n",
            "9     Paris        C      81  36.924274\n",
            "\n",
            "### 1. .loc and .iloc for Efficient Selection\n",
            "\n",
            "Data for New York (using .loc):\n",
            "        City Category  Value1     Value2\n",
            "0  New York        A      57  26.775444\n",
            "3  New York        C      20  35.303420\n",
            "8  New York        B      76  64.650646\n",
            "\n",
            "Subset (rows 0-2, cols 0,2 using .iloc):\n",
            "        City  Value1\n",
            "0  New York      57\n",
            "1    London      73\n",
            "2     Paris      69\n",
            "\n",
            "### 2. apply(), map(), and applymap()\n",
            "\n",
            "DataFrame with 'Value1_Category' (using map()):\n",
            "        City Category  Value1     Value2 Value1_Category_map\n",
            "0  New York        A      57  26.775444                High\n",
            "1    London        B      73  87.551274                High\n",
            "2     Paris        A      69  27.667813                High\n",
            "3  New York        C      20  35.303420                 Low\n",
            "4    London        B      70  47.956620                High\n",
            "5     Paris        C      20  60.599445                 Low\n",
            "6     Tokyo        A      12  84.617912                 Low\n",
            "7    London        A      24  30.764830                 Low\n",
            "8  New York        B      76  64.650646                High\n",
            "9     Paris        C      81  36.924274                High\n",
            "\n",
            "DataFrame with 'City_Category' (using apply() row-wise):\n",
            "        City Category  Value1     Value2 Value1_Category_map  \\\n",
            "0  New York        A      57  26.775444                High   \n",
            "1    London        B      73  87.551274                High   \n",
            "2     Paris        A      69  27.667813                High   \n",
            "3  New York        C      20  35.303420                 Low   \n",
            "4    London        B      70  47.956620                High   \n",
            "5     Paris        C      20  60.599445                 Low   \n",
            "6     Tokyo        A      12  84.617912                 Low   \n",
            "7    London        A      24  30.764830                 Low   \n",
            "8  New York        B      76  64.650646                High   \n",
            "9     Paris        C      81  36.924274                High   \n",
            "\n",
            "  City_Category_apply  \n",
            "0          New York-A  \n",
            "1            London-B  \n",
            "2             Paris-A  \n",
            "3          New York-C  \n",
            "4            London-B  \n",
            "5             Paris-C  \n",
            "6             Tokyo-A  \n",
            "7            London-A  \n",
            "8          New York-B  \n",
            "9             Paris-C  \n",
            "\n",
            "Formatted numeric columns (using map() on DataFrame):\n",
            "   Value1 Value2\n",
            "0  57.00  26.78\n",
            "1  73.00  87.55\n",
            "2  69.00  27.67\n",
            "3  20.00  35.30\n",
            "4  70.00  47.96\n",
            "\n",
            "Performance for 1000000 elements (map vs apply vs loop):\n",
            "  map(): 1.740562 seconds\n",
            "  apply(): 1.707197 seconds\n",
            "  Python loop: 1.722191 seconds\n",
            "\n",
            "### 3. Efficient groupby() Operations\n",
            "\n",
            "Mean Value1 by City:\n",
            " City\n",
            "London      55.666667\n",
            "New York    51.000000\n",
            "Paris       56.666667\n",
            "Tokyo       12.000000\n",
            "Name: Value1, dtype: float64\n",
            "\n",
            "Multiple Aggregations by City and Category:\n",
            "                    mean_value1  sum_value2  count_records\n",
            "City     Category                                        \n",
            "London   A                24.0   30.764830              1\n",
            "         B                71.5  135.507893              2\n",
            "New York A                57.0   26.775444              1\n",
            "         B                76.0   64.650646              1\n",
            "         C                20.0   35.303420              1\n",
            "Paris    A                69.0   27.667813              1\n",
            "         C                50.5   97.523719              2\n",
            "Tokyo    A                12.0   84.617912              1\n",
            "\n",
            "Value1 Centered by City (using transform()):\n",
            "        City  Value1  Value1_Centered_by_City\n",
            "0  New York      57                 6.000000\n",
            "1    London      73                17.333333\n",
            "2     Paris      69                12.333333\n",
            "3  New York      20               -31.000000\n",
            "4    London      70                14.333333\n",
            "5     Paris      20               -36.666667\n",
            "6     Tokyo      12                 0.000000\n",
            "7    London      24               -31.666667\n",
            "8  New York      76                25.000000\n",
            "9     Paris      81                24.333333\n",
            "\n",
            "--- General Pandas Best Practices ---\n",
            "- Avoid explicit Python for-loops over DataFrame rows; prefer vectorized operations.\n",
            "- Use .loc/.iloc for clear and efficient indexing.\n",
            "- Choose appropriate data types (`dtype`) to save memory and improve performance.\n",
            "- Be mindful of chained assignments (e.g., `df[col][idx] = val`), which can lead to `SettingWithCopyWarning` and unexpected behavior; prefer `.loc` or `.iloc` for assignments.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f4994b22"
      },
      "source": [
        "## Senior-Level ML/Framework Applications - PyTorch/JAX/TensorFlow\n",
        "\n",
        "### Subtask:\n",
        "Discuss key aspects of modern ML frameworks: PyTorch tensor operations, JAX functional programming patterns, and TensorFlow/Keras model optimization algorithms. Provide concise explanations and small, illustrative code snippets (as Colab snippets) to demonstrate core concepts and their application in practical scenarios.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b15af954"
      },
      "source": [
        "### PyTorch Tensor Operations and Autograd\n",
        "\n",
        "PyTorch is an open-source machine learning library primarily used for applications such as computer vision and natural language processing. Its defining features are strong GPU acceleration and a flexible automatic differentiation system (autograd).\n",
        "\n",
        "#### 1. PyTorch Tensors\n",
        "\n",
        "**Concept:** Tensors are the fundamental data structure in PyTorch. They are multi-dimensional arrays, similar to NumPy arrays, but with the added capability of being able to run on GPUs and efficiently track gradients for automatic differentiation.\n",
        "\n",
        "**Key Characteristics:**\n",
        "-   **Multi-dimensional:** Can represent scalars (0-D), vectors (1-D), matrices (2-D), and higher-dimensional data.\n",
        "-   **Data Type:** Holds a uniform type of data (e.g., `torch.float32`, `torch.int64`).\n",
        "-   **Device:** Can reside on CPU or GPU.\n",
        "\n",
        "**Common Operations & Complexity:**\n",
        "-   **Creation:**\n",
        "    -   `torch.tensor()`: Creates a tensor from Python list/NumPy array. O(N) where N is total elements.\n",
        "    -   `torch.zeros()`, `torch.ones()`, `torch.rand()`, `torch.empty()`: Creates tensors with specific values/shapes. O(N) to initialize, O(1) if uninitialized like `empty()`.\n",
        "    -   `torch.arange()`, `torch.linspace()`: Creates 1D tensors with sequences. O(N).\n",
        "-   **Basic Arithmetic Operations:**\n",
        "    -   Element-wise operations (`+`, `-`, `*`, `/`, `**`): O(N) for tensors with N elements. Highly optimized C/CUDA code.\n",
        "    -   Matrix Multiplication (`torch.mm()`, `@`): O(N^3) for N x N matrices (standard algorithm), or O(rows1 * cols1 * cols2) for general matrices. Can be faster with optimized libraries (e.g., Strassen's algorithm, usually vendor-optimized BLAS).\n",
        "    -   Dot Product (`torch.dot()` for 1D, `torch.matmul()` for general): O(N) for 1D, O(N^3) for matrices (or general `matmul`).\n",
        "-   **Indexing and Slicing:** Similar to NumPy. O(1) for direct access, O(k) for slices of length k.\n",
        "-   **Reshaping/Views:** `view()`, `reshape()`: O(1) if data is contiguous (returns a view), O(N) if a copy is needed.\n",
        "\n",
        "**Space Complexity:**\n",
        "-   Generally O(N) for a tensor with N elements, to store its data. Operations returning new tensors also require O(N) space.\n",
        "\n",
        "#### 2. Automatic Differentiation (Autograd)\n",
        "\n",
        "**Concept:** Autograd is PyTorch's engine for automatically computing gradients. It enables efficient backpropagation, which is crucial for training neural networks. When you perform operations on tensors with `requires_grad=True`, PyTorch builds a dynamic computational graph. This graph records all operations, and during the backward pass (`.backward()`), it automatically computes gradients for all tensors involved in the graph.\n",
        "\n",
        "**How it Works:**\n",
        "-   **`requires_grad=True`:** Marks a tensor as needing gradient computation. This tells Autograd to track all operations performed on it.\n",
        "-   **Computational Graph:** Every operation on `requires_grad=True` tensors creates a node in a DAG (Directed Acyclic Graph) representing the computation. Each node stores its inputs and the function that computed its output.\n",
        "-   **Backward Pass (`.backward()`):** Calling `.backward()` on a scalar tensor (usually the loss) traverses the computational graph backward from the output to the inputs. At each node, it applies the chain rule to compute gradients for all input tensors with `requires_grad=True`. The gradients accumulate in the `.grad` attribute of these tensors.\n",
        "-   **`torch.no_grad()`:** Used to disable gradient calculation, useful during inference or when updating model weights, as it can speed up computations and reduce memory usage.\n",
        "\n",
        "**Complexity:**\n",
        "-   **Forward Pass:** Time complexity is determined by the operations themselves (e.g., matrix multiplications are O(N^3)).\n",
        "-   **Backward Pass:** The time complexity is proportional to the forward pass, roughly O(N) if the forward pass takes O(N). Each operation in the graph requires corresponding gradient computation. The space complexity is also proportional to the forward pass, as it needs to store intermediate values for gradient calculations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aadb7fb9",
        "outputId": "6112ec8f-3944-4bea-e78d-e34f2cb63a1a"
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "print('--- PyTorch Tensor Operations Demonstration ---')\n",
        "\n",
        "# 1. Tensor Creation\n",
        "print('\\n### Tensor Creation')\n",
        "\n",
        "# From Python list: O(N) where N is total elements\n",
        "list_data = [[1, 2], [3, 4]]\n",
        "tensor_from_list = torch.tensor(list_data)\n",
        "print(f\"Tensor from list:\\n{tensor_from_list}\")\n",
        "\n",
        "# From NumPy array: O(N)\n",
        "numpy_array = np.array([[5, 6], [7, 8]])\n",
        "tensor_from_numpy = torch.tensor(numpy_array)\n",
        "print(f\"Tensor from NumPy array:\\n{tensor_from_numpy}\")\n",
        "\n",
        "# Zeros, Ones, Random: O(N) to initialize, O(1) for empty (uninitialized)\n",
        "zeros_tensor = torch.zeros(2, 3) # O(N)\n",
        "ones_tensor = torch.ones(2, 2)   # O(N)\n",
        "rand_tensor = torch.rand(3, 3)   # O(N)\n",
        "empty_tensor = torch.empty(1, 4) # O(1) for creation, values are uninitialized\n",
        "print(f\"Zeros tensor:\\n{zeros_tensor}\")\n",
        "print(f\"Ones tensor:\\n{ones_tensor}\")\n",
        "print(f\"Random tensor:\\n{rand_tensor}\")\n",
        "print(f\"Empty tensor (uninitialized values):\\n{empty_tensor}\")\n",
        "\n",
        "# Arange, Linspace: O(N)\n",
        "arange_tensor = torch.arange(0, 5, 1) # O(N)\n",
        "linspace_tensor = torch.linspace(0, 10, 5) # O(N)\n",
        "print(f\"Arange tensor: {arange_tensor}\")\n",
        "print(f\"Linspace tensor: {linspace_tensor}\")\n",
        "\n",
        "# Tensor properties: O(1)\n",
        "print(f\"\\nProperties of tensor_from_list: shape={tensor_from_list.shape}, dtype={tensor_from_list.dtype}, device={tensor_from_list.device}\")\n",
        "\n",
        "# 2. Basic Arithmetic Operations\n",
        "print('\\n### Basic Arithmetic Operations')\n",
        "\n",
        "t1 = torch.tensor([[1., 2.], [3., 4.]])\n",
        "t2 = torch.tensor([[5., 6.], [7., 8.]])\n",
        "print(f\"t1:\\n{t1}\")\n",
        "print(f\"t2:\\n{t2}\")\n",
        "\n",
        "# Element-wise addition: O(N) where N is total elements\n",
        "sum_tensor = t1 + t2\n",
        "print(f\"Element-wise sum:\\n{sum_tensor}\")\n",
        "\n",
        "# Element-wise multiplication: O(N)\n",
        "prod_tensor = t1 * t2\n",
        "print(f\"Element-wise product:\\n{prod_tensor}\")\n",
        "\n",
        "# Matrix multiplication (@ or torch.matmul()): O(rows1*cols1*cols2) generally, O(N^3) for N x N\n",
        "mat_mul_tensor = t1 @ t2 # Or torch.matmul(t1, t2)\n",
        "print(f\"Matrix multiplication:\\n{mat_mul_tensor}\")\n",
        "\n",
        "# Scalar operations: O(N)\n",
        "scalar_add = t1 + 10\n",
        "print(f\"Scalar addition:\\n{scalar_add}\")\n",
        "\n",
        "# 3. Indexing and Slicing\n",
        "print('\\n### Indexing and Slicing')\n",
        "\n",
        "tensor_idx = torch.tensor([[10, 11, 12],\n",
        "                           [13, 14, 15],\n",
        "                           [16, 17, 18]])\n",
        "print(f\"Original tensor:\\n{tensor_idx}\")\n",
        "\n",
        "# Access single element: O(1)\n",
        "first_element = tensor_idx[0, 0]\n",
        "print(f\"First element: {first_element}\")\n",
        "\n",
        "# Slice a row: O(k) where k is length of slice\n",
        "first_row = tensor_idx[0, :]\n",
        "print(f\"First row: {first_row}\")\n",
        "\n",
        "# Slice a column: O(k)\n",
        "second_col = tensor_idx[:, 1]\n",
        "print(f\"Second column: {second_col}\")\n",
        "\n",
        "# Sub-tensor: O(k*m)\n",
        "sub_tensor = tensor_idx[0:2, 1:3]\n",
        "print(f\"Sub-tensor (rows 0-1, cols 1-2):\\n{sub_tensor}\")\n",
        "\n",
        "# 4. Reshaping and Views\n",
        "print('\\n### Reshaping and Views')\n",
        "\n",
        "tensor_flat = torch.arange(1, 10) # O(N)\n",
        "print(f\"Original 1D tensor: {tensor_flat}\")\n",
        "\n",
        "# Reshape to 3x3 matrix: O(1) if data contiguous, O(N) if copy needed\n",
        "reshaped_tensor = tensor_flat.reshape(3, 3)\n",
        "print(f\"Reshaped to 3x3:\\n{reshaped_tensor}\")\n",
        "\n",
        "# View (shares underlying data): O(1)\n",
        "view_tensor = reshaped_tensor.view(-1) # -1 infers dimension\n",
        "print(f\"View (flattened): {view_tensor}\")\n",
        "# Modifying the view changes the original tensor:\n",
        "view_tensor[0] = 99\n",
        "print(f\"Original tensor after view modification:\\n{reshaped_tensor}\")\n",
        "\n",
        "# 5. Automatic Differentiation (Autograd)\n",
        "print('\\n### Automatic Differentiation (Autograd)')\n",
        "\n",
        "# Create tensors with requires_grad=True to track gradients\n",
        "x = torch.tensor(2.0, requires_grad=True) # O(1) to create, marks for gradient tracking\n",
        "y = torch.tensor(3.0, requires_grad=True) # O(1)\n",
        "\n",
        "# Define a simple computation graph\n",
        "# z = x^2 + y*x + 5\n",
        "z = x**2 + y*x + 5 # Forward pass: O(1) for this simple calculation\n",
        "print(f\"x: {x}, y: {y}\")\n",
        "print(f\"z = x^2 + y*x + 5 = {z}\")\n",
        "\n",
        "# Perform backward pass to compute gradients\n",
        "# Time Complexity: Proportional to the forward pass. Space: Proportional to forward pass for intermediate results.\n",
        "z.backward() # O(1) for this simple graph\n",
        "\n",
        "# Gradients are stored in .grad attribute\n",
        "# dz/dx = 2x + y = 2(2) + 3 = 7\n",
        "# dz/dy = x = 2\n",
        "print(f\"Gradient of z with respect to x (dz/dx): {x.grad}\")\n",
        "print(f\"Gradient of z with respect to y (dz/dy): {y.grad}\")\n",
        "\n",
        "# Demonstrate torch.no_grad()\n",
        "print('\\n--- Demonstrating torch.no_grad() ---')\n",
        "with torch.no_grad(): # Context manager to disable gradient tracking\n",
        "    a = torch.tensor(5.0, requires_grad=True)\n",
        "    b = a * 2 # This operation will NOT be tracked in the graph\n",
        "    print(f\"Tensor 'a' (requires_grad=True): {a}\")\n",
        "    print(f\"Tensor 'b' (created inside no_grad, requires_grad={b.requires_grad}): {b}\")\n",
        "    # If we try to call .backward() on b, it would typically error if 'a' was the only source of grad\n",
        "\n",
        "# Operations on a tensor that had requires_grad=True BEFORE no_grad block\n",
        "# But 'b' itself does not require grad, even though 'a' does.\n",
        "# This illustrates that operations within no_grad() don't contribute to the graph.\n",
        "\n",
        "# Let's create a scenario where no_grad is used for inference\n",
        "model_output = torch.tensor(10.0, requires_grad=True)\n",
        "loss_target = torch.tensor(12.0)\n",
        "\n",
        "with torch.no_grad():\n",
        "    # During inference, we don't need to compute gradients for these ops\n",
        "    prediction = model_output * 0.5 + 3 # O(1)\n",
        "    print(f\"\\nPrediction during inference (no_grad): {prediction}\")\n",
        "    print(f\"Prediction requires grad? {prediction.requires_grad}\")\n",
        "\n",
        "# Outside no_grad, ops resume tracking gradients\n",
        "model_output_train = torch.tensor(10.0, requires_grad=True)\n",
        "loss_train = (model_output_train - loss_target)**2 # O(1)\n",
        "print(f\"Loss for training (with grad tracking): {loss_train}\")\n",
        "print(f\"Loss requires grad? {loss_train.requires_grad}\")\n",
        "loss_train.backward()\n",
        "print(f\"Gradient of model_output_train: {model_output_train.grad}\") # Expected: 2 * (10 - 12) = -4"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- PyTorch Tensor Operations Demonstration ---\n",
            "\n",
            "### Tensor Creation\n",
            "Tensor from list:\n",
            "tensor([[1, 2],\n",
            "        [3, 4]])\n",
            "Tensor from NumPy array:\n",
            "tensor([[5, 6],\n",
            "        [7, 8]])\n",
            "Zeros tensor:\n",
            "tensor([[0., 0., 0.],\n",
            "        [0., 0., 0.]])\n",
            "Ones tensor:\n",
            "tensor([[1., 1.],\n",
            "        [1., 1.]])\n",
            "Random tensor:\n",
            "tensor([[0.0853, 0.2650, 0.1850],\n",
            "        [0.8457, 0.5419, 0.9788],\n",
            "        [0.4478, 0.6083, 0.1747]])\n",
            "Empty tensor (uninitialized values):\n",
            "tensor([[1.7860e+25, 1.6930e+22, 3.0386e+29, 6.9138e+28]])\n",
            "Arange tensor: tensor([0, 1, 2, 3, 4])\n",
            "Linspace tensor: tensor([ 0.0000,  2.5000,  5.0000,  7.5000, 10.0000])\n",
            "\n",
            "Properties of tensor_from_list: shape=torch.Size([2, 2]), dtype=torch.int64, device=cpu\n",
            "\n",
            "### Basic Arithmetic Operations\n",
            "t1:\n",
            "tensor([[1., 2.],\n",
            "        [3., 4.]])\n",
            "t2:\n",
            "tensor([[5., 6.],\n",
            "        [7., 8.]])\n",
            "Element-wise sum:\n",
            "tensor([[ 6.,  8.],\n",
            "        [10., 12.]])\n",
            "Element-wise product:\n",
            "tensor([[ 5., 12.],\n",
            "        [21., 32.]])\n",
            "Matrix multiplication:\n",
            "tensor([[19., 22.],\n",
            "        [43., 50.]])\n",
            "Scalar addition:\n",
            "tensor([[11., 12.],\n",
            "        [13., 14.]])\n",
            "\n",
            "### Indexing and Slicing\n",
            "Original tensor:\n",
            "tensor([[10, 11, 12],\n",
            "        [13, 14, 15],\n",
            "        [16, 17, 18]])\n",
            "First element: 10\n",
            "First row: tensor([10, 11, 12])\n",
            "Second column: tensor([11, 14, 17])\n",
            "Sub-tensor (rows 0-1, cols 1-2):\n",
            "tensor([[11, 12],\n",
            "        [14, 15]])\n",
            "\n",
            "### Reshaping and Views\n",
            "Original 1D tensor: tensor([1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
            "Reshaped to 3x3:\n",
            "tensor([[1, 2, 3],\n",
            "        [4, 5, 6],\n",
            "        [7, 8, 9]])\n",
            "View (flattened): tensor([1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
            "Original tensor after view modification:\n",
            "tensor([[99,  2,  3],\n",
            "        [ 4,  5,  6],\n",
            "        [ 7,  8,  9]])\n",
            "\n",
            "### Automatic Differentiation (Autograd)\n",
            "x: 2.0, y: 3.0\n",
            "z = x^2 + y*x + 5 = 15.0\n",
            "Gradient of z with respect to x (dz/dx): 7.0\n",
            "Gradient of z with respect to y (dz/dy): 2.0\n",
            "\n",
            "--- Demonstrating torch.no_grad() ---\n",
            "Tensor 'a' (requires_grad=True): 5.0\n",
            "Tensor 'b' (created inside no_grad, requires_grad=False): 10.0\n",
            "\n",
            "Prediction during inference (no_grad): 8.0\n",
            "Prediction requires grad? False\n",
            "Loss for training (with grad tracking): 4.0\n",
            "Loss requires grad? True\n",
            "Gradient of model_output_train: -4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9a790fa1"
      },
      "source": [
        "### JAX Functional Programming Patterns\n",
        "\n",
        "JAX is a high-performance numerical computing library for Python, designed for high-performance machine learning research. It combines NumPy's familiar API with automatic differentiation, JIT compilation, and the ability to run on accelerators (GPUs/TPUs). Its core design principles are functional programming and explicit transformations.\n",
        "\n",
        "#### 1. Just-In-Time Compilation (`jax.jit`)\n",
        "\n",
        "**Concept:** `jax.jit` (Just-In-Time compilation) compiles Python functions into highly optimized XLA (Accelerated Linear Algebra) computations. This significantly speeds up numerical operations by removing Python interpreter overhead and allowing for aggressive compiler optimizations, especially for functions that perform many array operations.\n",
        "\n",
        "**How it Works:** When a JAX function is decorated with `@jax.jit`, the first time it's called with a specific input signature (shape and dtype), JAX traces the Python code, converts it into an XLA computation graph, compiles it, and caches the result. Subsequent calls with compatible inputs execute the compiled XLA code directly.\n",
        "\n",
        "**Performance Benefits:**\n",
        "-   **Speed:** Dramatically faster execution for computationally intensive functions, especially on accelerators.\n",
        "-   **Reduced Overhead:** Eliminates Python loop and dispatch overhead.\n",
        "\n",
        "**Complexity:**\n",
        "-   **First Call (Compilation):** Can be slow, as it involves tracing and compilation. This overhead is amortized over subsequent calls.\n",
        "-   **Subsequent Calls:** The actual execution time of the compiled XLA code is highly optimized, often reducing operations from O(N) Python operations to much faster C++/CUDA execution. The asymptotic complexity (e.g., O(N) for element-wise, O(N^3) for matrix multiplication) remains, but the constant factor is drastically smaller.\n",
        "\n",
        "#### 2. Automatic Differentiation (`jax.grad`)\n",
        "\n",
        "**Concept:** `jax.grad` is JAX's powerful tool for automatic differentiation. It can transform a Python function that computes a scalar output into a new function that computes the gradient of the original function with respect to its inputs.\n",
        "\n",
        "**How it Works:** Similar to PyTorch's autograd, JAX traces the forward pass of a function to build a computational graph. When `grad` is applied, it uses reverse-mode automatic differentiation (backpropagation) to efficiently compute derivatives. JAX functions are pure functions, which makes differentiation more robust.\n",
        "\n",
        "**Performance Benefits:**\n",
        "-   **Efficiency:** Highly optimized for computing gradients, especially for complex functions and deep neural networks.\n",
        "-   **Flexibility:** Can compose with `jit`, `vmap`, and `pmap` for further optimizations.\n",
        "\n",
        "**Complexity:**\n",
        "-   **Time Complexity:** The time complexity to compute gradients using `grad` is generally proportional to the time complexity of the forward pass, often within a small constant factor (e.g., 2-4x).\n",
        "-   **Space Complexity:** The space complexity is also proportional to the forward pass, as it needs to store intermediate values for backpropagation, typically similar to the forward pass or a small constant factor more.\n",
        "\n",
        "#### 3. Automatic Vectorization (`jax.vmap`)\n",
        "\n",
        "**Concept:** `jax.vmap` (vectorized map) is a powerful transformation that automatically vectorizes a function across an arbitrary axis of its inputs. This allows you to write functions that operate on single examples and then effortlessly apply them to batches of examples without explicit batching loops.\n",
        "\n",
        "**How it Works:** `vmap` takes a function that expects non-batched inputs and returns a new function that operates on batched inputs. It effectively"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7d2c2096",
        "outputId": "3c7cb3bf-5d87-40b6-9cda-262dcbd31a77"
      },
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import numpy as np\n",
        "import timeit\n",
        "\n",
        "print('--- JAX Functional Programming Patterns Demonstration ---')\n",
        "\n",
        "# 1. Just-In-Time Compilation (jax.jit)\n",
        "print('\\n### 1. jax.jit: Just-In-Time Compilation')\n",
        "\n",
        "def sum_and_square(x):\n",
        "    # This function will be JIT-compiled\n",
        "    return jnp.sum(x)**2\n",
        "\n",
        "jitted_sum_and_square = jax.jit(sum_and_square)\n",
        "\n",
        "# Data for demonstration\n",
        "key = jax.random.PRNGKey(0)\n",
        "long_array = jax.random.normal(key, (10**6,))\n",
        "\n",
        "# First call includes compilation overhead\n",
        "print(\"First call (includes compilation) for 10^6 elements...\")\n",
        "start_time_jit_first = timeit.default_timer()\n",
        "result_jit_first = jitted_sum_and_square(long_array)\n",
        "end_time_jit_first = timeit.default_timer()\n",
        "print(f\"  JIT first call time: {end_time_jit_first - start_time_jit_first:.6f} seconds\")\n",
        "\n",
        "# Subsequent calls are faster due to caching\n",
        "print(\"Subsequent calls (compiled code) for 10^6 elements...\")\n",
        "start_time_jit_subsequent = timeit.default_timer()\n",
        "result_jit_subsequent = jitted_sum_and_square(long_array)\n",
        "end_time_jit_subsequent = timeit.default_timer()\n",
        "print(f\"  JIT subsequent call time: {end_time_jit_subsequent - start_time_jit_subsequent:.6f} seconds\")\n",
        "\n",
        "# Compare with non-JIT (pure Python + JAX ops, still faster than pure Python due to JAX ops)\n",
        "def raw_sum_and_square(x):\n",
        "    return jnp.sum(x)**2\n",
        "\n",
        "print(\"Non-JIT calculation for 10^6 elements...\")\n",
        "start_time_raw = timeit.default_timer()\n",
        "result_raw = raw_sum_and_square(long_array)\n",
        "end_time_raw = timeit.default_timer()\n",
        "print(f\"  Non-JIT time: {end_time_raw - start_time_raw:.6f} seconds\")\n",
        "\n",
        "print(f\"  JIT-compiled output: {result_jit_subsequent}\")\n",
        "\n",
        "# Complexity Analysis for jax.jit:\n",
        "# Time Complexity: First call O(P + C) where P is Python tracing time, C is XLA compilation time. Subsequent calls O(N) where N is data size, with much smaller constant factor due to optimized XLA execution.\n",
        "# Space Complexity: O(N) for data, plus overhead for compiled XLA graph.\n",
        "\n",
        "# 2. Automatic Differentiation (jax.grad)\n",
        "print('\\n### 2. jax.grad: Automatic Differentiation')\n",
        "\n",
        "# Define a simple scalar function\n",
        "def f(x):\n",
        "    return x**2 + 2*x + 1 # (x+1)^2\n",
        "\n",
        "# Compute the gradient of f(x) using jax.grad\n",
        "grad_f = jax.grad(f) # O(1) to create the gradient function, then proportional to f(x) for actual computation\n",
        "\n",
        "x_val = 3.0\n",
        "# f'(x) = 2x + 2\n",
        "# f'(3) = 2*3 + 2 = 8\n",
        "\n",
        "# Compute the gradient at x=3.0\n",
        "df_dx = grad_f(x_val)\n",
        "print(f\"Function f(x) = x^2 + 2x + 1, at x={x_val}, f(x)={f(x_val)}\")\n",
        "print(f\"Gradient df/dx at x={x_val}: {df_dx}\")\n",
        "\n",
        "# Combined with jit for performance\n",
        "@jax.jit\n",
        "def jitted_f(x):\n",
        "    return x**2 + 2*x + 1\n",
        "\n",
        "jitted_grad_f = jax.grad(jitted_f)\n",
        "\n",
        "df_dx_jitted = jitted_grad_f(x_val)\n",
        "print(f\"JIT-compiled gradient df/dx at x={x_val}: {df_dx_jitted}\")\n",
        "\n",
        "# Another example with multiple inputs\n",
        "def complex_func(x, y):\n",
        "    return jnp.sin(x) + jnp.cos(y) + x*y\n",
        "\n",
        "# grad of complex_func with respect to x (first argument by default)\n",
        "grad_complex_func_x = jax.grad(complex_func)\n",
        "print(f\"\\nFunction: jnp.sin(x) + jnp.cos(y) + x*y\")\n",
        "x_complex, y_complex = jnp.array(jnp.pi/2), jnp.array(jnp.pi)\n",
        "# d/dx(sin(x) + cos(y) + xy) = cos(x) + y\n",
        "# d/dx at x=pi/2, y=pi: cos(pi/2) + pi = 0 + pi = pi\n",
        "print(f\"x={x_complex:.2f}, y={y_complex:.2f}\")\n",
        "print(f\"Gradient d/dx at ({x_complex:.2f}, {y_complex:.2f}): {grad_complex_func_x(x_complex, y_complex)}\")\n",
        "\n",
        "# grad of complex_func with respect to y (second argument)\n",
        "grad_complex_func_y = jax.grad(complex_func, argnums=1)\n",
        "# d/dy(sin(x) + cos(y) + xy) = -sin(y) + x\n",
        "# d/dy at x=pi/2, y=pi: -sin(pi) + pi/2 = 0 + pi/2 = pi/2\n",
        "print(f\"Gradient d/dy at ({x_complex:.2f}, {y_complex:.2f}): {grad_complex_func_y(x_complex, y_complex)}\")\n",
        "\n",
        "# Complexity Analysis for jax.grad:\n",
        "# Time Complexity: O(T_forward) where T_forward is time for forward pass, typically 2-4x T_forward for computing gradients.\n",
        "# Space Complexity: O(S_forward) for storing intermediate values for backpropagation, typically similar to S_forward.\n",
        "\n",
        "# 3. Automatic Vectorization (jax.vmap)\n",
        "print('\\n### 3. jax.vmap: Automatic Vectorization')\n",
        "\n",
        "# Define a function that operates on a single element or vector\n",
        "def polynomial(x):\n",
        "    return 3 * x**2 + 2 * x + 1\n",
        "\n",
        "# Test with a single input\n",
        "single_input = jnp.array(2.0)\n",
        "print(f\"Polynomial for single input {single_input}: {polynomial(single_input)}\")\n",
        "\n",
        "# Create a batch of inputs\n",
        "batch_inputs = jnp.array([1.0, 2.0, 3.0, 4.0])\n",
        "\n",
        "# Apply vmap to vectorize the polynomial function across the first axis (default)\n",
        "vmap_polynomial = jax.vmap(polynomial)\n",
        "\n",
        "# The vmapped function now directly accepts a batch and returns a batch\n",
        "batched_output = vmap_polynomial(batch_inputs)\n",
        "print(f\"\\nBatch inputs: {batch_inputs}\")\n",
        "print(f\"Vmapped polynomial output: {batched_output}\")\n",
        "\n",
        "# Compare with explicit loop (less efficient in JAX context)\n",
        "loop_output = jnp.array([polynomial(x) for x in batch_inputs])\n",
        "print(f\"Loop-based polynomial output: {loop_output}\")\n",
        "\n",
        "# vmap with multiple inputs (specify which axes to map over)\n",
        "def dot_product(vec1, vec2):\n",
        "    return jnp.dot(vec1, vec2)\n",
        "\n",
        "batch_vec1 = jnp.array([[1, 2], [3, 4], [5, 6]]) # shape (3, 2)\n",
        "batch_vec2 = jnp.array([[7, 8], [9, 10], [11, 12]]) # shape (3, 2)\n",
        "\n",
        "# vmap both inputs along their 0-th axis\n",
        "vmap_dot_product = jax.vmap(dot_product)\n",
        "batched_dot_products = vmap_dot_product(batch_vec1, batch_vec2)\n",
        "print(f\"\\nBatch vec1:\\n{batch_vec1}\")\n",
        "print(f\"Batch vec2:\\n{batch_vec2}\")\n",
        "print(f\"Vmapped dot products (each row is a dot product): {batched_dot_products}\")\n",
        "# Expected: [1*7+2*8, 3*9+4*10, 5*11+6*12] = [7+16, 27+40, 55+72] = [23, 67, 127]\n",
        "\n",
        "# Complexity Analysis for jax.vmap:\n",
        "# Time Complexity: Effectively O(N_batch * T_single_example) but highly optimized by JAX to avoid explicit looping, often performing as fast as a single batched operation on the accelerator.\n",
        "# Space Complexity: O(N_batch * S_single_example) as it processes a batch of data, but avoids duplicating the function's internal structure for each example.\n"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- JAX Functional Programming Patterns Demonstration ---\n",
            "\n",
            "### 1. jax.jit: Just-In-Time Compilation\n",
            "First call (includes compilation) for 10^6 elements...\n",
            "  JIT first call time: 0.142882 seconds\n",
            "Subsequent calls (compiled code) for 10^6 elements...\n",
            "  JIT subsequent call time: 0.000271 seconds\n",
            "Non-JIT calculation for 10^6 elements...\n",
            "  Non-JIT time: 0.112898 seconds\n",
            "  JIT-compiled output: 24788.373046875\n",
            "\n",
            "### 2. jax.grad: Automatic Differentiation\n",
            "Function f(x) = x^2 + 2x + 1, at x=3.0, f(x)=16.0\n",
            "Gradient df/dx at x=3.0: 8.0\n",
            "JIT-compiled gradient df/dx at x=3.0: 8.0\n",
            "\n",
            "Function: jnp.sin(x) + jnp.cos(y) + x*y\n",
            "x=1.57, y=3.14\n",
            "Gradient d/dx at (1.57, 3.14): 3.1415927410125732\n",
            "Gradient d/dy at (1.57, 3.14): 1.5707964897155762\n",
            "\n",
            "### 3. jax.vmap: Automatic Vectorization\n",
            "Polynomial for single input 2.0: 17.0\n",
            "\n",
            "Batch inputs: [1. 2. 3. 4.]\n",
            "Vmapped polynomial output: [ 6. 17. 34. 57.]\n",
            "Loop-based polynomial output: [ 6. 17. 34. 57.]\n",
            "\n",
            "Batch vec1:\n",
            "[[1 2]\n",
            " [3 4]\n",
            " [5 6]]\n",
            "Batch vec2:\n",
            "[[ 7  8]\n",
            " [ 9 10]\n",
            " [11 12]]\n",
            "Vmapped dot products (each row is a dot product): [ 23  67 127]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7e13c869"
      },
      "source": [
        "### TensorFlow/Keras Model Optimization Algorithms\n",
        "\n",
        "TensorFlow and Keras provide a high-level API for building and training machine learning models. Optimization algorithms are crucial for training these models efficiently, minimizing loss functions, and improving generalization.\n",
        "\n",
        "#### 1. Optimizers (Gradient Descent Variants)\n",
        "\n",
        "**Concept:** Optimizers are algorithms used to modify the attributes of the neural network, such as weights and learning rate, in order to reduce the loss function. They are based on the principle of gradient descent, where parameters are iteratively adjusted in the direction opposite to the gradient of the loss function.\n",
        "\n",
        "**Common Optimizers:**\n",
        "-   **SGD (Stochastic Gradient Descent):** Updates model parameters using the gradient of a single training example or a mini-batch. It can be noisy but can escape local minima. Adding `momentum` helps accelerate SGD in the right direction and dampens oscillations.\n",
        "-   **Adam (Adaptive Moment Estimation):** Combines the advantages of two other extensions of SGD: AdaGrad and RMSprop. It computes adaptive learning rates for each parameter, making it very efficient for many tasks.\n",
        "-   **RMSprop (Root Mean Square Propagation):** Divides the learning rate by an exponentially decaying average of squared gradients. It helps in dealing with vanishing/exploding gradients.\n",
        "-   **AdaGrad (Adaptive Gradient Algorithm):** Adapts the learning rate to the parameters, performing smaller updates for parameters associated with frequently occurring features and larger updates for parameters associated with infrequent features.\n",
        "\n",
        "**Operational Complexity (per update step):**\n",
        "-   **O(N)**, where N is the number of parameters in the model. Each parameter's gradient needs to be computed and updated. The actual computation involves matrix multiplications (e.g., in backpropagation) which contribute significantly to the constant factor but the number of updates scales linearly with the number of parameters.\n",
        "\n",
        "#### 2. Loss Functions\n",
        "\n",
        "**Concept:** A loss function (or cost function) quantifies the difference between the predicted output of a model and the true target values. The goal of training is to minimize this loss.\n",
        "\n",
        "**Common Loss Functions:**\n",
        "-   **Mean Squared Error (MSE):** `(y_true - y_pred)^2`. Commonly used for regression tasks.\n",
        "-   **Mean Absolute Error (MAE):** `|y_true - y_pred|`. Also for regression, less sensitive to outliers than MSE.\n",
        "-   **Binary Cross-Entropy:** Used for binary classification tasks, measuring the performance of a classification model whose output is a probability value between 0 and 1.\n",
        "-   **Categorical Cross-Entropy / Sparse Categorical Cross-Entropy:** Used for multi-class classification tasks. Categorical Cross-Entropy is used when target labels are one-hot encoded, while Sparse Categorical Cross-Entropy is used when target labels are integers.\n",
        "\n",
        "**Operational Complexity (per batch):**\n",
        "-   **O(BatchSize * OutputDimension)** for computing the loss over a batch. The gradient computation for the loss function also scales similarly.\n",
        "\n",
        "#### 3. Regularization Techniques\n",
        "\n",
        "**Concept:** Regularization techniques are used to prevent overfitting, which occurs when a model learns the training data too well, leading to poor performance on unseen data. They add a penalty to the loss function for large coefficient values or reduce the complexity of the model.\n",
        "\n",
        "**Common Techniques:**\n",
        "-   **L1 Regularization (Lasso):** Adds the absolute value of the magnitude of coefficients to the loss function. It can lead to sparse models, effectively performing feature selection.\n",
        "-   **L2 Regularization (Ridge / Weight Decay):** Adds the squared magnitude of coefficients to the loss function. It encourages smaller weights, distributing the error across all features.\n",
        "-   **Dropout:** Randomly sets a fraction of input units to 0 at each update step during training. This prevents complex co-adaptations on training data, forcing the network to learn more robust features.\n",
        "-   **Early Stopping:** Monitors the performance of the model on a validation set during training and stops training when the performance on the validation set starts to degrade (i.e., loss increases or accuracy decreases), even if the training loss is still decreasing.\n",
        "\n",
        "**Operational Complexity:**\n",
        "-   **L1/L2 Regularization:** Adds O(N) complexity to the loss calculation and gradient computation per update step, where N is the number of parameters.\n",
        "-   **Dropout:** Incurs a small overhead during training (random mask generation) but no overhead during inference. The forward pass becomes O(N) as usual, and backpropagation also O(N).\n",
        "-   **Early Stopping:** Primarily monitoring overhead, which is O(BatchSize * OutputDimension) per validation step (to compute validation loss/metric)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "77edfe41",
        "outputId": "98cfb23f-8790-4155-e19b-20f17a6976b1"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import numpy as np\n",
        "\n",
        "print('--- TensorFlow/Keras Model Optimization Demonstration ---')\n",
        "\n",
        "# 1. Optimizers (Gradient Descent Variants)\n",
        "print('\\n### 1. Optimizers')\n",
        "\n",
        "# Let's define a simple model for demonstration\n",
        "def create_simple_model():\n",
        "    model = keras.Sequential([\n",
        "        keras.Input(shape=(10,)), # Use keras.Input as the first layer\n",
        "        layers.Dense(10, activation='relu'),\n",
        "        layers.Dense(1, activation='linear')\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "# Example of compiling a model with different optimizers\n",
        "model_sgd = create_simple_model()\n",
        "# SGD Optimizer: O(N_params) per update step for gradient computation and parameter update\n",
        "model_sgd.compile(optimizer=keras.optimizers.SGD(learning_rate=0.01), loss='mse')\n",
        "print(\"Model compiled with SGD optimizer (basic gradient descent).\")\n",
        "\n",
        "model_adam = create_simple_model()\n",
        "# Adam Optimizer: O(N_params) per update step, involves more computations for adaptive learning rates\n",
        "model_adam.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001), loss='mse')\n",
        "print(\"Model compiled with Adam optimizer (adaptive learning rate).\")\n",
        "\n",
        "# Operational Complexity (per update step for optimizers): O(N_params) for parameter updates and gradient computations.\n",
        "\n",
        "\n",
        "# 2. Loss Functions\n",
        "print('\\n### 2. Loss Functions')\n",
        "\n",
        "# Example of compiling a model with different loss functions\n",
        "# For Regression:\n",
        "model_mse = create_simple_model()\n",
        "# MSE Loss: O(BatchSize * OutputDim) to compute loss and its gradient\n",
        "model_mse.compile(optimizer='adam', loss='mse')\n",
        "print(\"Model compiled with Mean Squared Error (MSE) loss (for regression).\")\n",
        "\n",
        "model_mae = create_simple_model()\n",
        "# MAE Loss: O(BatchSize * OutputDim) to compute loss and its gradient\n",
        "model_mae.compile(optimizer='adam', loss='mae')\n",
        "print(\"Model compiled with Mean Absolute Error (MAE) loss (for regression).\")\n",
        "\n",
        "# For Classification (need to adjust output layer and data for proper demo)\n",
        "model_binary_ce = keras.Sequential([\n",
        "    keras.Input(shape=(10,)), # Use keras.Input here as well\n",
        "    layers.Dense(10, activation='relu'),\n",
        "    layers.Dense(1, activation='sigmoid') # Sigmoid for binary classification\n",
        "])\n",
        "# Binary Cross-Entropy Loss: O(BatchSize * OutputDim) to compute loss and its gradient\n",
        "model_binary_ce.compile(optimizer='adam', loss='binary_crossentropy')\n",
        "print(\"Model compiled with Binary Cross-Entropy loss (for binary classification).\")\n",
        "\n",
        "# Operational Complexity (per batch for loss functions): O(BatchSize * OutputDim) for computing loss and its gradient.\n",
        "\n",
        "\n",
        "# 3. Regularization Techniques\n",
        "print('\\n### 3. Regularization Techniques')\n",
        "\n",
        "# L2 Regularization (Weight Decay)\n",
        "# Complexity: Adds O(N_params) to loss and gradient computation per update step.\n",
        "model_l2 = keras.Sequential([\n",
        "    keras.Input(shape=(10,)), # Use keras.Input here\n",
        "    layers.Dense(10, activation='relu', kernel_regularizer=keras.regularizers.l2(0.01)),\n",
        "    layers.Dense(1, activation='linear', kernel_regularizer=keras.regularizers.l2(0.01))\n",
        "])\n",
        "model_l2.compile(optimizer='adam', loss='mse')\n",
        "print(\"Model with L2 regularization on kernel weights.\")\n",
        "\n",
        "# Dropout\n",
        "# Complexity: Small overhead during training (random mask generation) O(N_layer_neurons) per layer. No overhead during inference.\n",
        "model_dropout = keras.Sequential([\n",
        "    keras.Input(shape=(10,)), # Use keras.Input here\n",
        "    layers.Dense(10, activation='relu'),\n",
        "    layers.Dropout(0.5), # Dropout layer\n",
        "    layers.Dense(1, activation='linear')\n",
        "])\n",
        "model_dropout.compile(optimizer='adam', loss='mse')\n",
        "print(\"Model with Dropout regularization.\")\n",
        "\n",
        "# Early Stopping (a Callback, not a direct model compilation parameter)\n",
        "# Complexity: Monitoring overhead, O(BatchSize * OutputDim) per validation step.\n",
        "print(\"Early Stopping is implemented as a callback during model training.\")\n",
        "early_stopping_callback = keras.callbacks.EarlyStopping(\n",
        "    monitor='val_loss', # Monitor validation loss\n",
        "    patience=3,         # Stop if val_loss doesn't improve for 3 epochs\n",
        "    restore_best_weights=True\n",
        ")\n",
        "print(\"EarlyStopping callback created: will monitor 'val_loss' with patience=3.\")\n",
        "\n",
        "# Demonstration of training with regularization and callback (simplified data)\n",
        "print('\\n--- Training Demonstration with Regularization and Early Stopping ---')\n",
        "# Generate some dummy data\n",
        "X_train = np.random.rand(100, 10)\n",
        "y_train = np.random.rand(100, 1)\n",
        "X_val = np.random.rand(20, 10)\n",
        "y_val = np.random.rand(20, 1)\n",
        "\n",
        "print(\"Training model with L2 regularization and Early Stopping callback...\")\n",
        "history_l2 = model_l2.fit(X_train, y_train, epochs=10, batch_size=10,\n",
        "                           validation_data=(X_val, y_val), verbose=0,\n",
        "                           callbacks=[early_stopping_callback])\n",
        "print(f\"Training finished after {len(history_l2.history['loss'])} epochs (due to Early Stopping/max epochs).\")\n",
        "print(f\"Final training loss: {history_l2.history['loss'][-1]:.4f}\")\n",
        "print(f\"Final validation loss: {history_l2.history['val_loss'][-1]:.4f}\")\n",
        "\n",
        "print('\\n--- Best Practices for TensorFlow/Keras Optimization ---')\n",
        "print('- Select optimizers based on problem type and dataset characteristics (Adam for general, SGD for fine-tuning).')\n",
        "print('- Choose loss functions appropriate for the task (MSE for regression, Cross-Entropy for classification).')\n",
        "print('- Apply regularization techniques (L1, L2, Dropout, Early Stopping) to prevent overfitting, especially on complex models.')\n",
        "print('- Tune hyperparameters (learning rate, regularization strengths) using validation sets.')"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- TensorFlow/Keras Model Optimization Demonstration ---\n",
            "\n",
            "### 1. Optimizers\n",
            "Model compiled with SGD optimizer (basic gradient descent).\n",
            "Model compiled with Adam optimizer (adaptive learning rate).\n",
            "\n",
            "### 2. Loss Functions\n",
            "Model compiled with Mean Squared Error (MSE) loss (for regression).\n",
            "Model compiled with Mean Absolute Error (MAE) loss (for regression).\n",
            "Model compiled with Binary Cross-Entropy loss (for binary classification).\n",
            "\n",
            "### 3. Regularization Techniques\n",
            "Model with L2 regularization on kernel weights.\n",
            "Model with Dropout regularization.\n",
            "Early Stopping is implemented as a callback during model training.\n",
            "EarlyStopping callback created: will monitor 'val_loss' with patience=3.\n",
            "\n",
            "--- Training Demonstration with Regularization and Early Stopping ---\n",
            "Training model with L2 regularization and Early Stopping callback...\n",
            "Training finished after 10 epochs (due to Early Stopping/max epochs).\n",
            "Final training loss: 0.5688\n",
            "Final validation loss: 0.4866\n",
            "\n",
            "--- Best Practices for TensorFlow/Keras Optimization ---\n",
            "- Select optimizers based on problem type and dataset characteristics (Adam for general, SGD for fine-tuning).\n",
            "- Choose loss functions appropriate for the task (MSE for regression, Cross-Entropy for classification).\n",
            "- Apply regularization techniques (L1, L2, Dropout, Early Stopping) to prevent overfitting, especially on complex models.\n",
            "- Tune hyperparameters (learning rate, regularization strengths) using validation sets.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48b76f6c"
      },
      "source": [
        "## Senior-Level ML/Framework Applications - Real-world Patterns\n",
        "\n",
        "### Subtask:\n",
        "Explore common real-world data science interview patterns and scenarios, linking theoretical knowledge from algorithms and ML frameworks to practical application and problem-solving. This includes discussions on system design for ML, A/B testing, and model deployment considerations.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8de25eb0"
      },
      "source": [
        "### Real-world Data Science Interview Patterns: Bridging Theory and Practice\n",
        "\n",
        "Data science interviews, especially at senior levels, increasingly move beyond pure algorithmic challenges to assessing a candidate's ability to apply theoretical knowledge to practical, real-world scenarios. This involves evaluating your understanding of how machine learning models are designed, developed, validated, and deployed in production environments. It tests your holistic understanding of the ML lifecycle and your ability to make engineering and business trade-offs.\n",
        "\n",
        "Here, we explore common real-world patterns:\n",
        "\n",
        "#### 1. System Design for ML\n",
        "\n",
        "**Importance:** In data science interviews, especially for roles like Staff Data Scientist or Machine Learning Engineer, system design for ML is critical. It assesses your ability to architect robust, scalable, and maintainable machine learning solutions. This moves beyond just building a model to thinking about the entire ecosystem it operates within.\n",
        "\n",
        "**Key Components & Considerations:**\n",
        "-   **Data Pipelines:** How is data ingested, cleaned, transformed, and stored? (e.g., using technologies like Apache Kafka, Spark, Flink for real-time processing; ETL/ELT processes; data warehousing concepts).\n",
        "    -   **Theoretical Link:** Efficient data processing often relies on optimized algorithms (sorting, hashing) and data structures (queues for streaming, distributed file systems).\n",
        "-   **Model Training Infrastructure:** How are models trained, often on large datasets? This includes managing compute resources (CPUs, GPUs, TPUs), distributed training frameworks (e.g., Horovod, Ray), and experiment tracking (e.g., MLflow, Weights & Biases).\n",
        "    -   **Theoretical Link:** Understanding optimization algorithms (SGD, Adam, etc.) and their parallelization strategies (e.g., data parallelism, model parallelism in distributed training).\n",
        "-   **Model Serving Systems:** How are predictions made available to end-users or other services? This involves considerations for latency, throughput, and online vs. offline inference.\n",
        "    -   **Theoretical Link:** Efficient data structures (hash maps for quick lookups), search algorithms (binary search for decision trees), and understanding trade-offs between model complexity and inference speed. ML frameworks like PyTorch and TensorFlow provide optimized inference engines (TorchScript, TensorFlow Lite/Serving).\n",
        "-   **Monitoring:** How do you track model performance (accuracy, latency, drift) in production? This includes data quality checks, model output monitoring, and alerting mechanisms.\n",
        "    -   **Theoretical Link:** Statistical concepts for detecting drift, anomaly detection algorithms, and understanding metric calculation.\n",
        "-   **Scalability & Reliability:** How does the system handle increased load? What happens if a component fails? This involves load balancing, redundancy, and fault tolerance.\n",
        "    -   **Theoretical Link:** Distributed algorithms, queueing theory, and understanding system bottlenecks.\n",
        "\n",
        "#### 2. A/B Testing\n",
        "\n",
        "**Purpose & Core Principles:** A/B testing (or online controlled experiments) is a method of comparing two versions of a product or feature (A and B) to determine which one performs better. It's crucial for data-driven decision-making in product development, marketing, and feature rollouts.\n",
        "\n",
        "**Key Aspects:**\n",
        "-   **Hypothesis Formulation:** Clearly define the null and alternative hypotheses before running the experiment (e.g., \"H0: New feature has no effect on CTR; H1: New feature increases CTR\").\n",
        "    -   **Theoretical Link:** Foundation in statistical hypothesis testing.\n",
        "-   **Experimental Design:** Define target audience, segmentation, randomization strategy (to ensure groups are comparable), sample size calculation (power analysis to detect a meaningful effect).\n",
        "    -   **Theoretical Link:** Statistical concepts of random sampling, sample size estimation, and experimental validity.\n",
        "-   **Metric Selection:** Choose relevant primary and secondary metrics to evaluate the impact (e.g., click-through rate, conversion rate, time on page, revenue per user).\n",
        "    -   **Theoretical Link:** Understanding different types of metrics, their distributions, and how to aggregate them.\n",
        "-   **Statistical Significance:** Determine if the observed difference between groups is likely due to the change or due to random chance, using p-values and confidence intervals.\n",
        "    -   **Theoretical Link:** Inferential statistics, understanding t-tests, z-tests, chi-squared tests, and non-parametric tests.\n",
        "-   **Potential Pitfalls:** Seasonality, novelty effects, network effects, S.R.M. (Sample Ratio Mismatch), multiple testing problem, and ethical considerations.\n",
        "    -   **Theoretical Link:** Advanced statistical concepts and critical thinking.\n",
        "\n",
        "#### 3. Model Deployment Considerations\n",
        "\n",
        "**Challenges & Considerations:** Moving a machine learning model from a development environment (notebook, local machine) to a production system where it can generate predictions for real-world applications involves significant engineering challenges.\n",
        "\n",
        "**Key Aspects:**\n",
        "-   **API Design:** How will other services or applications interact with the model? Designing RESTful APIs, gRPC endpoints, or message queues for asynchronous processing.\n",
        "    -   **Theoretical Link:** Understanding communication protocols, data serialization (e.g., JSON, Protocol Buffers), and latency implications.\n",
        "-   **Latency & Throughput:** Minimizing the time it takes for the model to produce a prediction (latency) and maximizing the number of predictions it can make per unit of time (throughput). This involves optimizing model inference, batching requests, and efficient hardware utilization.\n",
        "    -   **Theoretical Link:** Algorithm optimization, understanding hardware accelerators (GPU/TPU) and their parallel processing capabilities. ML frameworks (PyTorch, JAX, TensorFlow) provide tools for model optimization (quantization, pruning, graph compilation).\n",
        "-   **Monitoring:** Continuous tracking of model performance, input data quality, resource utilization, and error rates in production. Setting up alerts for anomalies.\n",
        "    -   **Theoretical Link:** Statistical process control, anomaly detection, and data validation techniques.\n",
        "-   **Versioning & Rollback Strategies:** Managing different versions of models and their associated code/data. The ability to quickly revert to a previous, stable version if a new deployment causes issues.\n",
        "    -   **Theoretical Link:** Software engineering principles, version control systems, and robust deployment pipelines.\n",
        "-   **Infrastructure Choices:** Selecting the right platform for deployment, including cloud platforms (AWS Sagemaker, Google AI Platform, Azure ML), containerization (Docker), and orchestration (Kubernetes).\n",
        "    -   **Theoretical Link:** Distributed systems concepts, resource management, and understanding infrastructure scaling strategies.\n",
        "\n",
        "Throughout these discussions, the theoretical knowledge from **algorithms** (e.g., efficiency, complexity, distributed computing) and **ML frameworks** (e.g., tensor operations, autograd, JIT compilation, optimized data manipulation) directly underpins the practical solutions and best practices in designing, testing, and deploying ML systems."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3785abec"
      },
      "source": [
        "## Final Task\n",
        "\n",
        "### Subtask:\n",
        "Summarize the intensive 1-day preparation course, reiterate the importance of continuous practice, and provide guidance on how to effectively leverage the learned material and problem-solving strategies for HackerRank assessments and future interviews.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d834ae69"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   **Comprehensive Course Structure:** The 1-day crash course for senior data scientists was designed to cover Python fundamentals, essential algorithms (sorting, searching, graph traversal, dynamic programming), progressive problem-solving across various data structures, and senior-level ML/framework applications (NumPy, Pandas, PyTorch/JAX/TensorFlow).\n",
        "*   **Python Fundamentals Reinforced:**\n",
        "    *   A thorough review of core Python syntax, basic data types, operators, and control flow mechanisms was provided, emphasizing their `O(1)` or `O(N)` time complexities.\n",
        "    *   Key built-in data structures (Lists, Tuples, Sets, Dictionaries) were detailed with their characteristics, common operations, and associated time/space complexities (e.g., List `append` as amortized `O(1)`, Set/Dict lookups as average `O(1)`).\n",
        "    *   String manipulation methods and basic regular expressions were demonstrated with their `O(N)` or `O(N*M)` complexities.\n",
        "    *   List comprehensions (`O(N)`) and lambda functions were introduced as concise and efficient Pythonic constructs, with examples demonstrating their use in `map()`, `filter()`, and `sorted()` operations.\n",
        "*   **Algorithm Essentials Covered:**\n",
        "    *   **Sorting:** Merge Sort (stable `O(N log N)` time, `O(N)` space) and Quick Sort (average `O(N log N)` time, `O(log N)` space; worst-case `O(N^2)` time) were explained and implemented.\n",
        "    *   **Searching:** Binary Search (iterative and recursive, `O(log N)` time) was demonstrated on sorted arrays.\n",
        "    *   **Techniques:** The Two-Pointer Technique (`O(N)` time, `O(1)` space) was illustrated for problems like \"Two Sum\" and \"Remove Duplicates.\" The Sliding Window Technique (`O(N)` time, `O(1)` or `O(K)` space) was shown for fixed-size (max sum subarray) and dynamic-size (smallest subarray with sum `>=` target) problems.\n",
        "    *   **Graph Traversal:** Both Breadth-First Search (BFS) and Depth-First Search (DFS) were implemented (recursive and iterative for DFS) with `O(V + E)` time and `O(V)` space complexities.\n",
        "    *   **Dynamic Programming (DP):** The core concepts of overlapping subproblems and optimal substructure were explained. Memoization (top-down) and Tabulation (bottom-up), including space-optimized approaches, were demonstrated using the Fibonacci sequence, all achieving `O(N)` time complexity.\n",
        "*   **Progressive HackerRank Practice:**\n",
        "    *   Specific \"Easy\" problems (\"Python If-Else\", \"List Comprehensions\", \"Finding the Percentage\") were suggested to reinforce Python fundamentals.\n",
        "    *   \"Medium\" problems (\"Merge Two Sorted Lists\", \"Minimum Bribes\", \"Coin Change\", \"Roads and Libraries\") were proposed to apply core algorithms.\n",
        "    *   \"Advanced\" problems (\"Roads and Libraries\", \"Abbreviation\", \"BFS: Shortest Reach in a Graph\") were suggested to challenge senior-level algorithmic thinking.\n",
        "    *   Comprehensive guidance was provided for each difficulty level, emphasizing optimal solutions, meticulous complexity analysis, and strategic use of the provided GitHub repository.\n",
        "*   **Senior-Level ML/Framework Applications:**\n",
        "    *   **NumPy & Pandas:** Advanced NumPy operations like vectorization (e.g., ~39x faster than Python loops for 10^6 elements) and broadcasting were explained with performance benefits. Efficient Pandas techniques including `.loc`/`.iloc` for selection, `map()`/`apply()` for transformations, and optimized `groupby()` operations were demonstrated.\n",
        "    *   **PyTorch:** Key concepts included Tensors (multi-dimensional arrays, GPU capability) and Autograd (dynamic computational graph, `requires_grad`, `.backward()`, `torch.no_grad()`) for automatic differentiation.\n",
        "    *   **JAX:** Functional programming patterns were covered, including `jax.jit` for Just-In-Time compilation (significant speedup after first call), `jax.grad` for automatic differentiation, and `jax.vmap` for automatic vectorization.\n",
        "    *   **TensorFlow/Keras:** Essential model optimization algorithms were reviewed, including various Optimizers (SGD, Adam), Loss Functions (MSE, MAE, Cross-Entropy), and Regularization Techniques (L1/L2, Dropout, Early Stopping).\n",
        "*   **Real-World ML Patterns:** The course concluded by bridging theoretical knowledge to practical scenarios, discussing:\n",
        "    *   **System Design for ML:** Covering data pipelines, training/serving infrastructure, monitoring, and scalability.\n",
        "    *   **A/B Testing:** Detailing hypothesis formulation, experimental design, metric selection, and statistical significance.\n",
        "    *   **Model Deployment Considerations:** Addressing API design, latency, throughput, monitoring, versioning, and infrastructure choices.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   The crash course provides a solid foundation for senior data scientists preparing for HackerRank assessments and technical interviews, covering a broad spectrum of necessary skills from Python fundamentals to advanced ML system design.\n",
        "*   To maximize learning, candidates should prioritize hands-on practice, meticulously analyze solution complexities, and strategically leverage provided resources like the GitHub repository after attempting problems independently, focusing on understanding *why* solutions are optimal rather than just replicating them.\n"
      ]
    }
  ]
}
