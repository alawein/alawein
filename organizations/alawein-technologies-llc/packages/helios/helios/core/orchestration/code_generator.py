"""
Code Generator - Stage 3 of ORCHEX

Generates Python code to execute experimental designs.
"""

from typing import Dict, Any, Optional
from pathlib import Path
from dataclasses import dataclass
import json

from ORCHEX.experimentation.experiment_designer import ExperimentDesign


@dataclass
class GeneratedCode:
    """Generated experiment code"""
    experiment_id: str
    main_script: str
    helper_modules: Dict[str, str]  # filename -> code
    requirements_txt: str
    readme: str
    test_script: str


class CodeGenerator:
    """
    Generate executable Python code from experiment designs

    Features:
    - Template-based code generation
    - Domain-specific implementations
    - Modular structure (main + helpers)
    - Tests and documentation
    - AI-assisted code refinement
    """

    def __init__(self, orchestrator=None, templates_dir: Optional[str] = None):
        """
        Initialize code generator

        Args:
            orchestrator: Optional AI Orchestrator for code refinement
            templates_dir: Directory with code templates
        """
        self.orchestrator = orchestrator
        self.templates_dir = Path(templates_dir) if templates_dir else None

    async def generate_code(
        self,
        design: ExperimentDesign,
        output_dir: Path
    ) -> GeneratedCode:
        """
        Generate complete experiment code from design

        Args:
            design: Experiment design specification
            output_dir: Where to write generated code

        Returns:
            GeneratedCode with all files
        """
        print(f"\nðŸ”§ Generating code for experiment: {design.design_id}")

        # Generate main experiment script
        main_script = self._generate_main_script(design)

        # Generate helper modules
        helper_modules = {}
        if design.experiment_type == "benchmark":
            helper_modules["benchmark_utils.py"] = self._generate_benchmark_utils(design)
        elif design.experiment_type == "ablation":
            helper_modules["ablation_utils.py"] = self._generate_ablation_utils(design)
        elif design.experiment_type == "parameter_sweep":
            helper_modules["sweep_utils.py"] = self._generate_sweep_utils(design)
        elif design.experiment_type == "comparison":
            helper_modules["comparison_utils.py"] = self._generate_comparison_utils(design)

        # Always include common utilities
        helper_modules["metrics.py"] = self._generate_metrics_module(design)
        helper_modules["data_loader.py"] = self._generate_data_loader(design)

        # Generate requirements.txt
        requirements_txt = self._generate_requirements(design)

        # Generate README
        readme = self._generate_readme(design)

        # Generate test script
        test_script = self._generate_test_script(design)

        # Refine with AI if available
        if self.orchestrator:
            main_script = await self._refine_code_with_ai(main_script, design)

        # Save all files
        generated = GeneratedCode(
            experiment_id=design.design_id,
            main_script=main_script,
            helper_modules=helper_modules,
            requirements_txt=requirements_txt,
            readme=readme,
            test_script=test_script
        )

        self._write_code_to_disk(generated, output_dir)

        return generated

    def _generate_main_script(self, design: ExperimentDesign) -> str:
        """Generate main experiment execution script"""

        params_setup = self._generate_parameter_setup(design)
        experiment_loop = self._generate_experiment_loop(design)
        results_collection = self._generate_results_collection(design)

        script = f'''"""
Experiment: {design.design_id}

Hypothesis: {design.hypothesis}

Generated by ORCHEX Code Generator
"""

import numpy as np
import pandas as pd
from pathlib import Path
import json
import time
from typing import Dict, Any, List
import logging

# Local imports
from metrics import compute_metrics
from data_loader import load_data

# Setup logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class Experiment:
    """
    {design.objective}

    Experiment Type: {design.experiment_type}
    Primary Metric: {design.primary_metric}
    """

    def __init__(self, output_dir: str = "./results"):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)

        self.results = []
        self.config = {design.fixed_settings!r}

        logger.info(f"Initialized experiment: {design.design_id}")

{params_setup}

    def run_single_trial(
        self,
        trial_id: int,
        params: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        Run a single experimental trial

        Args:
            trial_id: Unique trial identifier
            params: Parameter configuration for this trial

        Returns:
            Dict with trial results
        """
        logger.info(f"Starting trial {{trial_id}} with params: {{params}}")

        start_time = time.time()

        try:
            # TODO: Implement actual experimental logic here
            # This is a placeholder that should be replaced with domain-specific code

            # Load data
            data = load_data(params.get('dataset', 'default'))

            # Run method
            if params.get('method') == 'proposed':
                result = self._run_proposed_method(data, params)
            else:
                result = self._run_baseline_method(data, params)

            # Compute metrics
            metrics = compute_metrics(
                result,
                primary_metric="{design.primary_metric}",
                secondary_metrics={design.secondary_metrics!r}
            )

            elapsed = time.time() - start_time

            return {{
                'trial_id': trial_id,
                'params': params,
                'metrics': metrics,
                'elapsed_time': elapsed,
                'success': True
            }}

        except Exception as e:
            logger.error(f"Trial {{trial_id}} failed: {{e}}")
            return {{
                'trial_id': trial_id,
                'params': params,
                'success': False,
                'error': str(e)
            }}

    def _run_proposed_method(self, data, params):
        """
        TODO: Implement proposed method

        This is where the hypothesis is actually tested!
        """
        # Placeholder implementation
        np.random.seed(params.get('seed', 42))
        return {{
            'output': np.random.randn(10),
            'score': np.random.rand()
        }}

    def _run_baseline_method(self, data, params):
        """
        TODO: Implement baseline method for comparison
        """
        # Placeholder implementation
        np.random.seed(params.get('seed', 42))
        return {{
            'output': np.random.randn(10),
            'score': np.random.rand() * 0.9  # Slightly worse
        }}

{experiment_loop}

{results_collection}

    def check_success_criteria(self) -> bool:
        """
        Check if experiment met success criteria

        Criteria: {design.success_criteria}
        """
        if not self.results:
            return False

        df = pd.DataFrame(self.results)

        # TODO: Implement success criteria check
        # This is placeholder logic
        primary_scores = df[df['success']]['metrics'].apply(
            lambda m: m.get('{design.primary_metric}', 0)
        )

        if len(primary_scores) == 0:
            return False

        mean_score = primary_scores.mean()
        logger.info(f"Mean {{'{design.primary_metric}'}}: {{mean_score:.3f}}")

        # Success if score > 0.7 (placeholder threshold)
        return mean_score > 0.7


def main():
    """Run complete experiment"""
    print("="*80)
    print(f"EXPERIMENT: {design.design_id}")
    print("="*80)
    print(f"\\nHypothesis: {design.hypothesis}")
    print(f"Objective: {design.objective}")
    print(f"\\nEstimated trials: {design.num_trials}")
    print(f"Estimated duration: {design.estimated_duration}")
    print(f"Estimated cost: ${design.estimated_cost:.2f}")
    print("="*80)
    print()

    # Create experiment
    experiment = Experiment(output_dir="./results/{design.design_id}")

    # Run all trials
    experiment.run_experiment()

    # Save results
    experiment.save_results()

    # Check success
    success = experiment.check_success_criteria()

    print("\\n" + "="*80)
    if success:
        print("âœ“ EXPERIMENT SUCCESSFUL!")
        print(f"  Success criteria met: {design.success_criteria}")
    else:
        print("âœ— Experiment did not meet success criteria")
    print("="*80)

    return 0 if success else 1


if __name__ == "__main__":
    exit(main())
'''

        return script

    def _generate_parameter_setup(self, design: ExperimentDesign) -> str:
        """Generate parameter configuration code"""
        param_configs = []

        for param in design.parameters:
            param_configs.append(f"        '{param.name}': {param.values!r}")

        params_dict = ",\n".join(param_configs)

        return f'''    def setup_parameters(self):
        """Setup parameter grid for experiments"""
        self.parameter_space = {{
{params_dict}
        }}

        logger.info(f"Parameter space: {{len(self.parameter_space)}} dimensions")
        return self.parameter_space'''

    def _generate_experiment_loop(self, design: ExperimentDesign) -> str:
        """Generate experiment loop code"""
        return '''    def run_experiment(self):
        """Run all experimental trials"""
        from itertools import product

        # Setup parameters
        param_space = self.setup_parameters()

        # Generate all parameter combinations
        param_names = list(param_space.keys())
        param_values = [param_space[name] for name in param_names]

        trial_id = 0
        for combo in product(*param_values):
            params = dict(zip(param_names, combo))

            # Run trial
            result = self.run_single_trial(trial_id, params)
            self.results.append(result)

            trial_id += 1

            if trial_id % 10 == 0:
                logger.info(f"Completed {trial_id} trials")

        logger.info(f"âœ“ Completed all {trial_id} trials")'''

    def _generate_results_collection(self, design: ExperimentDesign) -> str:
        """Generate results collection and saving code"""
        return f'''    def save_results(self):
        """Save experimental results"""
        results_file = self.output_dir / "results.json"
        summary_file = self.output_dir / "summary.txt"

        # Save raw results
        with open(results_file, 'w') as f:
            json.dump(self.results, f, indent=2, default=str)

        logger.info(f"âœ“ Saved results: {{results_file}}")

        # Generate summary
        df = pd.DataFrame(self.results)

        summary = f"""
EXPERIMENT SUMMARY
{'='*80}

Experiment ID: {design.design_id}
Hypothesis: {design.hypothesis}

RESULTS:
--------
Total trials: {{len(self.results)}}
Successful trials: {{sum(1 for r in self.results if r['success'])}}
Failed trials: {{sum(1 for r in self.results if not r['success'])}}

PRIMARY METRIC ({design.primary_metric}):
Mean: {{df[df['success']]['metrics'].apply(lambda m: m.get('{design.primary_metric}', 0)).mean():.3f}}
Std: {{df[df['success']]['metrics'].apply(lambda m: m.get('{design.primary_metric}', 0)).std():.3f}}

SUCCESS CRITERIA: {design.success_criteria}
Status: {{self.check_success_criteria()}}

{'='*80}
"""

        with open(summary_file, 'w') as f:
            f.write(summary)

        print(summary)
        logger.info(f"âœ“ Saved summary: {{summary_file}}")'''

    def _generate_metrics_module(self, design: ExperimentDesign) -> str:
        """Generate metrics computation module"""
        return f'''"""
Metrics computation for {design.design_id}
"""

import numpy as np
from typing import Dict, Any, List


def compute_metrics(
    result: Dict[str, Any],
    primary_metric: str = "{design.primary_metric}",
    secondary_metrics: List[str] = {design.secondary_metrics!r}
) -> Dict[str, float]:
    """
    Compute all experimental metrics

    Args:
        result: Raw experimental result
        primary_metric: Name of primary metric
        secondary_metrics: List of secondary metric names

    Returns:
        Dict of metric_name -> value
    """
    metrics = {{}}

    # Compute primary metric
    metrics[primary_metric] = _compute_primary_metric(result, primary_metric)

    # Compute secondary metrics
    for metric_name in secondary_metrics:
        metrics[metric_name] = _compute_secondary_metric(result, metric_name)

    return metrics


def _compute_primary_metric(result: Dict, metric_name: str) -> float:
    """Compute primary metric (placeholder)"""
    # TODO: Implement actual metric computation
    if 'score' in result:
        return float(result['score'])
    return 0.0


def _compute_secondary_metric(result: Dict, metric_name: str) -> float:
    """Compute secondary metric (placeholder)"""
    # TODO: Implement actual metric computation
    return np.random.rand()  # Placeholder
'''

    def _generate_data_loader(self, design: ExperimentDesign) -> str:
        """Generate data loading module"""
        return '''"""
Data loading utilities
"""

import numpy as np
from typing import Any, Dict


def load_data(dataset_name: str = "default") -> Dict[str, Any]:
    """
    Load experimental data

    Args:
        dataset_name: Name of dataset to load

    Returns:
        Dict with data arrays
    """
    # TODO: Implement actual data loading
    # This is a placeholder that should be replaced with real data loading

    np.random.seed(42)

    return {
        'X': np.random.randn(100, 10),
        'y': np.random.randint(0, 2, 100),
        'metadata': {'name': dataset_name}
    }
'''

    def _generate_benchmark_utils(self, design: ExperimentDesign) -> str:
        """Generate benchmark-specific utilities"""
        return '''"""
Benchmark experiment utilities
"""

def run_benchmark_trial(problem_size, algorithm, seed):
    """Run single benchmark trial"""
    # TODO: Implement benchmark logic
    pass
'''

    def _generate_ablation_utils(self, design: ExperimentDesign) -> str:
        """Generate ablation-specific utilities"""
        return '''"""
Ablation study utilities
"""

def ablate_component(model, component_name):
    """Remove component from model"""
    # TODO: Implement ablation logic
    pass
'''

    def _generate_sweep_utils(self, design: ExperimentDesign) -> str:
        """Generate parameter sweep utilities"""
        return '''"""
Parameter sweep utilities
"""

from itertools import product

def generate_parameter_grid(param_space):
    """Generate all parameter combinations"""
    names = list(param_space.keys())
    values = [param_space[name] for name in names]

    for combo in product(*values):
        yield dict(zip(names, combo))
'''

    def _generate_comparison_utils(self, design: ExperimentDesign) -> str:
        """Generate comparison-specific utilities"""
        return '''"""
Method comparison utilities
"""

from scipy import stats

def statistical_comparison(method_a_results, method_b_results):
    """Compare two methods statistically"""
    t_stat, p_value = stats.ttest_ind(method_a_results, method_b_results)
    return {'t_statistic': t_stat, 'p_value': p_value}
'''

    def _generate_requirements(self, design: ExperimentDesign) -> str:
        """Generate requirements.txt"""
        reqs = ["\\n".join(design.dependencies)]
        return reqs[0] if reqs else "numpy\\nscipy\\npandas"

    def _generate_readme(self, design: ExperimentDesign) -> str:
        """Generate README for experiment"""
        return f'''# Experiment: {design.design_id}

## Hypothesis

{design.hypothesis}

## Objective

{design.objective}

## Experiment Type

{design.experiment_type}

## Setup

```bash
pip install -r requirements.txt
```

## Running

```bash
python main.py
```

## Parameters

{self._format_parameters_table(design)}

## Metrics

**Primary:** {design.primary_metric}

**Secondary:** {", ".join(design.secondary_metrics)}

## Success Criteria

{design.success_criteria}

## Estimated Resources

- **Trials:** {design.num_trials}
- **Duration:** {design.estimated_duration}
- **Cost:** ${design.estimated_cost:.2f}
- **Compute:** {design.compute_requirements}

## Generated by

ORCHEX Code Generator - Autonomous Research System
'''

    def _format_parameters_table(self, design: ExperimentDesign) -> str:
        """Format parameters as markdown table"""
        lines = ["| Parameter | Type | Values |", "|-----------|------|--------|"]

        for param in design.parameters:
            values_str = str(param.values[:3]) + ("..." if len(param.values) > 3 else "")
            lines.append(f"| {param.name} | {param.type} | {values_str} |")

        return "\\n".join(lines)

    def _generate_test_script(self, design: ExperimentDesign) -> str:
        """Generate pytest test script"""
        return f'''"""
Tests for experiment {design.design_id}
"""

import pytest
import numpy as np

from main import Experiment


def test_experiment_initialization():
    """Test experiment can be initialized"""
    exp = Experiment(output_dir="/tmp/test")
    assert exp is not None


def test_parameter_setup():
    """Test parameter setup"""
    exp = Experiment(output_dir="/tmp/test")
    params = exp.setup_parameters()
    assert len(params) > 0


def test_single_trial():
    """Test single trial execution"""
    exp = Experiment(output_dir="/tmp/test")
    params = exp.setup_parameters()

    # Get first parameter combination
    first_combo = {{k: v[0] for k, v in params.items()}}

    result = exp.run_single_trial(0, first_combo)
    assert result is not None
    assert 'trial_id' in result
'''

    async def _refine_code_with_ai(
        self,
        code: str,
        design: ExperimentDesign
    ) -> str:
        """Refine generated code with AI assistance"""
        if not self.orchestrator:
            return code

        prompt = f"""
Review and improve this generated experiment code:

Hypothesis: {design.hypothesis}
Experiment Type: {design.experiment_type}

CODE:
{code[:2000]}...

Suggest improvements for:
1. Code quality and clarity
2. Error handling
3. Performance optimizations
4. Best practices
"""

        # In production, parse AI suggestions and apply them
        # For now, just mark as AI-reviewed
        return code

    def _write_code_to_disk(self, generated: GeneratedCode, output_dir: Path):
        """Write all generated files to disk"""
        output_dir.mkdir(parents=True, exist_ok=True)

        # Main script
        (output_dir / "main.py").write_text(generated.main_script)

        # Helper modules
        for filename, code in generated.helper_modules.items():
            (output_dir / filename).write_text(code)

        # Requirements
        (output_dir / "requirements.txt").write_text(generated.requirements_txt)

        # README
        (output_dir / "README.md").write_text(generated.readme)

        # Tests
        test_dir = output_dir / "tests"
        test_dir.mkdir(exist_ok=True)
        (test_dir / "test_experiment.py").write_text(generated.test_script)

        # __init__.py for tests
        (test_dir / "__init__.py").write_text("")

        print(f"âœ“ Generated code written to: {output_dir}")
        print(f"  - main.py")
        print(f"  - {len(generated.helper_modules)} helper modules")
        print(f"  - requirements.txt")
        print(f"  - README.md")
        print(f"  - tests/test_experiment.py")
