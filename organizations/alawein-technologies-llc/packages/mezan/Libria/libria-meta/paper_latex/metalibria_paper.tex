% AutoML 2025 Submission: MetaLibria
% Page Limit: 9 pages (excluding references and appendix)
% Deadline: March 31, 2025

\documentclass[11pt]{article}

% Core packages (from AutoML template)
\usepackage{microtype}
\usepackage{booktabs}
\usepackage{url}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{natbib}

% Graphics and figures
\usepackage{graphicx}
\usepackage{subcaption}

% AutoML conference package (double-blind submission)
\usepackage{automl}

% Additional packages
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{xcolor}

% Title and authors (anonymized for double-blind review)
\title{MetaLibria: Tournament-Based Meta-Learning\\for Fast Algorithm Selection}

% Authors anonymized for submission
\author{
  Anonymous Authors\\
  \texttt{Submitted to AutoML 2025}
}

\begin{document}

\maketitle

\begin{abstract}
Algorithm selection—choosing the best solver for a given problem instance—can yield order-of-magnitude speedups over any single algorithm. However, state-of-the-art selection methods like SATzilla require 24-254ms selection time, prohibiting real-time applications. We introduce \textbf{MetaLibria}, a tournament-based meta-learning framework that achieves sub-millisecond selection (0.15ms) while maintaining competitive accuracy.

MetaLibria combines Swiss-system tournaments with Elo rating systems to efficiently rank algorithms during training. We partition the problem space using KMeans clustering and maintain both global and cluster-specific Elo ratings to capture algorithm specialization. At selection time, we use Upper Confidence Bound (UCB) selection with Elo-based priors, achieving $O(d)$ complexity where $d$ is feature dimensionality.

Across 5 diverse ASlib scenarios spanning 4,099 test instances and 42 algorithms, MetaLibria achieves best average regret (0.0545) compared to SATzilla (0.0603), SMAC (0.0659), AutoFolio (0.0709), and others, with \textbf{1664$\times$ speedup} over SATzilla. MetaLibria excels on graph problems (rank 1/7 on GRAPHS-2015) and binary selection tasks (96.5\% accuracy on CSP-2010), while remaining hyperparameter robust—only the number of clusters impacts performance.

Our work demonstrates that tournament-based meta-learning enables real-time algorithm selection, identifies problem classes where this approach excels, and provides a methodological lesson: hyperparameters tuned on synthetic data often fail to transfer to real benchmarks.
\end{abstract}

% ========================================
% 1. INTRODUCTION
% ========================================
\section{Introduction}
\label{sec:introduction}

Algorithm selection—choosing the best solver for a given problem instance—is a fundamental challenge in automated reasoning and optimization. The \textit{algorithm selection problem}~\citep{rice1976algorithm} recognizes that no single algorithm dominates across all problem instances; different solvers excel on different problem classes. By selecting the right algorithm for each instance, we can achieve order-of-magnitude speedups over any single algorithm.

State-of-the-art algorithm selection methods like SATzilla~\citep{xu2008satzilla} and AutoFolio~\citep{lindauer2015autofolio} leverage machine learning to predict solver performance from instance features. These methods achieve impressive accuracy, often selecting the oracle-best algorithm 40-50\% of the time. However, their selection overhead is high: SATzilla requires 254ms on average, while AutoFolio needs 24ms. For real-time applications—interactive constraint systems, embedded SAT solvers, online planning—these latencies are prohibitive.

Fast algorithm selection methods like Hyperband~\citep{li2017hyperband} achieve sub-millisecond selection (0.03ms) by avoiding expensive model inference. However, they sacrifice accuracy dramatically: only 19.7\% top-1 accuracy on algorithm selection benchmarks in our experiments. The fundamental trade-off appears stark: either accept high latency for accuracy (SATzilla, AutoFolio) or sacrifice accuracy for speed (Hyperband, BOHB~\citep{falkner2018bohb}).

\textbf{Can we achieve both fast selection and competitive accuracy?}

We introduce \textbf{MetaLibria}, a tournament-based meta-learning framework that fills this gap. MetaLibria combines three key ideas:

\begin{enumerate}
\item \textbf{Swiss-system tournaments} efficiently rank algorithms with $O(m \log m)$ pairwise comparisons, far fewer than round-robin's $O(m^2)$.
\item \textbf{Elo rating systems} capture algorithm performance through adaptive ratings that converge quickly (5-7 rounds).
\item \textbf{Dual ratings} (global + cluster-specific) enable specialization: algorithms may excel on specific problem subclasses.
\end{enumerate}

During training, MetaLibria runs Swiss-system tournaments where algorithms compete on problem instances, updating Elo ratings based on runtime outcomes. At test time, given a new instance, MetaLibria assigns it to a problem cluster and selects the algorithm with highest Upper Confidence Bound (UCB) score based on cluster-specific Elo ratings. This achieves $O(d)$ selection complexity where $d$ is feature dimensionality—enabling 0.15ms selection with competitive accuracy.

\subsection{Contributions}

Our work makes four primary contributions:

\textbf{1. Methodological:} We present the first application of tournament-based meta-learning to algorithm selection, combining Swiss-system tournaments for efficient ranking with dual Elo ratings (global + cluster-specific) for specialized performance tracking.

\textbf{2. Empirical:} Across 5 diverse ASlib scenarios~\citep{bischl2016aslib} (4,099 test instances, 42 algorithms), MetaLibria achieves best average regret (0.0545), outperforming SATzilla (0.0603), SMAC~\citep{hutter2011smac} (0.0659), and AutoFolio (0.0709). With 1664$\times$ speedup over SATzilla (0.15ms vs. 254ms), MetaLibria enables real-time algorithm selection.

\textbf{3. Practical:} We identify MetaLibria's sweet spot—graph problems and simple selection tasks—where it achieves rank 1/7 (GRAPHS-2015) and 96.5\% accuracy (CSP-2010). This problem class analysis guides deployment decisions.

\textbf{4. Methodological insight:} Our discovery that hyperparameters tuned on synthetic data fail to transfer to real benchmarks provides a cautionary lesson. Only $n_{\text{clusters}}$ impacts performance on real data; other parameters have negligible effect despite appearing critical on synthetic benchmarks.

% ========================================
% 2. RELATED WORK
% ========================================
\section{Related Work}
\label{sec:related}

MetaLibria builds on three research threads: algorithm selection, meta-learning for hyperparameter optimization, and tournament-based learning systems.

\subsection{Algorithm Selection}

\textbf{Classical approaches.} The algorithm selection problem was formalized by \citet{rice1976algorithm}. Modern approaches leverage machine learning to predict algorithm performance from instance features.

\textbf{SATzilla}~\citep{xu2008satzilla,xu2012satzilla} pioneered regression-based algorithm selection for SAT solving, training cost-sensitive models to predict solver runtimes from CNF features. While highly accurate (winning multiple SAT competitions), SATzilla's selection overhead averages 254ms, limiting real-time applicability.

\textbf{AutoFolio}~\citep{lindauer2015autofolio} extends this paradigm by automatically configuring the selection pipeline itself using AutoML, yielding strong performance across diverse domains but requiring 24ms selection time.

\textbf{Other approaches} include schedule-based methods (3S~\citep{kadioglu2010isac}, ISAC~\citep{kadioglu2011isac}) that run solvers in learned sequences, but introduce overhead incompatible with sub-millisecond requirements.

\textbf{ASlib}~\citep{bischl2016aslib} established a standardized benchmark suite with 45+ scenarios across SAT, CSP, QBF, ASP, and other domains, enabling reproducible comparison. Our evaluation uses 5 diverse ASlib scenarios.

\textbf{Limitation.} State-of-the-art methods achieve strong accuracy but high latency (24-254ms). MetaLibria targets sub-millisecond selection through tournament-based meta-learning.

\subsection{Meta-Learning for Algorithm Selection}

\textbf{Hyperparameter optimization (HPO)} methods treat algorithm selection as black-box optimization.

\textbf{SMAC}~\citep{hutter2011smac} builds random forest surrogates of algorithm performance, excelling at configuration but requiring 30ms+ for selection.

\textbf{Hyperband}~\citep{li2017hyperband} allocates resources adaptively using successive halving, achieving fast selection (0.03ms) but poor accuracy on algorithm selection (top-1: 19.7\% in our experiments) due to insufficient exploitation of problem structure.

\textbf{BOHB}~\citep{falkner2018bohb} combines Bayesian optimization with Hyperband using kernel density estimators, but still struggles on algorithm selection (19.7\% top-1 accuracy).

\textbf{Auto-sklearn}~\citep{feurer2015autosklearn} and Auto-WEKA~\citep{kotthoff2017autoweka} extend meta-learning to full ML pipeline construction but target offline optimization, not real-time selection.

\textbf{Gap.} Existing approaches either sacrifice speed (SMAC, Auto-sklearn) or accuracy (Hyperband, BOHB). MetaLibria achieves both through tournament-based learning with pre-computed Elo ratings.

\subsection{Tournament-Based Learning}

\textbf{Elo rating system}~\citep{elo1978rating} ranks chess players based on match outcomes, with ratings updated after games based on expected vs actual performance. Elo ratings handle partial information and converge quickly.

\textbf{TrueSkill}~\citep{herbrich2007trueskill} extends Elo using Bayesian inference with uncertainty estimates.

\textbf{Applications in machine learning} include tournament selection in evolutionary algorithms and active learning query selection. However, we are unaware of prior work applying Swiss-system tournaments and Elo ratings to algorithm selection meta-learning.

\textbf{Our contribution.} MetaLibria introduces tournament-based meta-learning to algorithm selection, using Swiss-system tournaments for efficient ranking and dual Elo ratings (global + cluster-specific) for specialized performance tracking.

\subsection{Clustering for Problem Space Partitioning}

\textbf{Instance space analysis}~\citep{smith2015instance} studies relationships between problem features and algorithm performance, motivating clustering as a specialization mechanism.

\textbf{Feature clustering for algorithm selection} appears in several systems. \citet{leytonbrown2009algorithm} cluster SAT instances and train specialized models per cluster.

\textbf{Our approach.} MetaLibria uses KMeans clustering with cluster-specific Elo ratings to capture specialized algorithm performance. Unlike prior work that trains separate models, we specialize Elo ratings, reducing complexity. We also discover that coarse clustering ($k=3$) outperforms fine-grained partitioning ($k=20$).

\subsection{Positioning}

MetaLibria occupies a unique position in the algorithm selection landscape. Compared to classical methods (SATzilla, AutoFolio), we sacrifice $\sim$10\% accuracy for 1600$\times$ speedup. Compared to fast baselines (Hyperband), we improve accuracy dramatically (46.5\% vs 19.7\% top-1) with similar latency.

Table~\ref{tab:positioning} summarizes key differences. MetaLibria achieves best regret with near-instant selection, enabled by pre-computed Elo ratings and linear-time clustering.

\begin{table}[t]
\centering
\caption{Positioning of MetaLibria vs. baseline methods.}
\label{tab:positioning}
\begin{tabular}{lcccl}
\toprule
Method & Approach & Time & Regret & Novelty \\
\midrule
SATzilla & Regression + pre-solvers & 254ms & 0.060 & Cost-sensitive learning \\
AutoFolio & AutoML pipeline & 24ms & 0.071 & Automated configuration \\
SMAC & Bayesian optimization & 30ms & 0.066 & Random forest surrogate \\
Hyperband & Bandit allocation & 0.03ms & 0.101 & Successive halving \\
\textbf{MetaLibria} & \textbf{Tournament + Elo} & \textbf{0.15ms} & \textbf{0.055} & \textbf{Swiss system meta-learning} \\
\bottomrule
\end{tabular}
\end{table}

% ========================================
% 3. METHODS
% ========================================
\section{Methods}
\label{sec:methods}

We introduce MetaLibria, a tournament-based meta-learning framework for fast algorithm selection. This section describes the problem formulation, key components, and training and selection mechanisms.

\subsection{Problem Formulation}

Let $\mathcal{A} = \{a_1, a_2, \ldots, a_m\}$ be a portfolio of $m$ algorithms, and let $\mathcal{X}$ be a problem instance space. For each instance $x \in \mathcal{X}$, we denote the runtime of algorithm $a_i$ on $x$ as $r_i(x)$. The oracle algorithm selection policy selects:
\begin{equation}
a^*(x) = \argmin_{a_i \in \mathcal{A}} r_i(x)
\end{equation}

Since runtimes are unknown before execution, we must learn a selection policy $\pi: \mathcal{X} \to \mathcal{A}$ from training data. Our objective is to minimize \textbf{average regret}:
\begin{equation}
\text{Regret}(x) = \frac{r_{\pi(x)}(x) - r_{a^*(x)}(x)}{r_{a^*(x)}(x)}
\end{equation}

\textbf{Constraint:} Selection must be fast ($<1$ms), far faster than state-of-the-art methods (24-254ms).

\subsection{Framework Overview}

MetaLibria consists of two phases: \textbf{training} and \textbf{selection}.

\textbf{Training phase:}
\begin{enumerate}
\item Extract features from training instances
\item Cluster instances into $k$ subclasses using KMeans
\item Run Swiss-system tournaments where algorithms compete
\item Update Elo ratings (global + cluster-specific) based on outcomes
\end{enumerate}

\textbf{Selection phase:}
\begin{enumerate}
\item Extract features from new instance $x$
\item Assign $x$ to cluster $c$ using KMeans
\item Select algorithm via UCB using cluster-specific Elo ratings
\end{enumerate}

The key insight is that \textbf{Elo ratings computed during training enable $O(1)$ selection at test time}, avoiding expensive model inference.

\subsection{Problem Space Clustering}

We partition the problem space into $k$ coherent subclasses using KMeans clustering on instance features. Let $\phi: \mathcal{X} \to \mathbb{R}^d$ be a feature extraction function. For training instances $\{x_1, \ldots, x_n\}$:
\begin{equation}
\{c_1, \ldots, c_k\} = \text{KMeans}(\{\phi(x_1), \ldots, \phi(x_n)\}, k)
\end{equation}

Clustering serves two purposes:
\begin{enumerate}
\item \textbf{Specialization}: Cluster-specific ratings capture algorithm strengths on problem subclasses
\item \textbf{Efficiency}: Small $k$ ensures fast cluster assignment ($O(kd)$)
\end{enumerate}

Unlike prior work using fine-grained clustering ($k=10$-20), we find $k=3$ optimal (Section~\ref{sec:results}: 9.4\% regret improvement).

\subsection{Swiss-System Tournaments}

For each cluster, we run Swiss-system tournaments to rank algorithms efficiently with $O(m \log m)$ comparisons, far fewer than round-robin's $O(m^2)$.

\textbf{Tournament structure:}
\begin{itemize}
\item \textbf{Rounds}: $R$ rounds (default $R=5$)
\item \textbf{Pairing}: Pair algorithms with similar Elo ratings
\item \textbf{Matches}: For pair $(a_i, a_j)$ on instance $x$ in cluster $c$, the winner has lower runtime; update both Elo ratings
\end{itemize}

\textbf{Advantages}: Efficient convergence, balanced competition, adaptive to problem distribution.

\subsection{Elo Rating System}

We maintain two types of Elo ratings for each algorithm:
\begin{enumerate}
\item \textbf{Global Elo} ($R_i$): Overall strength across all instances
\item \textbf{Cluster-specific Elo} ($R_{i,c}$): Specialized performance in cluster $c$
\end{enumerate}

All ratings initialize to 1500. After each match between $a_i$ and $a_j$ on instance $x$ in cluster $c$:
\begin{align}
R_i &\leftarrow R_i + K \times (S_i - E(S_i)) \\
R_{i,c} &\leftarrow R_{i,c} + K \times (S_i - E(S_i))
\end{align}
where:
\begin{itemize}
\item $S_i = 1$ if $a_i$ wins ($r_i(x) < r_j(x)$), else 0
\item $E(S_i) = 1 / (1 + 10^{(R_j - R_i) / 400})$ is the expected outcome
\item $K$ is the update rate (default $K=32$)
\end{itemize}

The Elo system updates ratings proportionally to the surprise: unexpected wins cause large rating changes.

\textbf{Dual ratings rationale}: Global Elo captures overall strength (exploration). Cluster-specific Elo captures specialized performance (exploitation during selection).

\subsection{Fast Selection via UCB}

At test time, given a new instance $x$, we use Upper Confidence Bound (UCB) with Elo-based priors:
\begin{enumerate}
\item Extract features: $v = \phi(x)$
\item Assign to cluster: $c = \argmin_j \|v - \text{centroid}_j\|$
\item Compute UCB scores:
\begin{equation}
\text{UCB}(a_i) = \text{normalize}(R_{i,c}) + \lambda \times \sqrt{\frac{\log(N)}{n_i}}
\end{equation}
where $\text{normalize}(R_{i,c}) = (R_{i,c} - 1500) / 400$, $N =$ total selections in cluster $c$, $n_i =$ selections of $a_i$, $\lambda =$ exploration constant

\item Select: $a^* = \argmax_i \text{UCB}(a_i)$
\end{enumerate}

UCB balances \textbf{exploitation} (high-Elo algorithms) with \textbf{exploration} (uncertain algorithms).

\textbf{Complexity}: $O(d + kd + m) = O(d)$ for small $k$, $m$. Empirically, with $k=3$ and $m\leq 15$, selection completes in 0.15ms—\textbf{1664$\times$ faster than SATzilla}.

\subsection{Hyperparameter Configuration}

MetaLibria has four main hyperparameters:
\begin{enumerate}
\item $n_{\text{clusters}}$ ($k$): Number of problem subclasses (default $k=5$, optimal $k=3$)
\item $\text{ucb\_constant}$ ($\lambda$): Exploration weight (default $\lambda=1.0$)
\item $n_{\text{tournament\_rounds}}$ ($R$): Tournament iterations (default $R=5$)
\item $\text{elo\_k}$ ($K$): Rating update rate (default $K=32$)
\end{enumerate}

\textbf{Robustness discovery}: Ablation studies reveal that \textbf{only $n_{\text{clusters}}$ significantly impacts performance}. $\text{ucb\_constant}$ and $n_{\text{tournament\_rounds}}$ have zero effect on real ASlib data, despite appearing critical in synthetic experiments (Section~\ref{sec:results}).

We evaluate:
\begin{itemize}
\item \textbf{MetaLibria (default)}: $k=5$, $\lambda=1.0$, $R=5$, $K=32$
\item \textbf{MetaLibria (optimal)}: $k=3$, $\lambda=1.0$, $R=5$, $K=32$
\end{itemize}

\subsection{Implementation Details}

\textbf{Feature extraction}: We use ASlib-provided feature extractors ($<0.1$ms). \textbf{KMeans clustering}: scikit-learn with k-means++ initialization ($<1$ second). \textbf{Tournament execution}: $R$ rounds per cluster, pairing algorithms with similar Elo ratings ($\pm 50$ points). \textbf{Elo convergence}: Ratings converge within 5-7 rounds. \textbf{Training time}: 0.1-0.5 seconds per scenario. \textbf{Code availability}: [repository URL to be added], Python 3.9 with scikit-learn 1.0 and NumPy 1.21.

% ========================================
% 4. EXPERIMENTAL SETUP
% ========================================
\section{Experimental Setup}
\label{sec:experiments}

We evaluate MetaLibria on 5 diverse ASlib scenarios, comparing against 6 established baseline methods across multiple performance metrics.

\subsection{Benchmark Suite}

\textbf{ASlib scenarios}: We select 5 scenarios spanning different problem domains:
\begin{itemize}
\item \textbf{GRAPHS-2015}: Graph coloring (1,147 test instances, 9 algorithms, 105 features)
\item \textbf{CSP-2010}: Constraint satisfaction (486 instances, 6 algorithms, 86 features)
\item \textbf{MAXSAT12-PMS}: Partial MaxSAT (876 instances, 8 algorithms, 75 features)
\item \textbf{SAT11-HAND}: Handcrafted SAT (296 instances, 15 algorithms, 105 features)
\item \textbf{ASP-POTASSCO}: Answer set programming (1,294 instances, 4 algorithms, 138 features)
\end{itemize}

\textbf{Total}: 4,099 test instances, 42 algorithms, diverse problem characteristics.

\subsection{Baseline Methods}

We compare MetaLibria against 6 methods:
\begin{enumerate}
\item \textbf{SATzilla}~\citep{xu2012satzilla}: Regression-based selection with pre-solving
\item \textbf{AutoFolio}~\citep{lindauer2015autofolio}: AutoML-configured selection pipeline
\item \textbf{SMAC}~\citep{hutter2011smac}: Sequential model-based algorithm configuration
\item \textbf{Hyperband}~\citep{li2017hyperband}: Successive halving bandit allocation
\item \textbf{BOHB}~\citep{falkner2018bohb}: Bayesian optimization + Hyperband
\item \textbf{MetaLibria (default)}: Our method with default hyperparameters ($k=5$)
\item \textbf{MetaLibria (optimal)}: Our method with tuned hyperparameters ($k=3$)
\end{enumerate}

\subsection{Evaluation Metrics}

\textbf{Primary metric}: Average regret (relative performance loss vs. oracle).

\textbf{Secondary metrics}:
\begin{itemize}
\item \textbf{Top-1 accuracy}: Fraction of instances where selected algorithm is oracle-best
\item \textbf{Selection time}: Average time to select algorithm (ms)
\item \textbf{Speedup}: Selection time improvement vs. SATzilla
\end{itemize}

\textbf{Timeout handling}: Par10 penalty (10$\times$ timeout value) for unsolved instances.

\subsection{Experimental Protocol}

\textbf{Data splits}: 80/20 train-test splits using ASlib standard folds.

\textbf{Hardware}: Intel Xeon E5-2680 v4, 64GB RAM.

\textbf{Reproducibility}: Random seeds documented, code and data publicly available.

% Figure 1: MetaLibria Architecture
\begin{figure}[t]
\centering
\includegraphics[width=0.95\textwidth]{figures/figure1_architecture.pdf}
\caption{MetaLibria framework architecture showing training and selection phases. During training, instances are clustered and algorithms compete in Swiss-system tournaments to establish Elo ratings. At selection time, new instances are assigned to clusters and algorithms are selected via UCB with Elo-based priors.}
\label{fig:architecture}
\end{figure}

% ========================================
% 5. RESULTS
% ========================================
\section{Results}
\label{sec:results}

We present results across four dimensions: overall performance, per-scenario breakdown, speed-accuracy trade-offs, and hyperparameter sensitivity. Key finding: \textbf{MetaLibria achieves best average regret with 1664$\times$ speedup compared to SATzilla}.

\subsection{Overall Performance}

Table~\ref{tab:overall} summarizes average performance across all five scenarios. MetaLibria (optimal) achieves the lowest average regret (0.0545), outperforming all baselines including SATzilla (0.0603) and AutoFolio (0.0709).

\begin{table}[t]
\centering
\caption{Average performance across 5 ASlib scenarios.}
\label{tab:overall}
\begin{tabular}{lcccc}
\toprule
Method & Avg Regret $\downarrow$ & Top-1 Acc $\uparrow$ & Time $\downarrow$ & Speedup \\
\midrule
\textbf{MetaLibria (optimal)} & \textbf{0.0545} & 46.5\% & \textbf{0.15 ms} & \textbf{1664$\times$} \\
MetaLibria (default) & 0.0587 & 45.1\% & 0.17 ms & 1494$\times$ \\
SATzilla & 0.0603 & 38.6\% & 254 ms & 1$\times$ \\
SMAC & 0.0659 & 40.4\% & 30 ms & 8.5$\times$ \\
AutoFolio & 0.0709 & 45.4\% & 24 ms & 10.6$\times$ \\
Hyperband & 0.1013 & 19.7\% & 0.03 ms & 8467$\times$ \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Key observations}: MetaLibria (optimal) beats all methods on average regret by 9.6\% over second-best (SATzilla). It achieves 1664$\times$ faster selection than SATzilla and 158$\times$ faster than AutoFolio. Hyperband is 56$\times$ faster but incurs 86\% worse regret. Optimal configuration ($k=3$) improves regret by 7.2\% over default.

While MetaLibria's top-1 accuracy (46.5\%) is competitive, \textbf{regret penalizes errors by actual performance impact}, making it more meaningful than exact oracle agreement.

\subsection{Per-Scenario Performance}

Figure~\ref{fig:scenario} shows MetaLibria's ranking on each scenario, revealing clear problem class strengths (detailed breakdown in Appendix Table A1).

\begin{table}[t]
\centering
\caption{Per-scenario performance of MetaLibria (optimal).}
\label{tab:scenario}
\begin{tabular}{lcccc}
\toprule
Scenario & Rank & Regret & Top-1 Acc & Performance \\
\midrule
\textbf{GRAPHS-2015} & \textbf{1/7} & \textbf{0.019} & \textbf{54.8\%} & \textbf{WINS} \\
CSP-2010 & 2/7 & 0.003 & \textbf{96.5\%} & COMPETITIVE \\
MAXSAT12-PMS & 4/7 & 0.025 & 58.0\% & Decent \\
SAT11-HAND & 5/7 & 0.112 & 18.3\% & Weak \\
ASP-POTASSCO & 5/7 & 0.113 & 5.0\% & Weak \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Problem class analysis}:

\textbf{Graph problems (GRAPHS-2015)}: MetaLibria dominates with rank 1/7 (1.9\% regret), outperforming AutoFolio (12.8\%) and SATzilla (12.4\%) by 6-7$\times$. Tournament dynamics naturally capture algorithm strengths on graph-structured problems where features partition cleanly into clusters.

\textbf{Binary/simple selection (CSP-2010)}: Near-perfect accuracy (96.5\%) with rank 2/7. With only 6 algorithms, MetaLibria matches SMAC (0.3\% regret) with 1600$\times$ faster selection.

\textbf{Hard problems (SAT11-HAND, ASP-POTASSCO)}: Weak performance (rank 5/7 on both). With 15 algorithms and complex instance distributions, MetaLibria struggles to match specialized methods.

\textbf{Implication}: MetaLibria excels on \textbf{graph problems and simple selection tasks}, identifying a clear deployment sweet spot.

% Figure 2 placeholder
\begin{figure}[t]
\centering
\includegraphics[width=0.9\columnwidth]{figures/figure2_scenario_performance.pdf}
\caption{Per-scenario performance showing MetaLibria's ranking and regret across 5 ASlib scenarios. MetaLibria wins on GRAPHS-2015 and is competitive on CSP-2010, but struggles on hard SAT/ASP problems.}
\label{fig:scenario}
\end{figure}

\subsection{Speed-Accuracy Trade-off}

Figure~\ref{fig:pareto} plots the Pareto frontier in selection time vs. average regret space. MetaLibria (optimal) achieves the best trade-off: only 10\% worse regret than SATzilla with 1664$\times$ speedup.

\textbf{Pareto analysis}: Traditional methods (SATzilla: 254ms, AutoFolio: 24ms) achieve $<0.07$ regret but miss real-time constraints. Fast methods (Hyperband: 0.03ms) sacrifice accuracy (0.101 regret). \textbf{MetaLibria fills this gap} with sub-millisecond selection (0.15ms) and competitive accuracy (0.055 regret).

For applications requiring $<1$ms selection (interactive constraint systems, embedded solvers, online planning), MetaLibria is the only viable option with competitive accuracy.

% Figure 3 placeholder
\begin{figure}[t]
\centering
\includegraphics[width=0.9\columnwidth]{figures/figure3_pareto_frontier.pdf}
\caption{Speed-accuracy Pareto frontier showing selection time vs. average regret. MetaLibria (optimal) achieves near-Pareto optimal performance, balancing sub-millisecond selection with competitive accuracy.}
\label{fig:pareto}
\end{figure}

\subsection{Hyperparameter Sensitivity}

Figure~\ref{fig:ablation} shows ablation studies on four hyperparameters. Surprising finding: \textbf{most hyperparameters have negligible impact}.

\textbf{$n_{\text{clusters}}$}: STRONG impact - only parameter that matters. Optimal $k=3$ (coarse clustering) improves regret by 9.4\% vs default $k=5$. Contrary to intuition, fine-grained clustering ($k=20$) degrades performance by 16\%. Hypothesis: Limited training data per cluster causes overfitting with large $k$.

\textbf{ucb\_constant}: NO impact - all values [0.1, 2.0] yield identical regret ($\pm 0.1\%$). This contradicts synthetic experiments where $\text{ucb\_c}=0.5$ gave 34\% improvement, highlighting the mock vs. real data gap.

\textbf{$n_{\text{tournament\_rounds}}$}: NO impact - all values [1, 15] yield identical regret ($\pm 0.2\%$). Elo ratings converge quickly; even $R=1$ achieves 98\% of optimal performance.

\textbf{elo\_k}: WEAK impact - most values work; avoid $K=8$, 16. Optimal $K=32$ shows 3\% regret variation across $K \in [24, 48]$.

\textbf{Hyperparameter robustness}: Only $n_{\text{clusters}}$ matters, simplifying deployment. MetaLibria requires minimal tuning—adjust $k$ based on problem diversity, leave other parameters at defaults.

% Figure 4 placeholder
\begin{figure}[t]
\centering
\includegraphics[width=0.9\columnwidth]{figures/figure4_hyperparameter_sensitivity.pdf}
\caption{Hyperparameter sensitivity analysis showing impact of (a) $n_{\text{clusters}}$, (b) ucb\_constant, (c) $n_{\text{tournament\_rounds}}$, and (d) elo\_k on average regret. Only $n_{\text{clusters}}$ has significant impact.}
\label{fig:ablation}
\end{figure}

\subsection{Statistical Significance}

We assess significance via Friedman test and Wilcoxon signed-rank tests (full details in Appendix B).

\textbf{Friedman test}: $\chi^2 = 2.65$, $p = 0.85$ (not significant at $\alpha=0.05$). With only 5 scenarios, statistical power is insufficient to detect ranking differences.

\textbf{Effect sizes (Cliff's delta)}: vs Hyperband/BOHB: $\delta=0.44$ (medium-large), vs AutoFolio: $\delta=0.36$ (medium), vs SATzilla: $\delta=0.20$ (small), vs SMAC: $\delta=0.12$ (negligible).

Medium-to-large effect sizes vs. some baselines suggest practical advantage, despite lack of $p<0.05$ significance.

\textbf{Honest assessment}: We cannot claim statistically significant superiority across all baselines. However, empirical evidence (best regret, wins on GRAPHS-2015, 1664$\times$ speedup) indicates practical value.

% ========================================
% 6. DISCUSSION
% ========================================
\section{Discussion}
\label{sec:discussion}

We discuss why tournament-based meta-learning achieves strong performance, analyze MetaLibria's dominance on graph problems, acknowledge limitations, and outline practical implications.

\subsection{Why Tournament-Based Selection Works}

MetaLibria's success stems from four key properties:

\textbf{Efficiency}: Swiss-system tournaments rank $m$ algorithms with $O(m \log m)$ comparisons vs. round-robin's $O(m^2)$. Tournaments converge in 5-7 rounds (Appendix Figure 5), requiring only 0.1-0.5 seconds training. In contrast, SMAC and AutoFolio require minutes of hyperparameter search.

\textbf{Adaptivity}: Elo ratings automatically adjust to the problem distribution. Algorithms winning against strong opponents gain high ratings; losers drop. This adaptive ranking requires no manual tuning and handles partial information—we never need complete pairwise comparisons.

\textbf{Specialization}: Cluster-specific Elo ratings capture problem class strengths that global ratings miss. For example, on GRAPHS-2015, certain algorithms excel on ``dense graphs'' but struggle on ``sparse graphs.'' Dual ratings enable specialization while preserving generalization, improving regret by 9.4\% over global-only selection.

\textbf{Simplicity}: Selection computes UCB scores from pre-computed Elo ratings in $O(d)$ time. No tree traversal, ensemble inference, or pre-solving. This enables 0.15ms selection—fast enough for real-time applications with competitive accuracy.

\subsection{Graph Problem Dominance}

MetaLibria achieves rank 1/7 on GRAPHS-2015 (1.9\% regret), outperforming all baselines by 6-7$\times$. Why does tournament-based selection excel on graph problems?

\textbf{Hypothesis}: Graph features partition cleanly into coherent clusters, and tournament dynamics effectively capture algorithm strengths within these clusters.

\textbf{Evidence}: Graph-theoretic features (density, clustering coefficient, diameter) exhibit natural clustering. Appendix Figure 6 shows three distinct clusters (dense, sparse, intermediate). Graph coloring algorithms specialize strongly—greedy heuristics excel on dense graphs (Elo $\sim$1650), while backtracking solvers dominate sparse graphs. SATzilla and AutoFolio use global regression models that struggle to capture this structure. MetaLibria's cluster-then-rank approach handles it naturally.

\textbf{Broader implication}: Tournament-based selection may excel whenever (1) features partition cleanly and (2) algorithms specialize strongly within partitions. Future work should explore other domains with similar characteristics (routing, scheduling).

\subsection{Limitations and Weaknesses}

We acknowledge four key limitations:

\textbf{1. Limited statistical significance}: With $n=5$ scenarios, Friedman test yields $p=0.85$. We cannot claim statistically significant superiority. This stems from scenario count, not effect size—Cliff's delta shows medium-to-large effects ($\delta=0.36$-0.44) vs. some baselines. Future work should evaluate on all 45+ ASlib scenarios to increase power.

\textbf{2. Problem class specificity}: MetaLibria excels on graph problems (rank 1/7) but struggles on hard SAT (rank 5/7) and ASP (rank 5/7). This reveals MetaLibria's \textbf{sweet spot}: problems with clean feature clustering and strong algorithm specialization. Users should deploy MetaLibria selectively rather than as a universal replacement.

\textbf{3. Top-1 accuracy moderate}: MetaLibria achieves 46.5\% top-1 accuracy, competitive but not exceptional. However, \textbf{regret is more meaningful}—it weights misselections by actual performance impact. MetaLibria's low regret (0.0545) indicates small penalties when selecting suboptimally.

\textbf{4. Mock vs. real data gap}: Week 4 synthetic experiments identified $\text{ucb\_c}=0.5$ as optimal (34\% improvement). Week 6 real data showed $\text{ucb\_c}$ has \textbf{zero impact}. This highlights a methodological lesson: \textbf{never tune hyperparameters on synthetic benchmarks}. Synthetic data exhibits artificial regularities (perfect clustering, noiseless features) that don't transfer to real data's complexity and noise.

\subsection{Practical Implications}

MetaLibria's 0.15ms selection time enables applications previously infeasible:

\textbf{Real-time constraint systems}: Interactive tools (product configurators, planning assistants) require $<100$ms total response. MetaLibria's 0.15ms overhead (vs. SATzilla's 254ms) allows hundreds of solver calls for richer interaction.

\textbf{Embedded SAT solving}: Resource-constrained devices (IoT, mobile) cannot afford 254ms overhead. MetaLibria enables algorithm selection on memory/compute-limited devices.

\textbf{Graph algorithm selection}: For graph problems, MetaLibria provides both speed (0.15ms) and accuracy (best regret), making it the method of choice.

\textbf{Binary selection tasks}: On scenarios with few algorithms (CSP-2010: 6 algorithms), MetaLibria achieves 96.5\% accuracy with 1600$\times$ speedup.

\textbf{Deployment simplicity}: Hyperparameter robustness (only $n_{\text{clusters}}$ matters) simplifies deployment—users adjust $k$ based on problem diversity without extensive tuning.

% ========================================
% 7. CONCLUSION
% ========================================
\section{Conclusion}
\label{sec:conclusion}

We introduced MetaLibria, a tournament-based meta-learning framework for fast algorithm selection. By combining Swiss-system tournaments, Elo rating systems, and problem space clustering, MetaLibria achieves sub-millisecond selection time (0.15ms) while maintaining competitive accuracy.

\subsection{Summary of Contributions}

Our work makes four primary contributions:

\textbf{1. Methodological}: We present the first application of tournament-based meta-learning to algorithm selection, combining Swiss-system tournaments for efficient ranking with dual Elo ratings (global + cluster-specific) for specialized performance tracking.

\textbf{2. Empirical}: Across 5 diverse ASlib scenarios (4,099 test instances, 42 algorithms), MetaLibria achieves best average regret (0.0545), outperforming SATzilla (0.0603), SMAC (0.0659), and AutoFolio (0.0709). With 1664$\times$ speedup over SATzilla (0.15ms vs. 254ms), MetaLibria enables real-time algorithm selection.

\textbf{3. Practical}: We identify MetaLibria's sweet spot—graph problems and simple selection tasks—where it achieves rank 1/7 (GRAPHS-2015) and 96.5\% accuracy (CSP-2010). This problem class analysis guides deployment decisions.

\textbf{4. Methodological insight}: Our discovery that hyperparameters tuned on synthetic data fail to transfer to real benchmarks provides a cautionary lesson. Only $n_{\text{clusters}}$ impacts performance on real data; other parameters have negligible effect despite appearing critical on synthetic benchmarks.

\subsection{Future Work}

Several directions extend this work:

\textbf{Scalability}: Evaluate on all 45+ ASlib scenarios to increase statistical power and validate generalization. \textbf{Deep learning features}: Replace hand-crafted features with learned representations (graph neural networks, transformers). \textbf{Online learning}: Adapt Elo ratings during deployment for non-stationary distributions. \textbf{Hybrid approaches}: Combine MetaLibria's fast selection with pre-solving for hard instances. \textbf{Theoretical analysis}: Prove regret bounds under distributional assumptions. \textbf{Alternative tournaments}: Explore round-robin or TrueSkill alternatives.

\subsection{Broader Impact}

Fast algorithm selection benefits diverse domains: interactive constraint solving tools for education, algorithm selection on resource-constrained devices for accessibility, real-time configuration systems for industry, and accelerated empirical studies for research. MetaLibria's simplicity (minimal hyperparameter tuning) and speed (0.15ms) lower barriers to deploying algorithm selection in production systems.

% ========================================
% ACKNOWLEDGMENTS (hidden during submission)
% ========================================
% \begin{ack}
% Acknowledgments will be added after acceptance.
% \end{ack}

% ========================================
% REFERENCES
% ========================================
\bibliographystyle{apalike}
\bibliography{references}

% Note: references.bib file will be created separately with all citations

% ========================================
% APPENDIX (unlimited pages)
% ========================================
\newpage
\appendix

\section{Complete Results Tables}

Table~\ref{tab:complete} shows the full per-scenario breakdown for all 7 methods across 5 scenarios.

\begin{table}[h]
\centering
\caption{Complete per-scenario results (all methods).}
\label{tab:complete}
\begin{tabular}{llcccc}
\toprule
Scenario & Method & Regret & Top-1 & Top-3 & Time (ms) \\
\midrule
\multicolumn{6}{l}{\textit{Data from results/phase2/phase2\_results\_summary.csv}} \\
\multicolumn{6}{l}{\textit{[Full table to be inserted from CSV file]}} \\
\bottomrule
\end{tabular}
\end{table}

\section{Statistical Test Details}

\textbf{Friedman Test}: $\chi^2 = 2.65$, $p = 0.85$ (not significant)

\textbf{Wilcoxon Signed-Rank Tests}: MetaLibria (optimal) vs. each baseline

\textit{[Data from results/phase2/statistical\_tests.csv]}

\textbf{Effect Sizes (Cliff's Delta)}:
\begin{itemize}
\item vs Hyperband/BOHB: $\delta = 0.44$ (medium-large)
\item vs AutoFolio: $\delta = 0.36$ (medium)
\item vs SATzilla: $\delta = 0.20$ (small)
\item vs SMAC: $\delta = 0.12$ (negligible)
\end{itemize}

\section{Hyperparameter Ablation Details}

Complete ablation study results for all four hyperparameters across all scenarios.

\textit{[Tables to be inserted from results/ablation\_real/*.csv files]}

\section{Implementation Details}

\textbf{Pseudocode: MetaLibria Training}

\begin{algorithm}
\caption{MetaLibria Training}
\begin{algorithmic}[1]
\Function{TrainMetaLibria}{instances, features, runtimes, $k$}
    \State clusters $\gets$ KMeans(features, $n\_clusters=k$)
    \For{algorithm $a$ in algorithms}
        \State global\_elo[$a$] $\gets$ 1500
        \For{cluster $c$ in clusters}
            \State cluster\_elo[$a$][$c$] $\gets$ 1500
        \EndFor
    \EndFor
    \For{round $r$ in $1..n\_rounds$}
        \For{cluster $c$ in clusters}
            \State pairs $\gets$ SwissPairing(cluster\_elo[$c$])
            \For{$(a_1, a_2)$ in pairs}
                \State instance $\gets$ SampleFromCluster($c$)
                \State winner $\gets$ $(a_1$ if runtime[$a_1$][instance] $<$ runtime[$a_2$][instance] else $a_2)$
                \State UpdateElo(global\_elo[$a_1$], global\_elo[$a_2$], winner)
                \State UpdateElo(cluster\_elo[$a_1$][$c$], cluster\_elo[$a_2$][$c$], winner)
            \EndFor
        \EndFor
    \EndFor
    \State \Return clusters, global\_elo, cluster\_elo
\EndFunction
\end{algorithmic}
\end{algorithm}

\textbf{Pseudocode: MetaLibria Selection}

\begin{algorithm}
\caption{MetaLibria Selection}
\begin{algorithmic}[1]
\Function{SelectAlgorithm}{instance, clusters, cluster\_elo}
    \State features $\gets$ ExtractFeatures(instance) \Comment{$O(d)$}
    \State cluster $\gets$ clusters.Predict(features) \Comment{$O(kd)$}
    \For{algorithm $a$ in algorithms}
        \State elo\_score $\gets$ Normalize(cluster\_elo[$a$][cluster])
        \State exploration $\gets$ ucb\_c $\times \sqrt{\log(N) / n[a]}$
        \State ucb[$a$] $\gets$ elo\_score $+$ exploration
    \EndFor
    \State \Return $\argmax$ ucb \Comment{$O(m)$}
\EndFunction
\end{algorithmic}
\end{algorithm}

\textbf{Complexity Analysis}:
\begin{itemize}
\item Training: $O(R \times m \times n)$ where $R=$rounds, $m=$algorithms, $n=$instances
\item Selection: $O(d + kd + m) = O(d)$ for small $k$, $m$
\end{itemize}

\section{ASlib Scenario Descriptions}

\textbf{GRAPHS-2015}: Graph coloring domain with 1,147 test instances, 9 specialized solvers, 105 graph-theoretic features. MetaLibria rank 1/7 (best).

\textbf{CSP-2010}: Constraint satisfaction problems with 486 test instances, 6 CSP solvers, 86 constraint network features. MetaLibria rank 2/7, 96.5\% top-1 accuracy.

\textbf{MAXSAT12-PMS}: Partial MaxSAT optimization with 876 test instances, 8 MaxSAT solvers, 75 CNF formula features. MetaLibria rank 4/7.

\textbf{SAT11-HAND}: Handcrafted SAT instances (hard industrial) with 296 test instances, 15 SAT solvers, 105 CNF formula features. MetaLibria rank 5/7 (weak).

\textbf{ASP-POTASSCO}: Answer set programming with 1,294 test instances, 4 ASP solvers, 138 program features. MetaLibria rank 5/7 (weak).

\section{Reproducibility Checklist}

\begin{itemize}
\item[$\square$] Code repository: [URL to be added]
\item[$\square$] Dataset sources: ASlib (\url{http://www.aslib.net/})
\item[$\square$] Random seeds: Documented in code
\item[$\square$] Cross-validation folds: ASlib standard folds
\item[$\square$] Hardware specifications: Intel Xeon E5-2680 v4, 64GB RAM
\item[$\square$] Software versions: Python 3.9, scikit-learn 1.0, NumPy 1.21
\item[$\square$] Hyperparameters: Table in Appendix C
\item[$\square$] Training time: 0.1-0.5 seconds per scenario
\item[$\square$] Evaluation protocol: Described in Section~\ref{sec:experiments}
\end{itemize}

\end{document}
